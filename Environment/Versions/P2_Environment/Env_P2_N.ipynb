{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Phase Environment implemented based on Env_P1 but for more herds in a population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import copy\n",
    "import random\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import geom\n",
    "from scipy.stats import hypergeom\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.specs import BoundedArraySpec\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "from tf_agents.trajectories.time_step import TimeStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "Env_P2_N is a class that represents an epidemic with n herds.  \n",
    "\n",
    "<img src=\"Sketch_P2.jpeg\"\n",
    "     alt=\"Env_P2_N Sketch\"\n",
    "     style=\"float: left; margin-right: 5px;\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env_P2_N(py_environment.PyEnvironment):\n",
    "    def __init__(self,\n",
    "                num_herds = 10,\n",
    "                total_population = 3000,\n",
    "                split_even = True,\n",
    "                population_range = None,\n",
    "                num_transfers = 10,\n",
    "                weeks_until_exchange = 4,\n",
    "                rand_recovery_prob = 0.005,\n",
    "                rand_infection_prob = 0.01\n",
    "                ):\n",
    "        super(Env_P2_N, self).__init__()\n",
    "        self._state = np.zeros(((num_herds*2),), np.int32)\n",
    "        self._observation = np.zeros(((num_herds*3),), np.int32)\n",
    "        self._discount = np.float32(1)\n",
    "        self._time = 0\n",
    "        self._episode_length = 0\n",
    "        self._tests = []\n",
    "        self._reward = np.float32(0)\n",
    "        self._c_tests = 0.5   #cost for each test\n",
    "        self._c_prime_tests = 10    #organizational costs tests\n",
    "        self._e_removed = 3   #individual replacement cost\n",
    "        self._weeks_until_testresults = 3\n",
    "        self._split_even = split_even\n",
    "        self._population_range = population_range\n",
    "        self._num_herds = num_herds\n",
    "        self._num_transfers = num_transfers\n",
    "        self._total_population = total_population\n",
    "        self._exchanged_members = np.int32(np.round(total_population / (num_transfers*num_herds*5)))    #k from scrapsheet\n",
    "        self._weeks_until_exchange = weeks_until_exchange    #T from scrapsheet\n",
    "        self._rand_recovery_prob = rand_recovery_prob    #g from scrapsheet\n",
    "        self._rand_infection_prob = rand_infection_prob    #q from scrapsheet\n",
    "    \n",
    "    def action_spec(self):\n",
    "        #Actions for: number of subjects to be tested h1, h2. number of subjects to be eliminated h1, h2\n",
    "        max_array = np.ones(((self._num_herds*2),), np.int32)\n",
    "        for i in range (0, self._num_herds):\n",
    "            max_array[i] = self._state[i]\n",
    "        return BoundedArraySpec(((self._num_herds*2),), np.int32, minimum=0, maximum=max_array)\n",
    "    \n",
    "    \n",
    "    def observation_spec(self):\n",
    "        # tau, x0, x1 for both herds\n",
    "        return BoundedArraySpec(((self._num_herds*3),), np.int32, minimum=0, maximum=array(self._total_population))\n",
    "    \n",
    "    \n",
    "    def _reset(self):\n",
    "        '''\n",
    "        State consists of actual state of each herd (population and infected, state[1]),\n",
    "        and observation the agent gets to see (state[0]).\n",
    "        state[0] contains:\n",
    "        number of steps since test has taken place,\n",
    "        number of positive tests,\n",
    "        number of negative tests\n",
    "        for each herd.  \n",
    "        '''\n",
    "        self._state = np.zeros(((self._num_herds*2),), np.int32)\n",
    "        if self._split_even:\n",
    "            for i in range (0, self._num_herds):\n",
    "                self._state[i] = self._total_population / self._num_herds\n",
    "        else:\n",
    "            raise NameError('Work more Maurice.')\n",
    "        \n",
    "        initial_infected_h1 = np.random.randint(low = 1, high = (self._state[0]/8))\n",
    "        self._tests = []\n",
    "        self._time = 0\n",
    "        self._reward = np.float32(0)\n",
    "        self._episode_length = geom.rvs(p = 1/270)\n",
    "        self._state[self._num_herds] = initial_infected_h1    #infected h1\n",
    "        self._observation = np.zeros(((self._num_herds*3),), np.int32)\n",
    "        return TimeStep(StepType.FIRST, reward=self._reward,\n",
    "                    discount=self._discount, observation = self._observation)\n",
    "    \n",
    "    def _test(self, herd = -1, num_tests = 0):\n",
    "        '''\n",
    "        Randomly draws (without returning) num_tests subjects of a herd,\n",
    "        then tests whether they are infected or not before returning testresults.\n",
    "        '''\n",
    "        assert self._state[herd] >= num_tests, \"More tests than herd members.\"\n",
    "        if herd >= 0 and num_tests > 0:\n",
    "            test_out = hypergeom.rvs(M = self._state[herd], n = self._state[herd+self._num_herds], N = int(num_tests), size = None)\n",
    "            testresults = np.zeros(3, np.int32)\n",
    "            testresults[1] = num_tests - test_out #negative tests\n",
    "            testresults[2] = test_out #positive tests\n",
    "            return testresults\n",
    "        else:\n",
    "            return np.zeros(3, np.int32)\n",
    "        \n",
    "    def _transfer(self, origin_herd = -1, target_herd = -1):\n",
    "        ''' \n",
    "        Each self._weeks_until_exchange weeks, transfers subjects \n",
    "        from origin_herd to target_herd by randomly drawing (without return)\n",
    "        self._exchanged_members subjects from all subjects of origin_herd.\n",
    "        returns numbers of infected transfers and susceptible transfers.\n",
    "        '''\n",
    "\n",
    "        assert self._state[origin_herd] > self._exchanged_members, \"Population in origin herd too low.\"\n",
    "        if origin_herd >= 0 and target_herd >=0 and self._time % self._weeks_until_exchange == 0:\n",
    "            infected_transfers = hypergeom.rvs(M = self._state[origin_herd], \n",
    "                                                 n = self._state[origin_herd+self._num_herds], N = self._exchanged_members, size = None)\n",
    "            susceptible_transfers = self._exchanged_members - infected_transfers\n",
    "            return np.array([susceptible_transfers, infected_transfers])    \n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def _model(self, action: np.ndarray):\n",
    "        '''\n",
    "        Completes one time step in a herd (i.e. excluding transfers and tests).\n",
    "        In f(x), samples new infections from poisson dist with lambda = 0.05,\n",
    "        also considers spontaneous infection and recovery factors.\n",
    "        Then, depending on whether a herd is to be replaced by healthy subjects (action),\n",
    "        calls f(x) or simply replaces all subjects by healthy subjects for each herd.\n",
    "        '''\n",
    "        #Model for one herd\n",
    "        def f(x):\n",
    "            S = np.int32(x[0])\n",
    "            I = np.int32(x[1])\n",
    "            new_infs = 0\n",
    "            inf_rvs = poisson.rvs(0.05, size = I)\n",
    "            rec_rvs = poisson.rvs(self._rand_recovery_prob, size = I)\n",
    "            rand_rvs = poisson.rvs(self._rand_infection_prob, size = S)\n",
    "            potential_infs = np.sum(inf_rvs)\n",
    "            recoveries = np.sum(rec_rvs)\n",
    "            rand_infs = np.sum(rand_rvs)\n",
    "            potential_infs = min(potential_infs, S)\n",
    "            #code draw whether subject to be infected is already infected or sus\n",
    "            if potential_infs >= 1:\n",
    "                new_infs = hypergeom.rvs(M = (S + I), n = S, N = potential_infs, size = None)\n",
    "            new_I = I + new_infs + rand_infs - recoveries\n",
    "            return new_I\n",
    "        \n",
    "        #One step for each herd\n",
    "        step_results = self._state\n",
    "        temp_x = np.zeros((2,), np.int32)\n",
    "        for i in range (self._num_herds, self._num_herds*2):\n",
    "            if action[i] == 1:\n",
    "                step_results[i] = 0\n",
    "            else:\n",
    "                step_results[i] = f(np.array([(self._state[i-self._num_herds] - self._state[i]), self._state[i]]))\n",
    "        return step_results\n",
    "    \n",
    "    def _reward_func(self, action: np.ndarray):\n",
    "        '''\n",
    "        Calculates and returns reward.\n",
    "        '''\n",
    "        for i in range (0, self._num_herds):\n",
    "            self._reward -= self._discount * (action[i] * self._c_tests + min(action[i],1) * self._c_prime_tests) / (self._total_population/self._num_herds)\n",
    "            self._reward -= self._discount * (action[i+self._num_herds] * self._state[i] * self._e_removed ) / (self._total_population/self._num_herds)\n",
    "            self._reward -= self._discount * (self._state[i+self._num_herds])**1.5 / (self._total_population/self._num_herds)\n",
    "        return self._reward\n",
    "    \n",
    "    def _step(self, action: np.ndarray):\n",
    "        '''\n",
    "        Step completes one time step in the environment.\n",
    "        First, transfers subjects if time interval dictates it.\n",
    "        Then, calls model(action) to complete a time step in each herd.\n",
    "        Afterwards, tests subjects if action dictates it and outputs testresults\n",
    "        if time for testing has been concluded.\n",
    "        Finally, calculates reward and returns a Time_Step object.\n",
    "        TimeStep(StepType.MID, reward=reward, discount=self._discount, observation=self._observation)\n",
    "        '''\n",
    "        if self._current_time_step.is_last():\n",
    "            return self.reset()\n",
    "            \n",
    "        self._time += 1\n",
    "        \n",
    "        #Transfers\n",
    "        indices = np.arange(self._num_herds)\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range (0, self._num_transfers):\n",
    "            origin_herd = indices[i % (self._num_herds-1)]\n",
    "            if i % self._num_herds == 0:\n",
    "                target_herd = indices[self._num_herds-1]\n",
    "            else:\n",
    "                target_herd = indices[(i % (self._num_herds-1))-1]\n",
    "            transfers = self._transfer(origin_herd = origin_herd, target_herd = target_herd)\n",
    "            back_transfers = self._transfer(origin_herd = target_herd, target_herd = origin_herd)\n",
    "            if transfers is not None:\n",
    "                self._state[origin_herd+self._num_herds] = self._state[origin_herd+self._num_herds] - transfers[1] + back_transfers[1]\n",
    "                self._state[target_herd+self._num_herds] = self._state[target_herd+self._num_herds] + transfers[1] - back_transfers[1]\n",
    "            \n",
    "        #Model should make a step in between transfer and test\n",
    "        self._state = self._model(action)\n",
    "                \n",
    "        #Testing\n",
    "        for i in range (0, self._num_herds):\n",
    "            self._tests.append(self._test(herd = i, num_tests = action[i]))\n",
    "        lim = np.ma.size(self._tests, axis = 0)\n",
    "        get_obs = False\n",
    "        \n",
    "\n",
    "        for i in reversed (range (0, lim)):\n",
    "            if self._tests[i][0] < self._weeks_until_testresults:\n",
    "                self._tests[i][0] += 1\n",
    "            elif self._tests[i][0] > self._weeks_until_testresults:\n",
    "                raise ValueError()\n",
    "            elif self._tests[i][0] == self._weeks_until_testresults:\n",
    "                get_obs = True\n",
    "                break\n",
    "        \n",
    "        if get_obs:\n",
    "            for j in range (0, self._num_herds):\n",
    "                k = j*3\n",
    "                if self._tests[j][1] == 0 and self._tests[j][2] == 0:\n",
    "                    self._observation[k] += 1\n",
    "                else:\n",
    "                    self._observation[k] = self._tests[j][0]\n",
    "                    self._observation[k+1] = self._tests[j][1]\n",
    "                    self._observation[k+2] = self._tests[j][2]\n",
    "\n",
    "            for k in range(0, self._num_herds):\n",
    "                assert self._tests[k][0] == self._weeks_until_testresults, \"A Test that should not have been removed has been removed.\"\n",
    "                self._tests.pop(k)\n",
    "        else:\n",
    "            for l in range (0, np.size(self._observation), 3):\n",
    "                self._observation[l] += 1\n",
    "        \n",
    "\n",
    "        #Reward function\n",
    "        self._reward = np.float32(self._reward_func(action))\n",
    "        step_reward = np.float32(0)\n",
    "        \n",
    "        #Debugging\n",
    "        #if self._time % 50 == 0:\n",
    "            #print('Hidden State: ', self._state)\n",
    "            #print('Observation: ', self._observation)\n",
    "            \n",
    "        #output\n",
    "        if self._time == self._episode_length:\n",
    "            return TimeStep(StepType.LAST, reward=self._reward, discount=self._discount, observation=self._observation)\n",
    "        else:\n",
    "            return TimeStep(StepType.MID, reward=step_reward, discount=self._discount, observation=self._observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "Testresults with 4 Herds and 30000 total population (split_even=true).  \n",
    "\n",
    "<img src=\"4Herds30KPop.PNG\"\n",
    "     alt=\"Testresults 4H30KP\"\n",
    "     style=\"float: left; margin-right: 5px;\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for  100 Herds with total population of  500000 .\n",
      "Random policy.\n",
      "Final Reward after  85  Steps:  -268432.559763886\n",
      "Final Reward after  76  Steps:  -211303.97389214873\n",
      "Final Reward after  100  Steps:  -353251.2587958068\n",
      "Final Reward after  332  Steps:  -1797944.9302317353\n",
      "Final Reward after  111  Steps:  -420311.63728711253\n",
      "---------------------------------------------\n",
      "Test 100%, replace when more than 50% infected.\n",
      "Final Reward after  528  Steps:  -603605.2821569325\n",
      "Final Reward after  276  Steps:  -303552.7632626153\n",
      "Final Reward after  91  Steps:  -99551.83625983774\n",
      "Final Reward after  322  Steps:  -365964.05142968235\n",
      "Final Reward after  335  Steps:  -376233.1947763828\n",
      "---------------------------------------------\n",
      "Test 50%, replace when more than 50% infected.\n",
      "Final Reward after  1015  Steps:  -1992896.5352371477\n",
      "Final Reward after  320  Steps:  -437996.3629595957\n",
      "Final Reward after  131  Steps:  -144211.59199632437\n",
      "Final Reward after  50  Steps:  -49414.5647060265\n",
      "Final Reward after  68  Steps:  -59769.464506175325\n",
      "---------------------------------------------\n",
      "Do-nothing-policy.\n",
      "Final Reward after  21  Steps:  -8382.286666483014\n",
      "Final Reward after  811  Steps:  -4769337.3373759175\n",
      "Final Reward after  488  Steps:  -2760312.2028713124\n",
      "Final Reward after  803  Steps:  -4718033.421218839\n",
      "Final Reward after  256  Steps:  -1317021.363785989\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''Testing environment.'''\n",
    "n = 100\n",
    "pop = 500000\n",
    "env = Env_P2_N(num_herds = n, total_population = pop)\n",
    "action = np.zeros((2*n,), np.int32)\n",
    "print(\"Testing for \", n, \"Herds with total population of \", pop, \".\")\n",
    "\n",
    "time_step = env.reset()\n",
    "aspec = env.action_spec()\n",
    "counter = 0\n",
    "i = 0\n",
    "print(\"Random policy.\")\n",
    "while True:\n",
    "    if (counter == 5):\n",
    "        counter = 0\n",
    "        i = 0\n",
    "        print(\"---------------------------------------------\")\n",
    "        break\n",
    "    for j in range (0, (2*n)):\n",
    "        action[j] = np.random.randint(low = aspec.minimum, high = aspec.maximum[j])    \n",
    "    time_step = env.step(action)\n",
    "    i +=1\n",
    "    if env._current_time_step.is_last():\n",
    "        counter += 1\n",
    "        print(\"Final Reward after \", i , \" Steps: \", time_step.reward)\n",
    "        i = 0\n",
    "\n",
    "\n",
    "action = np.zeros((2*n,), np.int32)\n",
    "time_step = env.reset()\n",
    "aspec = env.action_spec()\n",
    "counter = 0\n",
    "i = 0\n",
    "print(\"Test 100%, replace when more than 50% infected.\")\n",
    "while True:\n",
    "    if (counter == 5):\n",
    "        counter = 0\n",
    "        i = 0\n",
    "        print(\"---------------------------------------------\")\n",
    "        break\n",
    "    for j in range (0, (2*n)):\n",
    "        if j < n:\n",
    "            action[j] = aspec.maximum[j]\n",
    "        elif j >= n: \n",
    "            if time_step.observation[((j-n)*3)+2] > time_step.observation[((j-n)*3)+1]:\n",
    "                action[j] = 1\n",
    "            else:\n",
    "                action[j] = 0\n",
    "        else:\n",
    "            print(\"how did this happen?\")  \n",
    "    time_step = env.step(action)\n",
    "    i +=1\n",
    "    if env._current_time_step.is_last():\n",
    "        counter += 1\n",
    "        print(\"Final Reward after \", i , \" Steps: \", time_step.reward)\n",
    "        i = 0\n",
    "        \n",
    "\n",
    "action = np.zeros((2*n,), np.int32)        \n",
    "time_step = env.reset()\n",
    "aspec = env.action_spec()\n",
    "counter = 0\n",
    "i = 0\n",
    "print(\"Test 50%, replace when more than 50% infected.\")\n",
    "while True:\n",
    "    if (counter == 5):\n",
    "        counter = 0\n",
    "        i = 0\n",
    "        print(\"---------------------------------------------\")\n",
    "        break\n",
    "    for j in range (0, (2*n)):\n",
    "        if j < n:\n",
    "            action[j] = aspec.maximum[j]/2\n",
    "        elif j >= n: \n",
    "            if time_step.observation[((j-n)*3)+2] > time_step.observation[((j-n)*3)+1]:\n",
    "                action[j] = 1\n",
    "            else:\n",
    "                action[j] = 0\n",
    "        else:\n",
    "            print(\"how did this happen?\")  \n",
    "    time_step = env.step(action)\n",
    "    i +=1\n",
    "    if env._current_time_step.is_last():\n",
    "        counter += 1\n",
    "        print(\"Final Reward after \", i , \" Steps: \", time_step.reward)\n",
    "        i = 0\n",
    "\n",
    "\n",
    "action = np.zeros((2*n,), np.int32)\n",
    "time_step = env.reset()\n",
    "aspec = env.action_spec()\n",
    "counter = 0\n",
    "i = 0\n",
    "print(\"Do-nothing-policy.\")\n",
    "while True:\n",
    "    if (counter == 5):\n",
    "        counter = 0\n",
    "        i = 0\n",
    "        print(\"---------------------------------------------\")\n",
    "        break\n",
    "    time_step = env.step(action)\n",
    "    i +=1\n",
    "    if env._current_time_step.is_last():\n",
    "        counter += 1\n",
    "        print(\"Final Reward after \", i , \" Steps: \", time_step.reward)\n",
    "        i = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
