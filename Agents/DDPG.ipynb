{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c844da0c-c325-4c1d-89aa-dc6f30a8cf54",
   "metadata": {},
   "source": [
    "# Agent for epidemic control model  \n",
    "This notebook will train an agent in an epidemic control environment using DDPG with RNNs.  \n",
    "  \n",
    "For use, please edit PATH variable below to any folder where training outputs can be stored.  \n",
    "Also, please create a folder titled 'policy' in PATH directory.  \n",
    "Default environment is EE0, for different environments see \"Environment\" section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae57fd21-7a8e-4f2b-a9fd-998fb8cc1d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/jovyan/Masterarbeit/Agent/Run_34'\n",
    "# Decide whether to use RNN DDPG or ANN DDPG\n",
    "use_rnns = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa4638f-e5a4-4a9c-aef3-0da5db96c608",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Firstly, all relevant dependencies will be imported.  \n",
    "Comments indicate what imports are generally used for or related to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beef1151-a79a-48a1-934b-dd2432ec325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "\n",
    "# Environment \n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "# Neural Networks\n",
    "from tf_agents.agents.ddpg import actor_rnn_network\n",
    "from tf_agents.agents.ddpg import critic_rnn_network\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.networks import nest_map\n",
    "from tf_agents.keras_layers import inner_reshape\n",
    "import functools\n",
    "# Agent \n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "# Experience Replay\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "#Training\n",
    "from tf_agents.utils import common\n",
    "#Evaluation\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.trajectories import time_step\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f283017f-7ad2-4b4e-80dd-63d77f64430b",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "Next, an environment will be imported and initialized.  \n",
    "For training different environments, edit lines 7-11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df64898d-0f22-4ee2-ab83-62cc7fe0c678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(1, '/home/jovyan/Masterarbeit/reinforce-one/Environments')\n",
    "sys.path.insert(1, '/home/jovyan/Masterarbeit/reinforce-one/Environments/Variations')\n",
    "from EE0 import EE0\n",
    "from EE0_A import EE0_A\n",
    "from EE0_NT import EE0_NT\n",
    "from EE1 import EE1\n",
    "from EE1_A import EE1_A\n",
    "\n",
    "num_herds = 2\n",
    "total_population = 300\n",
    "average_episode_length=200\n",
    "fix_episode_length = True\n",
    "py_env = EE0(num_herds = num_herds, total_population = total_population, fix_episode_length = fix_episode_length, \n",
    "               average_episode_length = average_episode_length)\n",
    "\n",
    "# Transforms py environment into tensorflow environment (i/o are now tensors)\n",
    "train_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acda5e7-21a5-4c7a-a525-7737fc0204f4",
   "metadata": {},
   "source": [
    "## Training\n",
    "In this section, define a function for agent training and evaluation.  \n",
    "First, create neural networks for use for variations in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f462ba1-b95b-40c0-b8e3-cb80620a660a",
   "metadata": {},
   "source": [
    "### RNN DDPG\n",
    "\n",
    "Set up actor and critic recurrent neural networks for training with DDPG using RNNs.  \n",
    "Edit hyperparams for different layer sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c24b1614-aa95-44fb-8ee1-2bf7a1453876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN hyperparams\n",
    "actor_fc_layers = (200, 150)\n",
    "actor_output_fc_layers = (50,)\n",
    "actor_lstm_size = (40,)\n",
    "critic_obs_fc_layers = (200,)\n",
    "critic_action_fc_layers = None\n",
    "critic_joint_fc_layers = (150,)\n",
    "critic_output_fc_layers = (50,)\n",
    "critic_lstm_size = (40,)\n",
    "\n",
    "# RNN actor critic\n",
    "actor_rnn = actor_rnn_network.ActorRnnNetwork(train_env.time_step_spec().observation, \n",
    "                                              train_env.action_spec(), \n",
    "                                              input_fc_layer_params=actor_fc_layers, \n",
    "                                              lstm_size = actor_lstm_size, \n",
    "                                              output_fc_layer_params=actor_output_fc_layers)\n",
    "\n",
    "critic_net_input_specs = (train_env.time_step_spec().observation, \n",
    "                          train_env.action_spec())\n",
    "\n",
    "critic_rnn = critic_rnn_network.CriticRnnNetwork(critic_net_input_specs, \n",
    "                                                 observation_fc_layer_params=critic_obs_fc_layers, \n",
    "                                                 action_fc_layer_params=critic_action_fc_layers, \n",
    "                                                 joint_fc_layer_params=critic_joint_fc_layers, \n",
    "                                                 lstm_size=critic_lstm_size, \n",
    "                                                 output_fc_layer_params=critic_output_fc_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0521fad7-dfd8-4bf9-8ae5-c1aee104bba5",
   "metadata": {},
   "source": [
    "### ANN DDPG  \n",
    "Create actor and critic artificial neural networks for DDPG.  \n",
    "Again, edit hyperparams for different layer sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98974f12-a15d-4624-a538-3fb74e751460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ann hyperparameters\n",
    "actor_fc_layers=(400, 300)\n",
    "critic_obs_fc_layers=(400,)\n",
    "critic_action_fc_layers=None\n",
    "critic_joint_fc_layers=(300,)\n",
    "\n",
    "\n",
    "# Define creation functions \n",
    "\n",
    "dense = functools.partial(tf.keras.layers.Dense,\n",
    "                          activation=tf.keras.activations.relu,\n",
    "                          kernel_initializer=tf.compat.v1.variance_scaling_initializer(\n",
    "                              scale=1./ 3.0, mode='fan_in', distribution='uniform')\n",
    "                         )\n",
    "\n",
    "\n",
    "def create_identity_layer():\n",
    "    return tf.keras.layers.Lambda(lambda x: x)\n",
    "\n",
    "\n",
    "def create_fc_network(layer_units):\n",
    "    return sequential.Sequential([dense(num_units) for num_units in layer_units])\n",
    "\n",
    "\n",
    "def create_actor_network(fc_layer_units, action_spec):\n",
    "    flat_action_spec = tf.nest.flatten(action_spec)\n",
    "    if len(flat_action_spec) > 1:\n",
    "        raise ValueError('Only a single action tensor is supported by this network')\n",
    "    flat_action_spec = flat_action_spec[0]\n",
    "\n",
    "    fc_layers = [dense(num_units) for num_units in fc_layer_units]\n",
    "\n",
    "    num_actions = flat_action_spec.shape.num_elements()\n",
    "    \n",
    "    action_fc_layer = tf.keras.layers.Dense(num_actions,\n",
    "                                            activation=tf.keras.activations.tanh,\n",
    "                                            kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "                                                minval=-0.003, maxval=0.003)\n",
    "                                           )\n",
    "\n",
    "    scaling_layer = tf.keras.layers.Lambda(\n",
    "        lambda x: common.scale_to_spec(x, flat_action_spec))\n",
    "    return sequential.Sequential(fc_layers + [action_fc_layer, scaling_layer])\n",
    "\n",
    "\n",
    "def create_critic_network(obs_fc_layer_units,\n",
    "                          action_fc_layer_units,\n",
    "                          joint_fc_layer_units):\n",
    "    def split_inputs(inputs):\n",
    "        return {'observation': inputs[0], 'action': inputs[1]}\n",
    "    \n",
    "    if obs_fc_layer_units:\n",
    "        obs_network = create_fc_network(obs_fc_layer_units)  \n",
    "    else:\n",
    "        obs_network = create_identity_layer()\n",
    "    if action_fc_layer_units:    \n",
    "        action_network = create_fc_network(action_fc_layer_units)\n",
    "    else:\n",
    "        action_network = create_identity_layer()\n",
    "    if joint_fc_layer_units:    \n",
    "        joint_network = create_fc_network(joint_fc_layer_units) \n",
    "    else: \n",
    "        joint_network = create_identity_layer()\n",
    "    value_fc_layer = tf.keras.layers.Dense(1,\n",
    "                                           activation=None,\n",
    "                                           kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.003, maxval=0.003)\n",
    "                                          )\n",
    "\n",
    "    return sequential.Sequential([tf.keras.layers.Lambda(split_inputs),\n",
    "                                  nest_map.NestMap({'observation': obs_network,\n",
    "                                                    'action': action_network}),\n",
    "                                  nest_map.NestFlatten(),\n",
    "                                  tf.keras.layers.Concatenate(),\n",
    "                                  joint_network,\n",
    "                                  value_fc_layer,\n",
    "                                  inner_reshape.InnerReshape([1], [])\n",
    "                                 ])\n",
    "\n",
    "\n",
    "# Create neural networks\n",
    "\n",
    "actor_ann = create_actor_network(actor_fc_layers, \n",
    "                                 train_env.action_spec())\n",
    "critic_ann = create_critic_network(critic_obs_fc_layers,\n",
    "                                   critic_action_fc_layers,\n",
    "                                   critic_joint_fc_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd8d1e-fc46-4eb6-8d77-ac4ea56a6def",
   "metadata": {},
   "source": [
    "### Hyperparameters  \n",
    "Set hyperparameters for DDPG training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8c325f5-dfc6-4ebe-b5d6-81e081675a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 1000000\n",
    "\n",
    "# Agent hyperparameters\n",
    "actor_learning_rate = 1e-4\n",
    "critic_learning_rate = 1e-3\n",
    "ou_stddev = 0.2\n",
    "ou_damping = 0.15\n",
    "target_update_tau = 0.05\n",
    "target_update_period = 100\n",
    "gamma = 0.995\n",
    "# Training hyperparameters\n",
    "train_steps_per_iteration = 1\n",
    "\n",
    "# Experience replay hyperparameters\n",
    "rb_capacity = 500000\n",
    "batch_size = 64\n",
    "train_sequence_length = 200    # Automatically set to 1 for ANN DDPG\n",
    "# For ANN DDPG\n",
    "collect_steps_per_iteration = 200\n",
    "initial_collect_steps = 25000\n",
    "# For RNN DDPG\n",
    "initial_collect_episodes = 10\n",
    "collect_episodes_per_iteration = 1\n",
    "\n",
    "# Summary params\n",
    "summary_interval = 1000\n",
    "# Evaluation hyperparameters\n",
    "eval_interval = 1000\n",
    "eval_episodes = 200\n",
    "threshhold_return = -30\n",
    "threshhold_reset_interval = 50000\n",
    "plots = False  # Only works if num_herds = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf4aa4-850d-4079-abdf-c644ad4095e9",
   "metadata": {},
   "source": [
    "### DDPG  \n",
    "Finally, define training function using tf-agent's ddpg agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb1188ab-e1ff-458f-b452-684bd5e5e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DDPG(num_iterations = num_iterations,\n",
    "         actor_net = None,\n",
    "         critic_net = None,\n",
    "         directory = PATH,\n",
    "         plots = plots,\n",
    "         eval_interval = eval_interval,\n",
    "         summary_interval = summary_interval,\n",
    "         best_return = threshhold_return,\n",
    "         threshhold_reset_interval = threshhold_reset_interval,\n",
    "         # Agent hyperparameters\n",
    "         actor_learning_rate = actor_learning_rate,\n",
    "         critic_learning_rate = critic_learning_rate,\n",
    "         ou_stddev = ou_stddev,\n",
    "         ou_damping = ou_damping,\n",
    "         target_update_tau = target_update_tau,\n",
    "         target_update_period = target_update_period,\n",
    "         gamma = gamma,\n",
    "         # Training hyperparameters\n",
    "         train_steps_per_iteration = train_steps_per_iteration,\n",
    "         # Experience replay hyperparameters\n",
    "         initial_collect_episodes = initial_collect_episodes,\n",
    "         collect_episodes_per_iteration = collect_episodes_per_iteration,\n",
    "         rb_capacity = rb_capacity,\n",
    "         batch_size = batch_size,\n",
    "         train_sequence_length = train_sequence_length):\n",
    "    \n",
    "    if actor_net is None or critic_net is None:\n",
    "        raise ValueError('Please input an actor network and critic network.')\n",
    "    \n",
    "    # Create directories for summary output\n",
    "    directory = os.path.expanduser(directory)\n",
    "    train_dir = os.path.join(directory, 'train')\n",
    "    eval_dir = os.path.join(directory, 'eval')\n",
    "    policy_dir = os.path.join(directory, 'policy')\n",
    "    \n",
    "    # Global step tracks number of train steps\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "    \n",
    "    # Initialize summary writers \n",
    "    train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "                               train_dir, flush_millis=10000)\n",
    "    train_summary_writer.set_as_default()\n",
    "\n",
    "    eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "                              eval_dir, flush_millis=10000)\n",
    "    eval_metrics = [tf_metrics.AverageReturnMetric(buffer_size=eval_episodes),\n",
    "                    tf_metrics.AverageEpisodeLengthMetric(buffer_size=eval_episodes)]\n",
    "    \n",
    "    with tf.compat.v2.summary.record_if(lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
    "    \n",
    "        # DDPG Agent\n",
    "        agent = ddpg_agent.DdpgAgent(train_env.time_step_spec(), \n",
    "                                     train_env.action_spec(), \n",
    "                                     actor_network = actor_net, \n",
    "                                     critic_network = critic_net, \n",
    "                                     actor_optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=actor_learning_rate), \n",
    "                                     critic_optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=critic_learning_rate), \n",
    "                                     ou_stddev = ou_stddev, \n",
    "                                     ou_damping = ou_damping, \n",
    "                                     target_update_tau = target_update_tau, \n",
    "                                     target_update_period = target_update_period,  \n",
    "                                     gamma = gamma, \n",
    "                                     train_step_counter = global_step)\n",
    "        agent.initialize()\n",
    "        \n",
    "        # Metrics to be tracked in the summary \n",
    "        train_metrics = [tf_metrics.NumberOfEpisodes(),\n",
    "                         tf_metrics.EnvironmentSteps(),\n",
    "                         tf_metrics.AverageReturnMetric(),\n",
    "                         tf_metrics.AverageEpisodeLengthMetric()]\n",
    "    \n",
    "        eval_metrics = [tf_metrics.AverageReturnMetric(buffer_size=eval_episodes), \n",
    "                        tf_metrics.AverageEpisodeLengthMetric(buffer_size=eval_episodes)]\n",
    "        \n",
    "        # Tools for evaluation\n",
    "        eval_policy = agent.policy\n",
    "        saver = policy_saver.PolicySaver(eval_policy)\n",
    "\n",
    "        # Experience replay and sample collection tools\n",
    "        collect_policy = agent.collect_policy\n",
    "        replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(agent.collect_data_spec,\n",
    "                                                                       batch_size=train_env.batch_size,\n",
    "                                                                       max_length=rb_capacity)\n",
    "    \n",
    "        # Assign step drivers to fill replay buffer \n",
    "        if isinstance(actor_net, actor_rnn_network.ActorRnnNetwork):\n",
    "            initial_collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(train_env,\n",
    "                                                                                 collect_policy,\n",
    "                                                                                 observers=[replay_buffer.add_batch],\n",
    "                                                                                 num_episodes=initial_collect_episodes)\n",
    "\n",
    "            collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(train_env,\n",
    "                                                                         collect_policy,\n",
    "                                                                         observers=[replay_buffer.add_batch] + train_metrics,\n",
    "                                                                         num_episodes=collect_episodes_per_iteration)\n",
    "        else:\n",
    "            initial_collect_driver = dynamic_step_driver.DynamicStepDriver(train_env,\n",
    "                                                                           collect_policy,\n",
    "                                                                           observers=[replay_buffer.add_batch],\n",
    "                                                                           num_steps=initial_collect_steps)\n",
    "\n",
    "            collect_driver = dynamic_step_driver.DynamicStepDriver(train_env,\n",
    "                                                                   collect_policy,\n",
    "                                                                   observers=[replay_buffer.add_batch] + train_metrics,\n",
    "                                                                   num_steps=collect_steps_per_iteration)\n",
    "        \n",
    "        # TF functions speed up training process\n",
    "        initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "        collect_driver.run = common.function(collect_driver.run)\n",
    "        agent.train = common.function(agent.train)\n",
    "    \n",
    "        # Collect initial random samples for replay buffer\n",
    "        initial_collect_driver.run()\n",
    "    \n",
    "        # Training starts\n",
    "        time_step = None\n",
    "        policy_state = collect_policy.get_initial_state(train_env.batch_size)\n",
    "    \n",
    "        # If it's an ANN, trajectories have to be two steps long\n",
    "        if not isinstance(actor_net, actor_rnn_network.ActorRnnNetwork):\n",
    "            train_sequence_length = 1\n",
    "        \n",
    "        dataset = replay_buffer.as_dataset(num_parallel_calls=3,\n",
    "                                           sample_batch_size=batch_size,\n",
    "                                           num_steps=train_sequence_length + 1).prefetch(3)\n",
    "        iterator = iter(dataset)\n",
    "    \n",
    "    \n",
    "        def train_step():\n",
    "            experience, other_info = next(iterator)\n",
    "            return agent.train(experience)\n",
    "        train_step = common.function(train_step)\n",
    "\n",
    "    \n",
    "        for _ in range(num_iterations):\n",
    "            time_step, policy_state = collect_driver.run(time_step=time_step,\n",
    "                                                         policy_state=policy_state)    \n",
    "            for _ in range(train_steps_per_iteration):\n",
    "                train_loss = train_step()\n",
    "            for train_metric in train_metrics:\n",
    "                train_metric.tf_summaries(train_step=global_step, step_metrics=train_metrics[:2])\n",
    "            # Evaluation\n",
    "            if global_step.numpy() % eval_interval == 0:\n",
    "                results = metric_utils.eager_compute(eval_metrics, \n",
    "                                                     eval_env,\n",
    "                                                     eval_policy,\n",
    "                                                     num_episodes=eval_episodes,\n",
    "                                                     train_step=global_step,\n",
    "                                                     summary_writer=eval_summary_writer,\n",
    "                                                     summary_prefix='Metrics')\n",
    "                metric_utils.log_metrics(eval_metrics)\n",
    "                if results['AverageReturn'].numpy() >= -10:\n",
    "                    eval_interval = 1000\n",
    "                    plots = False\n",
    "                if results['AverageReturn'].numpy() < -15:\n",
    "                    eval_interval = 1000\n",
    "                    plots = False\n",
    "                print('Global Step = {0}, Average Return = {1}.'.format(global_step.numpy(), results['AverageReturn'].numpy())) \n",
    "                if results['AverageReturn'].numpy() > best_return:\n",
    "                    best_return = results['AverageReturn'].numpy()\n",
    "                    print('New best return: ', best_return)\n",
    "                    #average_return, culls, tests = eval_agent(eval_env, \n",
    "                                                              #eval_policy, \n",
    "                                                              #num_episodes=eval_episodes, \n",
    "                                                              #create_plot = plots)\n",
    "                    #print('Re-Tested new best return: ', average_return)\n",
    "                    #print('Average Culls = {0}, Average Tests = {1}.'.format(culls, tests))  \n",
    "                    dir_name = str(global_step.numpy()) + '_' + str(best_return)\n",
    "                    saver.save(os.path.join(policy_dir, dir_name))\n",
    "            if global_step.numpy() % threshhold_reset_interval == 0:\n",
    "                best_return = threshhold_return\n",
    "                    \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b462205-04dc-45b7-8778-51384f7ea951",
   "metadata": {},
   "source": [
    "# Run Functions (rename)  \n",
    "Now you can execute ddpg using either artificial or recurrent NNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56903148-5341-4610-b7eb-c14e818cf72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n",
      "Global Step = 1000, Average Return = -53.10939407348633.\n",
      "Global Step = 2000, Average Return = -59.759033203125.\n",
      "Global Step = 3000, Average Return = -60.6397590637207.\n",
      "Global Step = 4000, Average Return = -60.23875045776367.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x7fd3487619d0>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x7fd3487619d0>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 5000, Average Return = -59.82742691040039.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x7fd3a00547c0>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x7fd3a00547c0>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 6000, Average Return = -57.5993537902832.\n",
      "Global Step = 7000, Average Return = -60.82661056518555.\n",
      "Global Step = 8000, Average Return = -57.135169982910156.\n",
      "Global Step = 9000, Average Return = -54.535400390625.\n",
      "Global Step = 10000, Average Return = -54.576759338378906.\n",
      "Global Step = 11000, Average Return = -56.02443313598633.\n",
      "Global Step = 12000, Average Return = -53.42489242553711.\n",
      "Global Step = 13000, Average Return = -59.2261962890625.\n",
      "Global Step = 14000, Average Return = -58.623260498046875.\n",
      "Global Step = 15000, Average Return = -62.480831146240234.\n",
      "Global Step = 16000, Average Return = -37.17106246948242.\n",
      "Global Step = 17000, Average Return = -41.667030334472656.\n",
      "Global Step = 18000, Average Return = -40.6563606262207.\n",
      "Global Step = 19000, Average Return = -39.5485954284668.\n",
      "Global Step = 20000, Average Return = -33.4965934753418.\n",
      "Global Step = 21000, Average Return = -32.589393615722656.\n",
      "Global Step = 22000, Average Return = -56.2103271484375.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 23000, Average Return = -29.31875991821289.\n",
      "New best return:  -29.31876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/23000_-29.31876/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/23000_-29.31876/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 24000, Average Return = -43.88429641723633.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 25000, Average Return = -26.352563858032227.\n",
      "New best return:  -26.352564\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/25000_-26.352564/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/25000_-26.352564/assets\n",
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 26000, Average Return = -26.064462661743164.\n",
      "New best return:  -26.064463\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/26000_-26.064463/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/26000_-26.064463/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 27000, Average Return = -32.631126403808594.\n",
      "Global Step = 28000, Average Return = -36.899993896484375.\n",
      "Global Step = 29000, Average Return = -27.40936279296875.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 30000, Average Return = -10.384936332702637.\n",
      "New best return:  -10.384936\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/30000_-10.384936/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/30000_-10.384936/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 31000, Average Return = -17.314132690429688.\n",
      "Global Step = 32000, Average Return = -11.012502670288086.\n",
      "Global Step = 33000, Average Return = -53.28799819946289.\n",
      "Global Step = 34000, Average Return = -31.761629104614258.\n",
      "Global Step = 35000, Average Return = -11.872901916503906.\n",
      "Global Step = 36000, Average Return = -11.748668670654297.\n",
      "Global Step = 37000, Average Return = -13.248934745788574.\n",
      "Global Step = 38000, Average Return = -25.570131301879883.\n",
      "Global Step = 39000, Average Return = -24.55963706970215.\n",
      "Global Step = 40000, Average Return = -22.616004943847656.\n",
      "Global Step = 41000, Average Return = -10.77617073059082.\n",
      "Global Step = 42000, Average Return = -14.339098930358887.\n",
      "Global Step = 43000, Average Return = -26.56790542602539.\n",
      "Global Step = 44000, Average Return = -18.115806579589844.\n",
      "Global Step = 45000, Average Return = -10.887299537658691.\n",
      "Global Step = 46000, Average Return = -11.246735572814941.\n",
      "Global Step = 47000, Average Return = -11.225532531738281.\n",
      "Global Step = 48000, Average Return = -14.086832046508789.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 49000, Average Return = -8.522932052612305.\n",
      "New best return:  -8.522932\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/49000_-8.522932/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/49000_-8.522932/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 50000, Average Return = -8.918566703796387.\n",
      "Global Step = 51000, Average Return = -38.59492874145508.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 52000, Average Return = -6.987432956695557.\n",
      "New best return:  -6.987433\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/52000_-6.987433/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/52000_-6.987433/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 53000, Average Return = -7.385532379150391.\n",
      "Global Step = 54000, Average Return = -11.458970069885254.\n",
      "Global Step = 55000, Average Return = -17.851369857788086.\n",
      "Global Step = 56000, Average Return = -56.09089660644531.\n",
      "Global Step = 57000, Average Return = -55.4398307800293.\n",
      "Global Step = 58000, Average Return = -54.31882858276367.\n",
      "Global Step = 59000, Average Return = -60.0962028503418.\n",
      "Global Step = 60000, Average Return = -57.86989212036133.\n",
      "Global Step = 61000, Average Return = -56.986961364746094.\n",
      "Global Step = 62000, Average Return = -61.158565521240234.\n",
      "Global Step = 63000, Average Return = -58.02336502075195.\n",
      "Global Step = 64000, Average Return = -53.97159194946289.\n",
      "Global Step = 65000, Average Return = -61.36299133300781.\n",
      "Global Step = 66000, Average Return = -60.73619079589844.\n",
      "Global Step = 67000, Average Return = -53.996368408203125.\n",
      "Global Step = 68000, Average Return = -56.267364501953125.\n",
      "Global Step = 69000, Average Return = -59.55859375.\n",
      "Global Step = 70000, Average Return = -59.31682586669922.\n",
      "Global Step = 71000, Average Return = -57.024269104003906.\n",
      "Global Step = 72000, Average Return = -59.08369445800781.\n",
      "Global Step = 73000, Average Return = -56.44986343383789.\n",
      "Global Step = 74000, Average Return = -55.316795349121094.\n",
      "Global Step = 75000, Average Return = -57.41813659667969.\n",
      "Global Step = 76000, Average Return = -58.382362365722656.\n",
      "Global Step = 77000, Average Return = -57.37786102294922.\n",
      "Global Step = 78000, Average Return = -56.074893951416016.\n",
      "Global Step = 79000, Average Return = -57.12086486816406.\n",
      "Global Step = 80000, Average Return = -57.2346305847168.\n",
      "Global Step = 81000, Average Return = -59.761661529541016.\n",
      "Global Step = 82000, Average Return = -58.36125946044922.\n",
      "Global Step = 83000, Average Return = -55.743263244628906.\n",
      "Global Step = 84000, Average Return = -54.17569351196289.\n",
      "Global Step = 85000, Average Return = -55.925960540771484.\n",
      "Global Step = 86000, Average Return = -59.27326202392578.\n",
      "Global Step = 87000, Average Return = -59.678855895996094.\n",
      "Global Step = 88000, Average Return = -55.15496063232422.\n",
      "Global Step = 89000, Average Return = -31.512929916381836.\n",
      "Global Step = 90000, Average Return = -29.390460968017578.\n",
      "Global Step = 91000, Average Return = -26.028427124023438.\n",
      "Global Step = 92000, Average Return = -48.17873001098633.\n",
      "Global Step = 93000, Average Return = -23.06583023071289.\n",
      "Global Step = 94000, Average Return = -61.19743347167969.\n",
      "Global Step = 95000, Average Return = -53.40013122558594.\n",
      "Global Step = 96000, Average Return = -43.032958984375.\n",
      "Global Step = 97000, Average Return = -41.6367301940918.\n",
      "Global Step = 98000, Average Return = -37.414329528808594.\n",
      "Global Step = 99000, Average Return = -30.035131454467773.\n",
      "Global Step = 100000, Average Return = -22.6319637298584.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 101000, Average Return = -25.096694946289062.\n",
      "New best return:  -25.096695\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/101000_-25.096695/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/101000_-25.096695/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 102000, Average Return = -36.32202911376953.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 103000, Average Return = -23.463462829589844.\n",
      "New best return:  -23.463463\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/103000_-23.463463/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/103000_-23.463463/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 104000, Average Return = -24.805828094482422.\n",
      "Global Step = 105000, Average Return = -26.433124542236328.\n",
      "Global Step = 106000, Average Return = -57.67112731933594.\n",
      "Global Step = 107000, Average Return = -59.59846115112305.\n",
      "Global Step = 108000, Average Return = -28.097562789916992.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 109000, Average Return = -22.37279510498047.\n",
      "New best return:  -22.372795\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/109000_-22.372795/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/109000_-22.372795/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 110000, Average Return = -27.703428268432617.\n",
      "Global Step = 111000, Average Return = -25.212928771972656.\n",
      "Global Step = 112000, Average Return = -25.514694213867188.\n",
      "Global Step = 113000, Average Return = -28.9289608001709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 114000, Average Return = -21.874731063842773.\n",
      "New best return:  -21.874731\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/114000_-21.874731/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/114000_-21.874731/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 115000, Average Return = -27.241161346435547.\n",
      "Global Step = 116000, Average Return = -30.910429000854492.\n",
      "Global Step = 117000, Average Return = -24.52096176147461.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 118000, Average Return = -21.672792434692383.\n",
      "New best return:  -21.672792\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/118000_-21.672792/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/118000_-21.672792/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 119000, Average Return = -28.870094299316406.\n",
      "Global Step = 120000, Average Return = -31.439027786254883.\n",
      "Global Step = 121000, Average Return = -28.25629425048828.\n",
      "Global Step = 122000, Average Return = -56.8611946105957.\n",
      "Global Step = 123000, Average Return = -58.118125915527344.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 124000, Average Return = -20.633695602416992.\n",
      "New best return:  -20.633696\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/124000_-20.633696/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/124000_-20.633696/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 125000, Average Return = -24.2319278717041.\n",
      "Global Step = 126000, Average Return = -21.309894561767578.\n",
      "Global Step = 127000, Average Return = -26.999364852905273.\n",
      "Global Step = 128000, Average Return = -29.736162185668945.\n",
      "Global Step = 129000, Average Return = -25.437728881835938.\n",
      "Global Step = 130000, Average Return = -23.240493774414062.\n",
      "Global Step = 131000, Average Return = -27.5947265625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 132000, Average Return = -14.312536239624023.\n",
      "New best return:  -14.312536\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/132000_-14.312536/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/132000_-14.312536/assets\n",
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 133000, Average Return = -13.36593246459961.\n",
      "New best return:  -13.365932\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/133000_-13.365932/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/133000_-13.365932/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 134000, Average Return = -22.08633804321289.\n",
      "Global Step = 135000, Average Return = -15.187599182128906.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 136000, Average Return = -11.346467018127441.\n",
      "New best return:  -11.346467\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/136000_-11.346467/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/136000_-11.346467/assets\n",
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 137000, Average Return = -10.798134803771973.\n",
      "New best return:  -10.798135\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/137000_-10.798135/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/137000_-10.798135/assets\n",
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 138000, Average Return = -6.704333305358887.\n",
      "New best return:  -6.7043333\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/138000_-6.7043333/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/Agent/Run_34/policy/138000_-6.7043333/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step = 139000, Average Return = -15.092665672302246.\n",
      "Global Step = 140000, Average Return = -8.73686695098877.\n",
      "Global Step = 141000, Average Return = -7.63640022277832.\n",
      "Global Step = 142000, Average Return = -16.318002700805664.\n",
      "Global Step = 143000, Average Return = -7.654534339904785.\n",
      "Global Step = 144000, Average Return = -18.359668731689453.\n",
      "Global Step = 145000, Average Return = -8.104233741760254.\n",
      "Global Step = 146000, Average Return = -18.005165100097656.\n"
     ]
    }
   ],
   "source": [
    "if (use_rnns):\n",
    "    anet = actor_rnn\n",
    "    cnet = critic_rnn\n",
    "else:\n",
    "    anet = actor_ann\n",
    "    cnet = critic_ann\n",
    "\n",
    "loss = DDPG(num_iterations = num_iterations,\n",
    "            actor_net = anet,\n",
    "            critic_net = cnet,\n",
    "            directory = PATH,\n",
    "            plots = plots,\n",
    "            eval_interval = eval_interval,\n",
    "            # Agent hyperparameters\n",
    "            actor_learning_rate = actor_learning_rate,\n",
    "            critic_learning_rate = critic_learning_rate,\n",
    "            ou_stddev = ou_stddev,\n",
    "            ou_damping = ou_damping,\n",
    "            target_update_tau = target_update_tau,\n",
    "            target_update_period = target_update_period,\n",
    "            gamma = gamma,\n",
    "            # Experience replay hyperparameters\n",
    "            initial_collect_episodes = initial_collect_episodes,\n",
    "            collect_episodes_per_iteration = collect_episodes_per_iteration,\n",
    "            rb_capacity = rb_capacity,\n",
    "            batch_size = batch_size,\n",
    "            train_sequence_length = train_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e35cb64-a254-4ce5-8895-30496ab2d890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
