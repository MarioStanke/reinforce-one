{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DDPG Implementation using tf agents library.\n",
    "Make sure PATH is an existing repository. \n",
    "Please create a depository PATH/plots for plots showing what the agent is doing\n",
    "in the environment during policy evaluation.\n",
    "\n",
    "Environment models epidemic with n herds and a fixed total population.\n",
    "\n",
    "Note: Plot outputs are in format 'Act_Inf_<episode steps>_<global_step>'\n",
    "'''\n",
    "num_herds = 5\n",
    "total_population = 5000\n",
    "PATH = '~/Masterarbeit/DDPG'  \n",
    "root_dir = PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "import gin\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import range\n",
    "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
    "\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import parallel_py_environment\n",
    "from tf_agents.environments import suite_mujoco\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.keras_layers import inner_reshape\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import nest_map\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from DDPG_Env_P2_N import Env_P2_N\n",
    "from P_DDPG_Env_P2_N import P_Env_P2_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e4183-6b8a-4629-9e9a-dc77f9c6b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plots(\n",
    "    plot_dir,    # Directory for plot output\n",
    "    plot_policy,    # Policy to be used for video creation\n",
    "    plot_id,\n",
    "    n_herds = num_herds,\n",
    "    total_pop = total_population,\n",
    "    num_episodes = 1):\n",
    "\n",
    "    plot_env = tf_py_environment.TFPyEnvironment(P_Env_P2_N(num_herds = n_herds, total_population = total_pop,\n",
    "                                                            root_dir = plot_dir,\n",
    "                                                         global_step = plot_id))\n",
    "    for _ in range (num_episodes):\n",
    "        time_step = plot_env.reset()\n",
    "        policy_init = plot_policy.get_initial_state(plot_env.batch_size)\n",
    "        policy_step = plot_policy.action(time_step, policy_init)\n",
    "        while True:\n",
    "            if time_step.is_last():\n",
    "                break\n",
    "            time_step = plot_env.step(policy_step.action)\n",
    "            policy_step = plot_policy.action(time_step, policy_step.state)\n",
    "        #plot_env.close()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-scotland",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_eval(\n",
    "    root_dir,\n",
    "    env_name='Env_P2_N',\n",
    "    n_herds = num_herds,\n",
    "    total_pop = total_population,\n",
    "    \n",
    "    eval_env_name=None,\n",
    "    env_load_fn=suite_mujoco.load,\n",
    "    num_iterations=500000,    #2000000\n",
    "    actor_fc_layers=(400, 300),\n",
    "    critic_obs_fc_layers=(400,),\n",
    "    critic_action_fc_layers=None,\n",
    "    critic_joint_fc_layers=(300,),\n",
    "    # Params for collect\n",
    "    initial_collect_steps=2000,     #1000\n",
    "    collect_steps_per_iteration=5,    #1\n",
    "    num_parallel_environments=1,\n",
    "    replay_buffer_capacity=100000,\n",
    "    ou_stddev=0.2,\n",
    "    ou_damping=0.15,\n",
    "    # Params for target update\n",
    "    target_update_tau=0.05,\n",
    "    target_update_period=5,\n",
    "    # Params for train\n",
    "    train_steps_per_iteration=1,\n",
    "    batch_size=64, #64\n",
    "    actor_learning_rate=1e-4,    #1e-4\n",
    "    critic_learning_rate=1e-3,    #1e-3\n",
    "    dqda_clipping=None,\n",
    "    td_errors_loss_fn=tf.compat.v1.losses.huber_loss,\n",
    "    gamma=0.99,\n",
    "    reward_scale_factor=1.0,\n",
    "    gradient_clipping=None,\n",
    "    use_tf_functions=True,\n",
    "    # Params for eval\n",
    "    num_eval_episodes=10,\n",
    "    eval_interval=10000,\n",
    "    # Params for checkpoints, summaries, and logging\n",
    "    log_interval=1000,\n",
    "    summary_interval=1000,\n",
    "    summaries_flush_secs=10,\n",
    "    debug_summaries=False,\n",
    "    summarize_grads_and_vars=False,\n",
    "    eval_metrics_callback=None):\n",
    "    \"\"\"A simple train and eval for DDPG.\"\"\"\n",
    "    root_dir = os.path.expanduser(root_dir)\n",
    "    train_dir = os.path.join(root_dir, 'train')\n",
    "    eval_dir = os.path.join(root_dir, 'eval')\n",
    "    plt_dir = os.path.join(root_dir, 'plt')\n",
    "\n",
    "    train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "        train_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "    train_summary_writer.set_as_default()\n",
    "\n",
    "    eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "        eval_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "    eval_metrics = [\n",
    "        tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n",
    "        tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n",
    "    ]\n",
    "\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "    with tf.compat.v2.summary.record_if(\n",
    "        lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
    "        if num_parallel_environments > 1:\n",
    "            tf_env = tf_py_environment.TFPyEnvironment(\n",
    "                parallel_py_environment.ParallelPyEnvironment(\n",
    "                    [lambda: env_load_fn(env_name)] * num_parallel_environments))\n",
    "        else:\n",
    "            tf_env = tf_py_environment.TFPyEnvironment(Env_P2_N(num_herds = n_herds, total_population = total_pop))\n",
    "            eval_env_name = eval_env_name or env_name\n",
    "            eval_tf_env = tf_py_environment.TFPyEnvironment(Env_P2_N(num_herds = n_herds, total_population = total_pop))\n",
    "\n",
    "    actor_net = create_actor_network(actor_fc_layers, tf_env.action_spec())\n",
    "    critic_net = create_critic_network(critic_obs_fc_layers,\n",
    "                                       critic_action_fc_layers,\n",
    "                                       critic_joint_fc_layers)\n",
    "\n",
    "    tf_agent = ddpg_agent.DdpgAgent(\n",
    "        tf_env.time_step_spec(),\n",
    "        tf_env.action_spec(),\n",
    "        actor_network=actor_net,\n",
    "        critic_network=critic_net,\n",
    "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=actor_learning_rate),\n",
    "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=critic_learning_rate),\n",
    "        ou_stddev=ou_stddev,\n",
    "        ou_damping=ou_damping,\n",
    "        target_update_tau=target_update_tau,\n",
    "        target_update_period=target_update_period,\n",
    "        dqda_clipping=dqda_clipping,\n",
    "        td_errors_loss_fn=td_errors_loss_fn,\n",
    "        gamma=gamma,\n",
    "        reward_scale_factor=reward_scale_factor,\n",
    "        gradient_clipping=gradient_clipping,\n",
    "        debug_summaries=debug_summaries,\n",
    "        summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "        train_step_counter=global_step)\n",
    "    tf_agent.initialize()\n",
    "\n",
    "    train_metrics = [\n",
    "        tf_metrics.NumberOfEpisodes(),\n",
    "        tf_metrics.EnvironmentSteps(),\n",
    "        tf_metrics.AverageReturnMetric(),\n",
    "        tf_metrics.AverageEpisodeLengthMetric(),\n",
    "    ]\n",
    "\n",
    "    eval_policy = tf_agent.policy\n",
    "    collect_policy = tf_agent.collect_policy\n",
    "\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        tf_agent.collect_data_spec,\n",
    "        batch_size=tf_env.batch_size,\n",
    "        max_length=replay_buffer_capacity)\n",
    "\n",
    "    initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        tf_env,\n",
    "        collect_policy,\n",
    "        observers=[replay_buffer.add_batch],\n",
    "        num_steps=initial_collect_steps)\n",
    "\n",
    "    collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        tf_env,\n",
    "        collect_policy,\n",
    "        observers=[replay_buffer.add_batch] + train_metrics,\n",
    "        num_steps=collect_steps_per_iteration)\n",
    "\n",
    "    if use_tf_functions:\n",
    "      initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "      collect_driver.run = common.function(collect_driver.run)\n",
    "      tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "    # Collect initial replay data.\n",
    "    logging.info(\n",
    "        'Initializing replay buffer by collecting experience for %d steps with '\n",
    "        'a random policy.', initial_collect_steps)\n",
    "    initial_collect_driver.run()\n",
    "    results = metric_utils.eager_compute(\n",
    "        eval_metrics,\n",
    "        eval_tf_env,\n",
    "        eval_policy,\n",
    "        num_episodes=num_eval_episodes,\n",
    "        train_step=global_step,\n",
    "        summary_writer=eval_summary_writer,\n",
    "        summary_prefix='Metrics',\n",
    "    )\n",
    "    if eval_metrics_callback is not None:\n",
    "      eval_metrics_callback(results, global_step.numpy())\n",
    "    metric_utils.log_metrics(eval_metrics)\n",
    "\n",
    "    time_step = None\n",
    "    policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n",
    "\n",
    "    timed_at_step = global_step.numpy()\n",
    "    time_acc = 0\n",
    "\n",
    "    # Dataset generates trajectories with shape [Bx2x...]\n",
    "    dataset = replay_buffer.as_dataset(\n",
    "        num_parallel_calls=3,\n",
    "        sample_batch_size=batch_size,\n",
    "        num_steps=2).prefetch(3)\n",
    "    iterator = iter(dataset)\n",
    "\n",
    "    def train_step():\n",
    "      experience, _ = next(iterator)\n",
    "      return tf_agent.train(experience)\n",
    "\n",
    "    if use_tf_functions:\n",
    "      train_step = common.function(train_step)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "      start_time = time.time()\n",
    "      time_step, policy_state = collect_driver.run(\n",
    "          time_step=time_step,\n",
    "          policy_state=policy_state,\n",
    "      )\n",
    "      for _ in range(train_steps_per_iteration):\n",
    "        train_loss = train_step()\n",
    "      time_acc += time.time() - start_time\n",
    "\n",
    "      if global_step.numpy() % log_interval == 0:\n",
    "        logging.info('step = %d, loss = %f', global_step.numpy(),\n",
    "                     train_loss.loss)\n",
    "        steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n",
    "        logging.info('%.3f steps/sec', steps_per_sec)\n",
    "        tf.compat.v2.summary.scalar(\n",
    "            name='global_steps_per_sec', data=steps_per_sec, step=global_step)\n",
    "        timed_at_step = global_step.numpy()\n",
    "        time_acc = 0\n",
    "\n",
    "      for train_metric in train_metrics:\n",
    "        train_metric.tf_summaries(\n",
    "            train_step=global_step, step_metrics=train_metrics[:2])\n",
    "\n",
    "      if global_step.numpy() % eval_interval == 0:\n",
    "        results = metric_utils.eager_compute(\n",
    "            eval_metrics,\n",
    "            eval_tf_env,\n",
    "            eval_policy,\n",
    "            num_episodes=num_eval_episodes,\n",
    "            train_step=global_step,\n",
    "            summary_writer=eval_summary_writer,\n",
    "            summary_prefix='Metrics',\n",
    "        )\n",
    "        if eval_metrics_callback is not None:\n",
    "          eval_metrics_callback(results, global_step.numpy())\n",
    "        metric_utils.log_metrics(eval_metrics)\n",
    "        if global_step.numpy() % (eval_interval) == 0:\n",
    "            plot_dir = os.path.join(root_dir, 'plots')\n",
    "            ep_length = create_plots(\n",
    "                n_herds = num_herds,\n",
    "                total_pop = total_population,\n",
    "                plot_dir = plot_dir,    # Directory for plot output\n",
    "                plot_policy = eval_policy,    # Policy to be used for plot creation\n",
    "                plot_id = global_step.numpy(),\n",
    "                num_episodes = 2)\n",
    "        \n",
    "\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "dense = functools.partial(\n",
    "    tf.keras.layers.Dense,\n",
    "    activation=tf.keras.activations.relu,\n",
    "    kernel_initializer=tf.compat.v1.variance_scaling_initializer(\n",
    "        scale=1./ 3.0, mode='fan_in', distribution='uniform'))\n",
    "\n",
    "\n",
    "def create_identity_layer():\n",
    "  return tf.keras.layers.Lambda(lambda x: x)\n",
    "\n",
    "\n",
    "def create_fc_network(layer_units):\n",
    "  return sequential.Sequential([dense(num_units) for num_units in layer_units])\n",
    "\n",
    "\n",
    "def create_actor_network(fc_layer_units, action_spec):\n",
    "  \"\"\"Create an actor network for DDPG.\"\"\"\n",
    "  flat_action_spec = tf.nest.flatten(action_spec)\n",
    "  if len(flat_action_spec) > 1:\n",
    "    raise ValueError('Only a single action tensor is supported by this network')\n",
    "  flat_action_spec = flat_action_spec[0]\n",
    "\n",
    "  fc_layers = [dense(num_units) for num_units in fc_layer_units]\n",
    "\n",
    "  num_actions = flat_action_spec.shape.num_elements()\n",
    "  action_fc_layer = tf.keras.layers.Dense(\n",
    "      num_actions,\n",
    "      activation=tf.keras.activations.tanh,\n",
    "      kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "          minval=-0.003, maxval=0.003))\n",
    "\n",
    "  scaling_layer = tf.keras.layers.Lambda(\n",
    "      lambda x: common.scale_to_spec(x, flat_action_spec))\n",
    "  return sequential.Sequential(fc_layers + [action_fc_layer, scaling_layer])\n",
    "\n",
    "\n",
    "def create_critic_network(obs_fc_layer_units,\n",
    "                          action_fc_layer_units,\n",
    "                          joint_fc_layer_units):\n",
    "  \"\"\"Create a critic network for DDPG.\"\"\"\n",
    "\n",
    "  def split_inputs(inputs):\n",
    "    return {'observation': inputs[0], 'action': inputs[1]}\n",
    "\n",
    "  obs_network = create_fc_network(\n",
    "      obs_fc_layer_units) if obs_fc_layer_units else create_identity_layer()\n",
    "  action_network = create_fc_network(\n",
    "      action_fc_layer_units\n",
    "  ) if action_fc_layer_units else create_identity_layer()\n",
    "  joint_network = create_fc_network(\n",
    "      joint_fc_layer_units) if joint_fc_layer_units else create_identity_layer(\n",
    "      )\n",
    "  value_fc_layer = tf.keras.layers.Dense(\n",
    "      1,\n",
    "      activation=None,\n",
    "      kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "          minval=-0.003, maxval=0.003))\n",
    "\n",
    "  return sequential.Sequential([\n",
    "      tf.keras.layers.Lambda(split_inputs),\n",
    "      nest_map.NestMap({\n",
    "          'observation': obs_network,\n",
    "          'action': action_network\n",
    "      }),\n",
    "      nest_map.NestFlatten(),\n",
    "      tf.keras.layers.Concatenate(),\n",
    "      joint_network,\n",
    "      value_fc_layer,\n",
    "      inner_reshape.InnerReshape([1], [])\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
