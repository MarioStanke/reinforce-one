{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the environment `HerdEnv` of `herd_env.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "from tf_agents.trajectories import TimeStep\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from herd_env import HerdEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "henv_val = HerdEnv(herd_sizes = [64,64], rand_recovery_prob = 0.1, rand_infection_prob = 0.05)\n",
    "utils.validate_py_environment(henv_val, episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Herd Environment instance to be trained for\n",
    "max_episode_length=100\n",
    "num_herds=2\n",
    "henv = HerdEnv(herd_sizes = [32,32], expected_episode_length=-1, max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.04, rand_infection_prob = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': 1.0,\n",
      " 'observation': array([0., 0., 0.], dtype=float32),\n",
      " 'reward': 0.0,\n",
      " 'step_type': array(0, dtype=int32)})\n",
      "state:  [1 0 1 0 1 1] observation:  [0.01 0.01 0.01] \treward:  0.0\n",
      "state:  [4 1 5 1 2 2] observation:  [0.02 0.02 0.02] \treward:  0.0\n",
      "state:  [ 5  3 10  4  3  3] observation:  [0.03 0.03 0.03] \treward:  0.0\n",
      "state:  [ 5  4 15  8  4  4] observation:  [0.04 0.04 0.04] \treward:  0.0\n",
      "state:  [ 7  4 22 12  5  5] observation:  [0.05 0.05 0.05] \treward:  0.0\n",
      "state:  [ 9  7 31 19  6  6] observation:  [0.06 0.06 0.06] \treward:  0.0\n",
      "state:  [ 9  8 40 27  7  7] observation:  [0.07 0.07 0.07] \treward:  0.0\n",
      "state:  [11 10 51 37  8  8] observation:  [0.08 0.08 0.08] \treward:  0.0\n",
      "state:  [17 10 68 47  9  9] observation:  [0.09 0.09 0.09] \treward:  0.0\n",
      "state:  [18 13 86 60 10 10] observation:  [0.1 0.1 0.1] \treward:  0.0\n",
      "state:  [ 18  14 104  74  11  11] observation:  [0.11 0.11 0.11] \treward:  0.0\n",
      "state:  [ 17  19 121  93  12  12] observation:  [0.12 0.12 0.12] \treward:  0.0\n",
      "state:  [ 16  20 137 113  13  13] observation:  [0.13 0.13 0.13] \treward:  0.0\n",
      "state:  [ 17  22 154 135  14  14] observation:  [0.14 0.14 0.14] \treward:  0.0\n",
      "state:  [ 17  21 171 156  15  15] observation:  [0.15 0.15 0.15] \treward:  0.0\n",
      "state:  [ 17  22 188 178  16  16] observation:  [0.16 0.16 0.16] \treward:  0.0\n",
      "state:  [ 18  24 206 202  17  17] observation:  [0.17 0.17 0.17] \treward:  0.0\n",
      "state:  [ 18  26 224 228  18  18] observation:  [0.18 0.18 0.18] \treward:  0.0\n",
      "state:  [ 17  26 241 254  19  19] observation:  [0.19 0.19 0.19] \treward:  0.0\n",
      "state:  [ 16  25 257 279  20  20] observation:  [0.2 0.2 0.2] \treward:  0.0\n",
      "state:  [ 17  25 274 304  21  21] observation:  [0.21 0.21 0.21] \treward:  0.0\n",
      "state:  [ 17  22 291 326  22  22] observation:  [0.22 0.22 0.22] \treward:  0.0\n",
      "state:  [ 17  22 308 348  23  23] observation:  [0.23 0.23 0.23] \treward:  0.0\n",
      "state:  [ 15  22 323 370  24  24] observation:  [0.24 0.24 0.24] \treward:  0.0\n",
      "state:  [ 16  22 339 392  25  25] observation:  [0.25 0.25 0.25] \treward:  0.0\n",
      "state:  [ 15  23 354 415  26  26] observation:  [0.26 0.26 0.26] \treward:  0.0\n",
      "state:  [ 16  22 370 437  27  27] observation:  [0.27 0.27 0.27] \treward:  0.0\n",
      "state:  [ 18  24 388 461  28  28] observation:  [0.28 0.28 0.28] \treward:  0.0\n",
      "state:  [ 17  24 405 485  29  29] observation:  [0.29 0.29 0.29] \treward:  0.0\n",
      "state:  [ 20  25 425 510  30  30] observation:  [0.3 0.3 0.3] \treward:  0.0\n",
      "state:  [ 20  26 445 536  31  31] observation:  [0.31 0.31 0.31] \treward:  0.0\n",
      "state:  [ 21  25 466 561  32  32] observation:  [0.32 0.32 0.32] \treward:  0.0\n",
      "state:  [ 20  25 486 586  33  33] observation:  [0.33 0.33 0.33] \treward:  0.0\n",
      "state:  [ 20  24 506 610  34  34] observation:  [0.34 0.34 0.34] \treward:  0.0\n",
      "state:  [ 20  22 526 632  35  35] observation:  [0.35 0.35 0.35] \treward:  0.0\n",
      "state:  [ 23  25 549 657  36  36] observation:  [0.36 0.36 0.36] \treward:  0.0\n",
      "state:  [ 23  23 572 680  37  37] observation:  [0.37 0.37 0.37] \treward:  0.0\n",
      "state:  [ 23  23 595 703  38  38] observation:  [0.38 0.38 0.38] \treward:  0.0\n",
      "state:  [ 23  24 618 727  39  39] observation:  [0.39 0.39 0.39] \treward:  0.0\n",
      "state:  [ 23  24 641 751  40  40] observation:  [0.4 0.4 0.4] \treward:  0.0\n",
      "state:  [ 19  24 660 775  41  41] observation:  [0.41 0.41 0.41] \treward:  0.0\n",
      "state:  [ 20  23 680 798  42  42] observation:  [0.42 0.42 0.42] \treward:  0.0\n",
      "state:  [ 18  24 698 822  43  43] observation:  [0.43 0.43 0.43] \treward:  0.0\n",
      "state:  [ 17  25 715 847  44  44] observation:  [0.44 0.44 0.44] \treward:  0.0\n",
      "state:  [ 14  24 729 871  45  45] observation:  [0.45 0.45 0.45] \treward:  0.0\n",
      "state:  [ 14  24 743 895  46  46] observation:  [0.46 0.46 0.46] \treward:  0.0\n",
      "state:  [ 15  24 758 919  47  47] observation:  [0.47 0.47 0.47] \treward:  0.0\n",
      "state:  [ 15  26 773 945  48  48] observation:  [0.48 0.48 0.48] \treward:  0.0\n",
      "state:  [ 16  24 789 969  49  49] observation:  [0.49 0.49 0.49] \treward:  0.0\n",
      "state:  [ 17  23 806 992  50  50] observation:  [0.5 0.5 0.5] \treward:  0.0\n",
      "state:  [  18   22  824 1014   51   51] observation:  [0.51 0.51 0.51] \treward:  0.0\n",
      "state:  [  17   21  841 1035   52   52] observation:  [0.52 0.52 0.52] \treward:  0.0\n",
      "state:  [  20   21  861 1056   53   53] observation:  [0.53 0.53 0.53] \treward:  0.0\n",
      "state:  [  21   20  882 1076   54   54] observation:  [0.54 0.54 0.54] \treward:  0.0\n",
      "state:  [  19   20  901 1096   55   55] observation:  [0.55 0.55 0.55] \treward:  0.0\n",
      "state:  [  21   19  922 1115   56   56] observation:  [0.56 0.56 0.56] \treward:  0.0\n",
      "state:  [  21   18  943 1133   57   57] observation:  [0.57 0.57 0.57] \treward:  0.0\n",
      "state:  [  22   17  965 1150   58   58] observation:  [0.58 0.58 0.58] \treward:  0.0\n",
      "state:  [  22   16  987 1166   59   59] observation:  [0.59 0.59 0.59] \treward:  0.0\n",
      "state:  [  22   17 1009 1183   60   60] observation:  [0.6 0.6 0.6] \treward:  0.0\n",
      "state:  [  22   18 1031 1201   61   61] observation:  [0.61 0.61 0.61] \treward:  0.0\n",
      "state:  [  23   18 1054 1219   62   62] observation:  [0.62 0.62 0.62] \treward:  0.0\n",
      "state:  [  23   18 1077 1237   63   63] observation:  [0.63 0.63 0.63] \treward:  0.0\n",
      "state:  [  24   19 1101 1256   64   64] observation:  [0.64 0.64 0.64] \treward:  0.0\n",
      "state:  [  23   19 1124 1275   65   65] observation:  [0.65 0.65 0.65] \treward:  0.0\n",
      "state:  [  23   22 1147 1297   66   66] observation:  [0.66 0.66 0.66] \treward:  0.0\n",
      "state:  [  23   23 1170 1320   67   67] observation:  [0.67 0.67 0.67] \treward:  0.0\n",
      "state:  [  23   23 1193 1343   68   68] observation:  [0.68 0.68 0.68] \treward:  0.0\n",
      "state:  [  22   24 1215 1367   69   69] observation:  [0.69 0.69 0.69] \treward:  0.0\n",
      "state:  [  21   25 1236 1392   70   70] observation:  [0.7 0.7 0.7] \treward:  0.0\n",
      "state:  [  22   25 1258 1417   71   71] observation:  [0.71 0.71 0.71] \treward:  0.0\n",
      "state:  [  22   24 1280 1441   72   72] observation:  [0.72 0.72 0.72] \treward:  0.0\n",
      "state:  [  21   23 1301 1464   73   73] observation:  [0.73 0.73 0.73] \treward:  0.0\n",
      "state:  [  21   23 1322 1487   74   74] observation:  [0.74 0.74 0.74] \treward:  0.0\n",
      "state:  [  21   22 1343 1509   75   75] observation:  [0.75 0.75 0.75] \treward:  0.0\n",
      "state:  [  22   24 1365 1533   76   76] observation:  [0.76 0.76 0.76] \treward:  0.0\n",
      "state:  [  22   23 1387 1556   77   77] observation:  [0.77 0.77 0.77] \treward:  0.0\n",
      "state:  [  22   23 1409 1579   78   78] observation:  [0.78 0.78 0.78] \treward:  0.0\n",
      "state:  [  23   23 1432 1602   79   79] observation:  [0.79 0.79 0.79] \treward:  0.0\n",
      "state:  [  24   24 1456 1626   80   80] observation:  [0.8 0.8 0.8] \treward:  0.0\n",
      "state:  [  24   23 1480 1649   81   81] observation:  [0.81 0.81 0.81] \treward:  0.0\n",
      "state:  [  22   23 1502 1672   82   82] observation:  [0.82 0.82 0.82] \treward:  0.0\n",
      "state:  [  23   24 1525 1696   83   83] observation:  [0.83 0.83 0.83] \treward:  0.0\n",
      "state:  [  25   24 1550 1720   84   84] observation:  [0.84 0.84 0.84] \treward:  0.0\n",
      "state:  [  23   22 1573 1742   85   85] observation:  [0.85 0.85 0.85] \treward:  0.0\n",
      "state:  [  21   23 1594 1765   86   86] observation:  [0.86 0.86 0.86] \treward:  0.0\n",
      "state:  [  19   25 1613 1790   87   87] observation:  [0.87 0.87 0.87] \treward:  0.0\n",
      "state:  [  18   26 1631 1816   88   88] observation:  [0.88 0.88 0.88] \treward:  0.0\n",
      "state:  [  17   27 1648 1843   89   89] observation:  [0.89 0.89 0.89] \treward:  0.0\n",
      "state:  [  16   25 1664 1868   90   90] observation:  [0.9 0.9 0.9] \treward:  0.0\n",
      "state:  [  16   26 1680 1894   91   91] observation:  [0.91 0.91 0.91] \treward:  0.0\n",
      "state:  [  17   27 1697 1921   92   92] observation:  [0.92 0.92 0.92] \treward:  0.0\n",
      "state:  [  18   27 1715 1948   93   93] observation:  [0.93 0.93 0.93] \treward:  0.0\n",
      "state:  [  19   23 1734 1971   94   94] observation:  [0.94 0.94 0.94] \treward:  0.0\n",
      "state:  [  20   22 1754 1993   95   95] observation:  [0.95 0.95 0.95] \treward:  0.0\n",
      "state:  [  21   22 1775 2015   96   96] observation:  [0.96 0.96 0.96] \treward:  0.0\n",
      "state:  [  22   22 1797 2037   97   97] observation:  [0.97 0.97 0.97] \treward:  0.0\n",
      "state:  [  21   19 1818 2056   98   98] observation:  [0.98 0.98 0.98] \treward:  0.0\n",
      "state:  [  21   21 1839 2077   99   99] observation:  [0.99 0.99 0.99] \treward:  0.0\n",
      "state:  [  22   22 1861 2099  100  100] observation:  [1. 1. 1.] \treward:  -1980.0\n",
      "Final Reward =  -1980.0\n"
     ]
    }
   ],
   "source": [
    "# show interor values of environment\n",
    "time_step = henv.reset()\n",
    "print(time_step)\n",
    "cumulative_reward = time_step.reward\n",
    "finished = False\n",
    "\n",
    "while not finished:\n",
    "  time_step = henv.step(0) # do nothing\n",
    "  s = henv.get_state()\n",
    "  print(\"state: \", s, \"observation: \", time_step.observation, \"\\treward: \", time_step.reward)\n",
    "  cumulative_reward += time_step.reward\n",
    "  if time_step.step_type == StepType.LAST:\n",
    "    finished = True\n",
    "\n",
    "print('Final Reward = ', cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action spec:\n",
      " BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=3) \n",
      "\n",
      "time step spec:\n",
      " TimeStep(\n",
      "{'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
      " 'observation': BoundedArraySpec(shape=(3,), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=1.0),\n",
      " 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n",
      " 'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "action_spec = henv.action_spec()\n",
    "ts_spec = henv.time_step_spec()\n",
    "print(\"action spec:\\n\", action_spec, \"\\n\\ntime step spec:\\n\", ts_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scripted policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do nothing policy: cull never\n",
    "action_script0 = [(max_episode_length, 0)]\n",
    "\n",
    "# cull first herd every 20th step and second herd every 20th step\n",
    "action_script1 = [(19, 0), \n",
    "                 (1, 1),\n",
    "                 (19, 0), \n",
    "                 (1, 3)] * int(1+max_episode_length/40)\n",
    "\n",
    "manual_scripted_policy0 = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=ts_spec,\n",
    "    action_spec=action_spec,\n",
    "    action_script=action_script0)\n",
    "\n",
    "manual_scripted_policy1 = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=ts_spec,\n",
    "    action_spec=action_spec,\n",
    "    action_script=action_script1)\n",
    "\n",
    "init_policy_state = manual_scripted_policy0.get_initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action= 0 \tpolicy_state [0, 1]\n",
      "action= 0 \tpolicy_state [0, 2]\n",
      "action= 0 \tpolicy_state [0, 3]\n",
      "action= 0 \tpolicy_state [0, 4]\n",
      "action= 0 \tpolicy_state [0, 5]\n",
      "action= 0 \tpolicy_state [0, 6]\n",
      "action= 0 \tpolicy_state [0, 7]\n",
      "action= 0 \tpolicy_state [0, 8]\n",
      "action= 0 \tpolicy_state [0, 9]\n",
      "action= 0 \tpolicy_state [0, 10]\n",
      "action= 0 \tpolicy_state [0, 11]\n",
      "action= 0 \tpolicy_state [0, 12]\n",
      "action= 0 \tpolicy_state [0, 13]\n",
      "action= 0 \tpolicy_state [0, 14]\n",
      "action= 0 \tpolicy_state [0, 15]\n",
      "action= 0 \tpolicy_state [0, 16]\n",
      "action= 0 \tpolicy_state [0, 17]\n",
      "action= 0 \tpolicy_state [0, 18]\n",
      "action= 0 \tpolicy_state [0, 19]\n",
      "action= 1 \tpolicy_state [1, 1]\n",
      "action= 0 \tpolicy_state [2, 1]\n"
     ]
    }
   ],
   "source": [
    "policy_state =  init_policy_state\n",
    "ts0 = henv.reset()\n",
    "for _ in range(21):\n",
    "    action_step = manual_scripted_policy1.action(ts0, policy_state)\n",
    "    policy_state = action_step.state\n",
    "    print(\"action=\", action_step.action, \"\\tpolicy_state\", policy_state)\n",
    "policy_state = manual_scripted_policy1.get_initial_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_py_policy.RandomPyPolicy(time_step_spec=ts_spec, action_spec=action_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drive a rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=50, verbose=False):\n",
    "  total_return = 0.0\n",
    "  cullsteps = 0\n",
    "  for e in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "        policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "    else:\n",
    "        policy_state = None # other policies without memory\n",
    "    episode_return = 0.0\n",
    "    i=0\n",
    "    while not time_step.is_last():\n",
    "        i+=1\n",
    "        action_step = policy.action(time_step, policy_state)\n",
    "        if action_step.action > 0:\n",
    "            cullsteps += 1\n",
    "        policy_state = action_step.state\n",
    "        time_step = environment.step(action_step.action)\n",
    "        if isinstance(environment, HerdEnv):\n",
    "            state = environment.get_state()\n",
    "        else:\n",
    "            state = None # TF environment from wrapper does not have get_state()\n",
    "        episode_return += time_step.reward\n",
    "        if verbose:\n",
    "            print (f\"episode {e:>2} step{i:>4} action: \", action_step.action, \"state=\", state, \"obs=\", time_step.observation, \"reward=\", time_step.reward)\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  cullsteps /= num_episodes\n",
    "  return avg_return, cullsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average return of random policy: -3366.370 avg steps with culls per episode: 75.34\n"
     ]
    }
   ],
   "source": [
    "random_reward, cullsteps = compute_avg_return(henv, random_policy)\n",
    "print (f\"average return of random policy: {random_reward:.3f} avg steps with culls per episode: {cullsteps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 step   1 action:  0 state= [0 5 0 5 1 1] obs= [0.01 0.01 0.01] reward= 0.0\n",
      "episode  0 step   2 action:  0 state= [ 4  7  4 12  2  2] obs= [0.02 0.02 0.02] reward= 0.0\n",
      "episode  0 step   3 action:  0 state= [ 8  8 12 20  3  3] obs= [0.03 0.03 0.03] reward= 0.0\n",
      "episode  0 step   4 action:  0 state= [ 8  9 20 29  4  4] obs= [0.04 0.04 0.04] reward= 0.0\n",
      "episode  0 step   5 action:  0 state= [10 10 30 39  5  5] obs= [0.05 0.05 0.05] reward= 0.0\n",
      "episode  0 step   6 action:  0 state= [10 13 40 52  6  6] obs= [0.06 0.06 0.06] reward= 0.0\n",
      "episode  0 step   7 action:  0 state= [11 11 51 63  7  7] obs= [0.07 0.07 0.07] reward= 0.0\n",
      "episode  0 step   8 action:  0 state= [14 10 65 73  8  8] obs= [0.08 0.08 0.08] reward= 0.0\n",
      "episode  0 step   9 action:  0 state= [16 13 81 86  9  9] obs= [0.09 0.09 0.09] reward= 0.0\n",
      "episode  0 step  10 action:  0 state= [ 17  16  98 102  10  10] obs= [0.1 0.1 0.1] reward= 0.0\n",
      "episode  0 step  11 action:  0 state= [ 16  14 114 116  11  11] obs= [0.11 0.11 0.11] reward= 0.0\n",
      "episode  0 step  12 action:  0 state= [ 16  15 130 131  12  12] obs= [0.12 0.12 0.12] reward= 0.0\n",
      "episode  0 step  13 action:  0 state= [ 18  15 148 146  13  13] obs= [0.13 0.13 0.13] reward= 0.0\n",
      "episode  0 step  14 action:  0 state= [ 18  17 166 163  14  14] obs= [0.14 0.14 0.14] reward= 0.0\n",
      "episode  0 step  15 action:  0 state= [ 18  15 184 178  15  15] obs= [0.15 0.15 0.15] reward= 0.0\n",
      "episode  0 step  16 action:  0 state= [ 17  16 201 194  16  16] obs= [0.16 0.16 0.16] reward= 0.0\n",
      "episode  0 step  17 action:  0 state= [ 16  15 217 209  17  17] obs= [0.17 0.17 0.17] reward= 0.0\n",
      "episode  0 step  18 action:  0 state= [ 16  16 233 225  18  18] obs= [0.18 0.18 0.18] reward= 0.0\n",
      "episode  0 step  19 action:  0 state= [ 16  17 249 242  19  19] obs= [0.19 0.19 0.19] reward= 0.0\n",
      "episode  0 step  20 action:  1 state= [  0  17 249 259   0  20] obs= [0.2 0.  0.2] reward= -32.0\n",
      "episode  0 step  21 action:  0 state= [  1  15 250 274   1  21] obs= [0.21 0.01 0.21] reward= 0.0\n",
      "episode  0 step  22 action:  0 state= [  4  14 254 288   2  22] obs= [0.22 0.02 0.22] reward= 0.0\n",
      "episode  0 step  23 action:  0 state= [  7  17 261 305   3  23] obs= [0.23 0.03 0.23] reward= 0.0\n",
      "episode  0 step  24 action:  0 state= [  7  18 268 323   4  24] obs= [0.24 0.04 0.24] reward= 0.0\n",
      "episode  0 step  25 action:  0 state= [  7  18 275 341   5  25] obs= [0.25 0.05 0.25] reward= 0.0\n",
      "episode  0 step  26 action:  0 state= [  8  19 283 360   6  26] obs= [0.26 0.06 0.26] reward= 0.0\n",
      "episode  0 step  27 action:  0 state= [  7  20 290 380   7  27] obs= [0.27 0.07 0.27] reward= 0.0\n",
      "episode  0 step  28 action:  0 state= [  7  17 297 397   8  28] obs= [0.28 0.08 0.28] reward= 0.0\n",
      "episode  0 step  29 action:  0 state= [  9  16 306 413   9  29] obs= [0.29 0.09 0.29] reward= 0.0\n",
      "episode  0 step  30 action:  0 state= [ 10  17 316 430  10  30] obs= [0.3 0.1 0.3] reward= 0.0\n",
      "episode  0 step  31 action:  0 state= [ 11  18 327 448  11  31] obs= [0.31 0.11 0.31] reward= 0.0\n",
      "episode  0 step  32 action:  0 state= [ 11  16 338 464  12  32] obs= [0.32 0.12 0.32] reward= 0.0\n",
      "episode  0 step  33 action:  0 state= [ 15  17 353 481  13  33] obs= [0.33 0.13 0.33] reward= 0.0\n",
      "episode  0 step  34 action:  0 state= [ 14  20 367 501  14  34] obs= [0.34 0.14 0.34] reward= 0.0\n",
      "episode  0 step  35 action:  0 state= [ 13  24 380 525  15  35] obs= [0.35 0.15 0.35] reward= 0.0\n",
      "episode  0 step  36 action:  0 state= [ 14  22 394 547  16  36] obs= [0.36 0.16 0.36] reward= 0.0\n",
      "episode  0 step  37 action:  0 state= [ 17  21 411 568  17  37] obs= [0.37 0.17 0.37] reward= 0.0\n",
      "episode  0 step  38 action:  0 state= [ 18  21 429 589  18  38] obs= [0.38 0.18 0.38] reward= 0.0\n",
      "episode  0 step  39 action:  0 state= [ 19  21 448 610  19  39] obs= [0.39 0.19 0.39] reward= 0.0\n",
      "episode  0 step  40 action:  3 state= [  0   0 448 610   0   0] obs= [0.4 0.  0. ] reward= -64.0\n",
      "episode  0 step  41 action:  0 state= [  0   0 448 610   1   1] obs= [0.41 0.01 0.01] reward= 0.0\n",
      "episode  0 step  42 action:  0 state= [  4   3 452 613   2   2] obs= [0.42 0.02 0.02] reward= 0.0\n",
      "episode  0 step  43 action:  0 state= [  4   3 456 616   3   3] obs= [0.43 0.03 0.03] reward= 0.0\n",
      "episode  0 step  44 action:  0 state= [  4   3 460 619   4   4] obs= [0.44 0.04 0.04] reward= 0.0\n",
      "episode  0 step  45 action:  0 state= [  7   6 467 625   5   5] obs= [0.45 0.05 0.05] reward= 0.0\n",
      "episode  0 step  46 action:  0 state= [ 10   7 477 632   6   6] obs= [0.46 0.06 0.06] reward= 0.0\n",
      "episode  0 step  47 action:  0 state= [ 11   9 488 641   7   7] obs= [0.47 0.07 0.07] reward= 0.0\n",
      "episode  0 step  48 action:  0 state= [ 14   9 502 650   8   8] obs= [0.48 0.08 0.08] reward= 0.0\n",
      "episode  0 step  49 action:  0 state= [ 15   9 517 659   9   9] obs= [0.49 0.09 0.09] reward= 0.0\n",
      "episode  0 step  50 action:  0 state= [ 17  12 534 671  10  10] obs= [0.5 0.1 0.1] reward= 0.0\n",
      "episode  0 step  51 action:  0 state= [ 17  14 551 685  11  11] obs= [0.51 0.11 0.11] reward= 0.0\n",
      "episode  0 step  52 action:  0 state= [ 19  14 570 699  12  12] obs= [0.52 0.12 0.12] reward= 0.0\n",
      "episode  0 step  53 action:  0 state= [ 22  14 592 713  13  13] obs= [0.53 0.13 0.13] reward= 0.0\n",
      "episode  0 step  54 action:  0 state= [ 22  16 614 729  14  14] obs= [0.54 0.14 0.14] reward= 0.0\n",
      "episode  0 step  55 action:  0 state= [ 21  15 635 744  15  15] obs= [0.55 0.15 0.15] reward= 0.0\n",
      "episode  0 step  56 action:  0 state= [ 23  14 658 758  16  16] obs= [0.56 0.16 0.16] reward= 0.0\n",
      "episode  0 step  57 action:  0 state= [ 20  13 678 771  17  17] obs= [0.57 0.17 0.17] reward= 0.0\n",
      "episode  0 step  58 action:  0 state= [ 19  13 697 784  18  18] obs= [0.58 0.18 0.18] reward= 0.0\n",
      "episode  0 step  59 action:  0 state= [ 21  14 718 798  19  19] obs= [0.59 0.19 0.19] reward= 0.0\n",
      "episode  0 step  60 action:  1 state= [  0  14 718 812   0  20] obs= [0.6 0.  0.2] reward= -32.0\n",
      "episode  0 step  61 action:  0 state= [  0  16 718 828   1  21] obs= [0.61 0.01 0.21] reward= 0.0\n",
      "episode  0 step  62 action:  0 state= [  2  16 720 844   2  22] obs= [0.62 0.02 0.22] reward= 0.0\n",
      "episode  0 step  63 action:  0 state= [  4  16 724 860   3  23] obs= [0.63 0.03 0.23] reward= 0.0\n",
      "episode  0 step  64 action:  0 state= [  5  14 729 874   4  24] obs= [0.64 0.04 0.24] reward= 0.0\n",
      "episode  0 step  65 action:  0 state= [  9  13 738 887   5  25] obs= [0.65 0.05 0.25] reward= 0.0\n",
      "episode  0 step  66 action:  0 state= [  9  13 747 900   6  26] obs= [0.66 0.06 0.26] reward= 0.0\n",
      "episode  0 step  67 action:  0 state= [  9  13 756 913   7  27] obs= [0.67 0.07 0.27] reward= 0.0\n",
      "episode  0 step  68 action:  0 state= [ 11  15 767 928   8  28] obs= [0.68 0.08 0.28] reward= 0.0\n",
      "episode  0 step  69 action:  0 state= [ 13  16 780 944   9  29] obs= [0.69 0.09 0.29] reward= 0.0\n",
      "episode  0 step  70 action:  0 state= [ 14  16 794 960  10  30] obs= [0.7 0.1 0.3] reward= 0.0\n",
      "episode  0 step  71 action:  0 state= [ 15  18 809 978  11  31] obs= [0.71 0.11 0.31] reward= 0.0\n",
      "episode  0 step  72 action:  0 state= [ 15  17 824 995  12  32] obs= [0.72 0.12 0.32] reward= 0.0\n",
      "episode  0 step  73 action:  0 state= [  17   18  841 1013   13   33] obs= [0.73 0.13 0.33] reward= 0.0\n",
      "episode  0 step  74 action:  0 state= [  17   18  858 1031   14   34] obs= [0.74 0.14 0.34] reward= 0.0\n",
      "episode  0 step  75 action:  0 state= [  18   20  876 1051   15   35] obs= [0.75 0.15 0.35] reward= 0.0\n",
      "episode  0 step  76 action:  0 state= [  17   20  893 1071   16   36] obs= [0.76 0.16 0.36] reward= 0.0\n",
      "episode  0 step  77 action:  0 state= [  18   21  911 1092   17   37] obs= [0.77 0.17 0.37] reward= 0.0\n",
      "episode  0 step  78 action:  0 state= [  17   22  928 1114   18   38] obs= [0.78 0.18 0.38] reward= 0.0\n",
      "episode  0 step  79 action:  0 state= [  17   21  945 1135   19   39] obs= [0.79 0.19 0.39] reward= 0.0\n",
      "episode  0 step  80 action:  3 state= [   0    0  945 1135    0    0] obs= [0.8 0.  0. ] reward= -64.0\n",
      "episode  0 step  81 action:  0 state= [   1    2  946 1137    1    1] obs= [0.81 0.01 0.01] reward= 0.0\n",
      "episode  0 step  82 action:  0 state= [   0    5  946 1142    2    2] obs= [0.82 0.02 0.02] reward= 0.0\n",
      "episode  0 step  83 action:  0 state= [   3    6  949 1148    3    3] obs= [0.83 0.03 0.03] reward= 0.0\n",
      "episode  0 step  84 action:  0 state= [   5    9  954 1157    4    4] obs= [0.84 0.04 0.04] reward= 0.0\n",
      "episode  0 step  85 action:  0 state= [   8    9  962 1166    5    5] obs= [0.85 0.05 0.05] reward= 0.0\n",
      "episode  0 step  86 action:  0 state= [   8   10  970 1176    6    6] obs= [0.86 0.06 0.06] reward= 0.0\n",
      "episode  0 step  87 action:  0 state= [  11   13  981 1189    7    7] obs= [0.87 0.07 0.07] reward= 0.0\n",
      "episode  0 step  88 action:  0 state= [  12   13  993 1202    8    8] obs= [0.88 0.08 0.08] reward= 0.0\n",
      "episode  0 step  89 action:  0 state= [  12   13 1005 1215    9    9] obs= [0.89 0.09 0.09] reward= 0.0\n",
      "episode  0 step  90 action:  0 state= [  12   16 1017 1231   10   10] obs= [0.9 0.1 0.1] reward= 0.0\n",
      "episode  0 step  91 action:  0 state= [  12   18 1029 1249   11   11] obs= [0.91 0.11 0.11] reward= 0.0\n",
      "episode  0 step  92 action:  0 state= [  12   19 1041 1268   12   12] obs= [0.92 0.12 0.12] reward= 0.0\n",
      "episode  0 step  93 action:  0 state= [  12   19 1053 1287   13   13] obs= [0.93 0.13 0.13] reward= 0.0\n",
      "episode  0 step  94 action:  0 state= [  13   18 1066 1305   14   14] obs= [0.94 0.14 0.14] reward= 0.0\n",
      "episode  0 step  95 action:  0 state= [  14   17 1080 1322   15   15] obs= [0.95 0.15 0.15] reward= 0.0\n",
      "episode  0 step  96 action:  0 state= [  14   20 1094 1342   16   16] obs= [0.96 0.16 0.16] reward= 0.0\n",
      "episode  0 step  97 action:  0 state= [  13   22 1107 1364   17   17] obs= [0.97 0.17 0.17] reward= 0.0\n",
      "episode  0 step  98 action:  0 state= [  13   23 1120 1387   18   18] obs= [0.98 0.18 0.18] reward= 0.0\n",
      "episode  0 step  99 action:  0 state= [  14   22 1134 1409   19   19] obs= [0.99 0.19 0.19] reward= 0.0\n",
      "episode  0 step 100 action:  1 state= [   0   21 1134 1430    0   20] obs= [1.  0.  0.2] reward= -1314.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1506.0, 5.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show states for one rollout of second scripted policy\n",
    "compute_avg_return(henv, manual_scripted_policy1, num_episodes=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average return of do-nothing-policy: -1940.193 avg culls 0.0\n",
      "average return of manual policy: -1510.665 avg culls 5.0\n"
     ]
    }
   ],
   "source": [
    "manual_reward0, cullsteps = compute_avg_return(henv, manual_scripted_policy0, num_episodes=500)\n",
    "print (f\"average return of do-nothing-policy: {manual_reward0:.3f} avg culls {cullsteps}\")\n",
    "manual_reward1, cullsteps = compute_avg_return(henv, manual_scripted_policy1, num_episodes=500)\n",
    "print (f\"average return of manual policy: {manual_reward1:.3f} avg culls {cullsteps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Deep-Q Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks.sequential import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 30000\n",
    "replay_buffer_max_length = 10000\n",
    "batch_size = 64\n",
    "num_eval_episodes = 100\n",
    "initial_collect_steps = 100\n",
    "collect_steps_per_iteration = 100\n",
    "log_interval = 200\n",
    "eval_interval = 1000\n",
    "target_update_period = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make actor network simple\n",
    "num_actions = 2**num_herds # this does not scale, obviously\n",
    "kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03)\n",
    "\n",
    "#q_net = Sequential([Dense(300, activation='tanh'),\n",
    "#                    Dense(4, activation=None,\n",
    "#                          kernel_initializer = kernel_initializer)])\n",
    "\n",
    "# from tutorial\n",
    "fc_layer_params = (100, 50)\n",
    "action_tensor_spec = tensor_spec.from_spec(henv.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# it's output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=kernel_initializer,\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(henv)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(henv)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    #boltzmann_temperature = 0.005,\n",
    "    #epsilon_greedy = 0.001,\n",
    "    #gamma=0.99,\n",
    "    optimizer=tf.keras.optimizers.RMSprop(), # tf.keras.optimizers.Adam(learning_rate = 1e-3),\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "    #target_update_period=target_update_period)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually initialize a reasonably good policy: kill both herds if the sum of observations is large\n",
    "#W = np.array([[0, 3 ,0, 2],[0, 0, 3, 2,]])\n",
    "#b = np.array([1, 0, 0, 0])\n",
    "#q_net.layers[0].set_weights([W,b])\n",
    "#agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mario/.local/lib/python3.8/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mario/.local/lib/python3.8/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mario/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mario/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(\n",
       "{action: (64, 2),\n",
       " discount: (64, 2),\n",
       " next_step_type: (64, 2),\n",
       " observation: (64, 2, 3),\n",
       " policy_info: (),\n",
       " reward: (64, 2),\n",
       " step_type: (64, 2)}), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(\n",
       "{action: tf.int32,\n",
       " discount: tf.float32,\n",
       " next_step_type: tf.int32,\n",
       " observation: tf.float32,\n",
       " policy_info: (),\n",
       " reward: tf.float32,\n",
       " step_type: tf.int32}), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mario/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mario/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1: loss = 2019.0221\tstep 1: average return = -1951.1 cullsteps = 0.0\n",
      "step 50: average return = -1931.6 cullsteps = 0.0\n",
      "step 100: average return = -1938.6 cullsteps = 0.0\n",
      "step = 200: loss = 35736.8008\tstep = 400: loss = 43.1391\tstep = 600: loss = 412.1095\tstep = 800: loss = 24133.8184\tstep = 1000: loss = 4633.9131\tstep 1000: average return = -1930.9 cullsteps = 0.0\n",
      "step = 1200: loss = 1200.1688\tstep = 1400: loss = 2168.2258\tstep = 1600: loss = 1453.5250\tstep = 1800: loss = 47495.2617\tstep = 2000: loss = 111400.5938\tstep 2000: average return = -4162.6 cullsteps = 100.0\n",
      "step = 2200: loss = 2209.4927\tstep = 2400: loss = 1778.2285\tstep = 2600: loss = 1331.7872\tstep = 2800: loss = 35893.3594\tstep = 3000: loss = 45362.9102\tstep 3000: average return = -4169.9 cullsteps = 100.0\n",
      "step = 3200: loss = 264.1064\tstep = 3400: loss = 39.4343\tstep = 3600: loss = 108.8394\tstep = 3800: loss = 6.4145\tstep = 4000: loss = 88.1631\tstep 4000: average return = -1938.6 cullsteps = 0.0\n",
      "step = 4200: loss = 878.5885\tstep = 4400: loss = 1.3048\tstep = 4600: loss = 0.8017\tstep = 4800: loss = 541.6308\tstep = 5000: loss = 3.5488\tstep 5000: average return = -1944.5 cullsteps = 0.0\n",
      "step = 5200: loss = 328.1508\tstep = 5400: loss = 2.9036\tstep = 5600: loss = 505.8521\tstep = 5800: loss = 65.9441\tstep = 6000: loss = 271.9703\tstep 6000: average return = -1936.1 cullsteps = 0.0\n",
      "step = 6200: loss = 1039.5027\tstep = 6400: loss = 6.5434\tstep = 6600: loss = 15.7442\tstep = 6800: loss = 558.3137\tstep = 7000: loss = 780.4868\tstep 7000: average return = -1941.7 cullsteps = 0.0\n",
      "step = 7200: loss = 95.0748\tstep = 7400: loss = 888.0450\tstep = 7600: loss = 2.0543\tstep = 7800: loss = 67.0717\tstep = 8000: loss = 2.9630\tstep 8000: average return = -1926.1 cullsteps = 0.0\n",
      "step = 8200: loss = 231.1548\tstep = 8400: loss = 1473.5970\tstep = 8600: loss = 34.0123\tstep = 8800: loss = 26.4808\tstep = 9000: loss = 393.1404\tstep 9000: average return = -1935.7 cullsteps = 0.0\n",
      "step = 9200: loss = 789.4473\tstep = 9400: loss = 8.4723\tstep = 9600: loss = 11.3774\tstep = 9800: loss = 11.3606\tstep = 10000: loss = 28.5382\tstep 10000: average return = -1941.9 cullsteps = 0.0\n",
      "step = 10200: loss = 1.8078\tstep = 10400: loss = 6.0492\tstep = 10600: loss = 782.0265\tstep = 10800: loss = 2561.6851\tstep = 11000: loss = 14.8943\tstep 11000: average return = -1944.4 cullsteps = 0.0\n",
      "step = 11200: loss = 780.4126\tstep = 11400: loss = 1896.4510\tstep = 11600: loss = 1075.2341\tstep = 11800: loss = 194.0882\tstep = 12000: loss = 555.9495\tstep 12000: average return = -1908.0 cullsteps = 12.0\n",
      "step = 12200: loss = 869.1750\tstep = 12400: loss = 554.4594\tstep = 12600: loss = 105.7197\tstep = 12800: loss = 107.8624\tstep = 13000: loss = 4.4331\tstep 13000: average return = -1952.5 cullsteps = 17.0\n",
      "step = 13200: loss = 450.5917\tstep = 13400: loss = 214.9818\tstep = 13600: loss = 21.4286\tstep = 13800: loss = 273.2949\tstep = 14000: loss = 7.4491\tstep 14000: average return = -2239.8 cullsteps = 27.0\n",
      "step = 14200: loss = 1.9428\tstep = 14400: loss = 122.0166\tstep = 14600: loss = 158.7813\tstep = 14800: loss = 22.8370\tstep = 15000: loss = 20.7551\tstep 15000: average return = -2186.6 cullsteps = 27.0\n",
      "step = 15200: loss = 989.9730\tstep = 15400: loss = 1331.5308\tstep = 15600: loss = 14.2460\tstep = 15800: loss = 120.0411\tstep = 16000: loss = 421.5592\tstep 16000: average return = -1642.9 cullsteps = 3.0\n",
      "step = 16200: loss = 43.8931\tstep = 16400: loss = 832.8069\tstep = 16600: loss = 94.6650\tstep = 16800: loss = 15.1199\tstep = 17000: loss = 3.6123\tstep 17000: average return = -1499.2 cullsteps = 6.0\n",
      "step = 17200: loss = 28.8018\tstep = 17400: loss = 207.9784\tstep = 17600: loss = 3.0526\tstep = 17800: loss = 13.5707\tstep = 18000: loss = 3847.6104\tstep 18000: average return = -1481.2 cullsteps = 3.0\n",
      "step = 18200: loss = 17.7049\tstep = 18400: loss = 106.0819\tstep = 18600: loss = 1431.1398\tstep = 18800: loss = 11.2462\tstep = 19000: loss = 26.2364\tstep 19000: average return = -1410.5 cullsteps = 4.0\n",
      "step = 19200: loss = 101.1403\tstep = 19400: loss = 86.0065\tstep = 19600: loss = 63.1048\tstep = 19800: loss = 7.5875\tstep = 20000: loss = 103.4577\tstep 20000: average return = -1387.0 cullsteps = 4.0\n",
      "step = 20200: loss = 163.4372\tstep = 20400: loss = 4.7162\tstep = 20600: loss = 5.7302\tstep = 20800: loss = 3.1010\tstep = 21000: loss = 11.5415\tstep 21000: average return = -1462.1 cullsteps = 4.0\n",
      "step = 21200: loss = 9.0995\tstep = 21400: loss = 58.9976\tstep = 21600: loss = 1.5749\tstep = 21800: loss = 20.4451\tstep = 22000: loss = 144.1387\tstep 22000: average return = -1456.5 cullsteps = 4.0\n",
      "step = 22200: loss = 1.2226\tstep = 22400: loss = 2.5008\tstep = 22600: loss = 244.3251\tstep = 22800: loss = 2.0249\tstep = 23000: loss = 68.3717\tstep 23000: average return = -1464.6 cullsteps = 3.0\n",
      "step = 23200: loss = 8.3777\tstep = 23400: loss = 140.7177\tstep = 23600: loss = 75.5221\tstep = 23800: loss = 1.8131\tstep = 24000: loss = 9.0273\tstep 24000: average return = -1465.6 cullsteps = 3.0\n",
      "step = 24200: loss = 3.6854\tstep = 24400: loss = 4.8378\tstep = 24600: loss = 1500.3416\tstep = 24800: loss = 103.5808\tstep = 25000: loss = 6.0040\tstep 25000: average return = -1509.4 cullsteps = 5.0\n",
      "step = 25200: loss = 6.2884\tstep = 25400: loss = 6.8488\tstep = 25600: loss = 267.4360\tstep = 25800: loss = 5.0796\tstep = 26000: loss = 14.7893\tstep 26000: average return = -1470.4 cullsteps = 3.0\n",
      "step = 26200: loss = 6.8289\tstep = 26400: loss = 119.7624\tstep = 26600: loss = 169.9693\tstep = 26800: loss = 53.2237\tstep = 27000: loss = 249.6337\tstep 27000: average return = -1472.7 cullsteps = 3.0\n",
      "step = 27200: loss = 10.2928\tstep = 27400: loss = 160.8537\tstep = 27600: loss = 9.3696\tstep = 27800: loss = 165.0525\tstep = 28000: loss = 13.6998\tstep 28000: average return = -1516.7 cullsteps = 4.0\n",
      "step = 28200: loss = 376.4402\tstep = 28400: loss = 3.3714\tstep = 28600: loss = 5.2901\tstep = 28800: loss = 1152.8905\tstep = 29000: loss = 344.0271\tstep 29000: average return = -1513.7 cullsteps = 3.0\n",
      "step = 29200: loss = 4.1880\tstep = 29400: loss = 0.2795\tstep = 29600: loss = 115.8084\tstep = 29800: loss = 1.8694\tstep = 30000: loss = 195.7005\tstep 30000: average return = -1550.3 cullsteps = 5.0\n"
     ]
    }
   ],
   "source": [
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step <= 1 or step % log_interval == 0:\n",
    "    print('step = {0:4>}: loss = {1:.4f}'.format(step, train_loss), end=\"\\t\")\n",
    "\n",
    "  if step <= 1 or (step <= 100 and step % 50 == 0) or step % eval_interval == 0:\n",
    "    avg_return, cullsteps = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step {0}: average return = {1:.1f} cullsteps = {2:.1f}'.format(step, avg_return.numpy().item(), cullsteps))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward of learned policy:  [-1542.952] cullsteps= 5.0\n"
     ]
    }
   ],
   "source": [
    "learned_reward, culleps = compute_avg_return(eval_env, agent.policy, num_episodes=500)\n",
    "print (\"reward of learned policy: \", learned_reward.numpy(), \"cullsteps=\", culleps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ts = eval_env.reset()\n",
    "\n",
    "def get_action(obs):\n",
    "    \"\"\" execute the learned policy network \n",
    "       obs:  one float for global time, one float for each herd - the time since last culling\n",
    "    \"\"\"\n",
    "    _ts = TimeStep(tf.constant([0.]),\n",
    "                   tf.constant([0.]),\n",
    "                   tf.constant([1]),\n",
    "                   tf.constant([obs]))\n",
    "    # a = agent.collect_policy.action(_ts) # just to see how much is explored versus exploited\n",
    "    a = agent.policy.action(_ts)\n",
    "    return a.action.numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.greedy_policy.GreedyPolicy at 0x7f95c84c69a0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 0, 0, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what the learned policy does on a grid of observations (5 steps per row&col)\n",
    "A = [[get_action([.0, x,y])\n",
    " for y in np.arange(0.,1.,.05,np.float32)]\n",
    " for x in np.arange(0.,1.,.05,np.float32)]\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with parameters of manually designed q_network policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W, b = agent.policy.trainable_variables\n",
    "# W = W.numpy()\n",
    "# b = b.numpy()\n",
    "# print (\"weights\\n\", W, \"\\nbias\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nn(obs):\n",
    "#    y = np.dot(obs, W)+b\n",
    "#    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn([0.5,.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
