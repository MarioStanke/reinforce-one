{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the environment `HerdEnv` of `herd_env.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "from tf_agents.trajectories import TimeStep\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from herd_env import HerdEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "henv_val = HerdEnv(herd_sizes = [64,64], rand_recovery_prob = 0.1, rand_infection_prob = 0.05)\n",
    "utils.validate_py_environment(henv_val, episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Herd Environment instance to be trained for\n",
    "max_episode_length=100\n",
    "num_herds=2\n",
    "henv = HerdEnv(herd_sizes = [32,32], expected_episode_length=50., max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.04, rand_infection_prob = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': 1.0,\n",
      " 'observation': array([0., 0.], dtype=float32),\n",
      " 'reward': 0.0,\n",
      " 'step_type': array(0, dtype=int32)})\n",
      "state:  [2 1 1 1] observation:  [0.01 0.01] \treward:  0.0\n",
      "state:  [1 1 2 2] observation:  [0.02 0.02] \treward:  0.0\n",
      "state:  [2 2 3 3] observation:  [0.03 0.03] \treward:  0.0\n",
      "state:  [2 3 4 4] observation:  [0.04 0.04] \treward:  0.0\n",
      "state:  [3 3 5 5] observation:  [0.05 0.05] \treward:  0.0\n",
      "state:  [4 6 6 6] observation:  [0.06 0.06] \treward:  0.0\n",
      "state:  [6 8 7 7] observation:  [0.07 0.07] \treward:  0.0\n",
      "state:  [9 9 8 8] observation:  [0.08 0.08] \treward:  0.0\n",
      "state:  [9 9 9 9] observation:  [0.09 0.09] \treward:  0.0\n",
      "state:  [12 10 10 10] observation:  [0.1 0.1] \treward:  0.0\n",
      "state:  [12 11 11 11] observation:  [0.11 0.11] \treward:  0.0\n",
      "state:  [13 10 12 12] observation:  [0.12 0.12] \treward:  0.0\n",
      "state:  [15 12 13 13] observation:  [0.13 0.13] \treward:  -540.0\n",
      "Final Reward =  -540.0\n"
     ]
    }
   ],
   "source": [
    "# show interor values of environment\n",
    "time_step = henv.reset()\n",
    "print(time_step)\n",
    "cumulative_reward = time_step.reward\n",
    "finished = False\n",
    "\n",
    "while not finished:\n",
    "  time_step = henv.step(0) # do nothing\n",
    "  s = henv.get_state()\n",
    "  print(\"state: \", s, \"observation: \", time_step.observation, \"\\treward: \", time_step.reward)\n",
    "  cumulative_reward += time_step.reward\n",
    "  if time_step.step_type == StepType.LAST:\n",
    "    finished = True\n",
    "\n",
    "print('Final Reward = ', cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action spec:\n",
      " BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=3) \n",
      "\n",
      "time step spec:\n",
      " TimeStep(\n",
      "{'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
      " 'observation': BoundedArraySpec(shape=(2,), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=1.0),\n",
      " 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n",
      " 'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "action_spec = henv.action_spec()\n",
    "ts_spec = henv.time_step_spec()\n",
    "print(\"action spec:\\n\", action_spec, \"\\n\\ntime step spec:\\n\", ts_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scripted policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do nothing policy: cull never\n",
    "action_script0 = [(max_episode_length, 0)]\n",
    "\n",
    "# cull first herd every 10th step and second herd every 20th step\n",
    "action_script1 = [(9, 0), \n",
    "                 (1, 1),\n",
    "                 (9, 0), \n",
    "                 (1, 3)] * int(1+max_episode_length/20)\n",
    "\n",
    "manual_scripted_policy0 = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=ts_spec,\n",
    "    action_spec=action_spec,\n",
    "    action_script=action_script0)\n",
    "\n",
    "manual_scripted_policy1 = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=ts_spec,\n",
    "    action_spec=action_spec,\n",
    "    action_script=action_script1)\n",
    "\n",
    "init_policy_state = manual_scripted_policy0.get_initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action= 0 \tpolicy_state [0, 1]\n",
      "action= 0 \tpolicy_state [0, 2]\n",
      "action= 0 \tpolicy_state [0, 3]\n",
      "action= 0 \tpolicy_state [0, 4]\n",
      "action= 0 \tpolicy_state [0, 5]\n",
      "action= 0 \tpolicy_state [0, 6]\n",
      "action= 0 \tpolicy_state [0, 7]\n",
      "action= 0 \tpolicy_state [0, 8]\n",
      "action= 0 \tpolicy_state [0, 9]\n",
      "action= 1 \tpolicy_state [1, 1]\n",
      "action= 0 \tpolicy_state [2, 1]\n",
      "action= 0 \tpolicy_state [2, 2]\n",
      "action= 0 \tpolicy_state [2, 3]\n",
      "action= 0 \tpolicy_state [2, 4]\n",
      "action= 0 \tpolicy_state [2, 5]\n",
      "action= 0 \tpolicy_state [2, 6]\n",
      "action= 0 \tpolicy_state [2, 7]\n",
      "action= 0 \tpolicy_state [2, 8]\n",
      "action= 0 \tpolicy_state [2, 9]\n",
      "action= 3 \tpolicy_state [3, 1]\n",
      "action= 0 \tpolicy_state [4, 1]\n"
     ]
    }
   ],
   "source": [
    "policy_state =  init_policy_state\n",
    "ts0 = henv.reset()\n",
    "for _ in range(21):\n",
    "    action_step = manual_scripted_policy1.action(ts0, policy_state)\n",
    "    policy_state = action_step.state\n",
    "    print(\"action=\", action_step.action, \"\\tpolicy_state\", policy_state)\n",
    "policy_state = manual_scripted_policy1.get_initial_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_py_policy.RandomPyPolicy(time_step_spec=ts_spec, action_spec=action_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drive a rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=50, verbose=False):\n",
    "  total_return = 0.0\n",
    "  cullsteps = 0\n",
    "  for e in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "        policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "    else:\n",
    "        policy_state = None # other policies without memory\n",
    "    episode_return = 0.0\n",
    "    i=0\n",
    "    while not time_step.is_last():\n",
    "        i+=1\n",
    "        action_step = policy.action(time_step, policy_state)\n",
    "        if action_step.action > 0:\n",
    "            cullsteps += 1\n",
    "        policy_state = action_step.state\n",
    "        time_step = environment.step(action_step.action)\n",
    "        if isinstance(environment, HerdEnv):\n",
    "            state = environment.get_state()\n",
    "        else:\n",
    "            state = None # TF environment from wrapper does not have get_state()\n",
    "        episode_return += time_step.reward\n",
    "        if verbose:\n",
    "            print (f\"episode {e:>2} step{i:>3} action: \", action_step.action, \"state=\", state, \"obs=\", time_step.observation, \"reward=\", time_step.reward)\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  cullsteps /= num_episodes\n",
    "  return avg_return, cullsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average return of random policy: -3060.000 avg steps with culls per episode: 36.2\n"
     ]
    }
   ],
   "source": [
    "random_reward, cullsteps = compute_avg_return(henv, random_policy)\n",
    "print (f\"average return of random policy: {random_reward:.3f} avg steps with culls per episode: {cullsteps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 step  1 action:  0 state= [2 1 1 1] obs= [0.01 0.01] reward= 0.0\n",
      "episode  0 step  2 action:  0 state= [3 5 2 2] obs= [0.02 0.02] reward= 0.0\n",
      "episode  0 step  3 action:  0 state= [3 6 3 3] obs= [0.03 0.03] reward= 0.0\n",
      "episode  0 step  4 action:  0 state= [5 8 4 4] obs= [0.04 0.04] reward= 0.0\n",
      "episode  0 step  5 action:  0 state= [6 8 5 5] obs= [0.05 0.05] reward= 0.0\n",
      "episode  0 step  6 action:  0 state= [8 9 6 6] obs= [0.06 0.06] reward= 0.0\n",
      "episode  0 step  7 action:  0 state= [ 8 12  7  7] obs= [0.07 0.07] reward= 0.0\n",
      "episode  0 step  8 action:  0 state= [ 9 14  8  8] obs= [0.08 0.08] reward= 0.0\n",
      "episode  0 step  9 action:  0 state= [10 16  9  9] obs= [0.09 0.09] reward= 0.0\n",
      "episode  0 step 10 action:  1 state= [ 0 16  0 10] obs= [0.  0.1] reward= -62.0\n",
      "episode  0 step 11 action:  0 state= [ 3 17  1 11] obs= [0.01 0.11] reward= 0.0\n",
      "episode  0 step 12 action:  0 state= [ 5 15  2 12] obs= [0.02 0.12] reward= 0.0\n",
      "episode  0 step 13 action:  0 state= [ 5 14  3 13] obs= [0.03 0.13] reward= 0.0\n",
      "episode  0 step 14 action:  0 state= [ 8 14  4 14] obs= [0.04 0.14] reward= 0.0\n",
      "episode  0 step 15 action:  0 state= [ 8 13  5 15] obs= [0.05 0.15] reward= 0.0\n",
      "episode  0 step 16 action:  0 state= [ 8 13  6 16] obs= [0.06 0.16] reward= 0.0\n",
      "episode  0 step 17 action:  0 state= [ 8 14  7 17] obs= [0.07 0.17] reward= 0.0\n",
      "episode  0 step 18 action:  0 state= [10 16  8 18] obs= [0.08 0.18] reward= -520.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-582.0, 1.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show states for one rollout of second scripted policy\n",
    "compute_avg_return(henv, manual_scripted_policy1, num_episodes=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average return of do-nothing-policy: -687.600 avg culls 0.0\n",
      "average return of manual policy: -689.460 avg culls 4.38\n"
     ]
    }
   ],
   "source": [
    "manual_reward0, cullsteps = compute_avg_return(henv, manual_scripted_policy0, num_episodes=100)\n",
    "print (f\"average return of do-nothing-policy: {manual_reward0:.3f} avg culls {cullsteps}\")\n",
    "manual_reward1, cullsteps = compute_avg_return(henv, manual_scripted_policy1, num_episodes=100)\n",
    "print (f\"average return of manual policy: {manual_reward1:.3f} avg culls {cullsteps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Deep-Q Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks.sequential import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 30000\n",
    "replay_buffer_max_length = 10000\n",
    "batch_size = 16\n",
    "num_eval_episodes = 20\n",
    "initial_collect_steps = 100\n",
    "collect_steps_per_iteration = 32\n",
    "log_interval = 100\n",
    "eval_interval = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make actor network simple\n",
    "num_actions = 2**num_herds # this does not scale, obviously\n",
    "kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03)\n",
    "q_net = Sequential([Dense(4, activation=None,\n",
    "                          kernel_initializer = kernel_initializer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(henv)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(henv)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    boltzmann_temperature = 0.005,\n",
    "    epsilon_greedy = None,\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-3),\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'sequential/dense/kernel:0' shape=(2, 4) dtype=float32, numpy=\n",
       " array([[ 0.00947275,  0.00769239, -0.02492321, -0.02975761],\n",
       "        [ 0.02453793, -0.02131323,  0.00301866,  0.01287598]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'sequential/dense/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'sequential/dense/kernel:0' shape=(2, 4) dtype=float32, numpy=\n",
       " array([[0., 0., 0., 2.],\n",
       "        [0., 0., 0., 2.]], dtype=float32)>,\n",
       " <tf.Variable 'sequential/dense/bias:0' shape=(4,) dtype=float32, numpy=array([1., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually initialize a reasonably good policy: kill both herds if the sum of observations is > 1\n",
    "W = np.array([[0, 0 ,0, 2],[0, 0, 0, 2,]])\n",
    "b = np.array([1, 0, 0, 0])\n",
    "q_net.layers[0].set_weights([W,b])\n",
    "agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mario/.local/lib/python3.8/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mario/.local/lib/python3.8/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mario/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mario/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(\n",
       "{action: (16, 2),\n",
       " discount: (16, 2),\n",
       " next_step_type: (16, 2),\n",
       " observation: (16, 2, 2),\n",
       " policy_info: (),\n",
       " reward: (16, 2),\n",
       " step_type: (16, 2)}), BufferInfo(ids=(16, 2), probabilities=(16,))), types: (Trajectory(\n",
       "{action: tf.int32,\n",
       " discount: tf.float32,\n",
       " next_step_type: tf.int32,\n",
       " observation: tf.float32,\n",
       " policy_info: (),\n",
       " reward: tf.float32,\n",
       " step_type: tf.int32}), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mario/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mario/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1: loss = 5893.6016\tstep = 1: average return = -687.800 cullsteps = 1.4\n",
      "step = 20: average return = -582.200 cullsteps = 1.3\n",
      "step = 40: average return = -703.400 cullsteps = 1.1\n",
      "step = 60: average return = -683.400 cullsteps = 1.4\n",
      "step = 80: average return = -734.800 cullsteps = 1.4\n",
      "step = 100: loss = 0.0000\tstep = 200: loss = 960.8949\tstep = 300: loss = 0.0000\tstep = 400: loss = 39057.0039\tstep = 400: average return = -669.000 cullsteps = 1.0\n",
      "step = 500: loss = 41985.3086\tstep = 600: loss = 0.0000\tstep = 700: loss = 81268.5234\tstep = 800: loss = 25637.8945\tstep = 800: average return = -752.600 cullsteps = 1.1\n",
      "step = 900: loss = 0.0001\tstep = 1000: loss = 957.6707\tstep = 1100: loss = 907.1018\tstep = 1200: loss = 0.0001\tstep = 1200: average return = -618.600 cullsteps = 1.1\n",
      "step = 1300: loss = 0.0001\tstep = 1400: loss = 956.4066\tstep = 1500: loss = 0.0002\tstep = 1600: loss = 30627.2422\tstep = 1600: average return = -1046.000 cullsteps = 8.2\n",
      "step = 1700: loss = 25604.6484\tstep = 1800: loss = 238.5965\tstep = 1900: loss = 34207.5234\tstep = 2000: loss = 953.8844\tstep = 2000: average return = -640.800 cullsteps = 0.7\n",
      "step = 2100: loss = 953.8865\tstep = 2200: loss = 953.0485\tstep = 2300: loss = 27185.3457\tstep = 2400: loss = 237.0770\tstep = 2400: average return = -690.800 cullsteps = 3.4\n",
      "step = 2500: loss = 21832.5957\tstep = 2600: loss = 0.0004\tstep = 2700: loss = 35120.3633\tstep = 2800: loss = 1572.0184\tstep = 2800: average return = -667.000 cullsteps = 1.2\n",
      "step = 2900: loss = 0.0005\tstep = 3000: loss = 238.3586\tstep = 3100: loss = 236.7771\tstep = 3200: loss = 0.0007\tstep = 3200: average return = -753.400 cullsteps = 1.9\n",
      "step = 3300: loss = 237.7659\tstep = 3400: loss = 1901.9855\tstep = 3500: loss = 45403.6211\tstep = 3600: loss = 4436.8955\tstep = 3600: average return = -628.000 cullsteps = 2.0\n",
      "step = 3700: loss = 237.6080\tstep = 3800: loss = 1190.1971\tstep = 3900: loss = 40538.7344\tstep = 4000: loss = 8049.9673\tstep = 4000: average return = -777.200 cullsteps = 6.0\n",
      "step = 4100: loss = 0.0011\tstep = 4200: loss = 238.3795\tstep = 4300: loss = 20920.7559\tstep = 4400: loss = 12856.9434\tstep = 4400: average return = -655.600 cullsteps = 2.0\n",
      "step = 4500: loss = 37825.5898\tstep = 4600: loss = 14314.6289\tstep = 4700: loss = 5128.1987\tstep = 4800: loss = 235.9638\tstep = 4800: average return = -645.300 cullsteps = 1.2\n",
      "step = 4900: loss = 0.0014\tstep = 5000: loss = 0.0014\tstep = 5100: loss = 59502.2422\tstep = 5200: loss = 237.1613\tstep = 5200: average return = -671.900 cullsteps = 3.0\n",
      "step = 5300: loss = 27279.0703\tstep = 5400: loss = 0.0017\tstep = 5500: loss = 59241.3594\tstep = 5600: loss = 0.0017\tstep = 5600: average return = -633.800 cullsteps = 2.9\n",
      "step = 5700: loss = 0.0017\tstep = 5800: loss = 239.7891\tstep = 5900: loss = 14138.5352\tstep = 6000: loss = 715.1669\tstep = 6000: average return = -740.800 cullsteps = 1.7\n",
      "step = 6100: loss = 25421.4766\tstep = 6200: loss = 0.0017\tstep = 6300: loss = 34213.9766\tstep = 6400: loss = 20584.1641\tstep = 6400: average return = -652.600 cullsteps = 1.4\n",
      "step = 6500: loss = 1182.4449\tstep = 6600: loss = 479.0164\tstep = 6700: loss = 0.0027\tstep = 6800: loss = 239.0034\tstep = 6800: average return = -605.000 cullsteps = 1.2\n",
      "step = 6900: loss = 237.4857\tstep = 7000: loss = 7979.1948\tstep = 7100: loss = 467.5930\tstep = 7200: loss = 237.0033\tstep = 7200: average return = -720.400 cullsteps = 3.2\n",
      "step = 7300: loss = 236.1404\tstep = 7400: loss = 238.4238\tstep = 7500: loss = 469.8961\tstep = 7600: loss = 18332.9492\tstep = 7600: average return = -526.200 cullsteps = 1.6\n",
      "step = 7700: loss = 4558.4482\tstep = 7800: loss = 0.0033\tstep = 7900: loss = 239.1750\tstep = 8000: loss = 946.6105\tstep = 8000: average return = -572.600 cullsteps = 1.6\n",
      "step = 8100: loss = 59850.1719\tstep = 8200: loss = 716.5961\tstep = 8300: loss = 236.8434\tstep = 8400: loss = 0.0040\tstep = 8400: average return = -805.100 cullsteps = 4.7\n",
      "step = 8500: loss = 2115.4902\tstep = 8600: loss = 0.0041\tstep = 8700: loss = 20278.4395\tstep = 8800: loss = 2591.7861\tstep = 8800: average return = -663.500 cullsteps = 2.8\n",
      "step = 8900: loss = 235.1685\tstep = 9000: loss = 10057.7363\tstep = 9100: loss = 0.0043\tstep = 9200: loss = 1415.8793\tstep = 9200: average return = -573.400 cullsteps = 1.4\n",
      "step = 9300: loss = 937.9261\tstep = 9400: loss = 937.0823\tstep = 9500: loss = 936.5770\tstep = 9600: loss = 232.4474\tstep = 9600: average return = -710.100 cullsteps = 4.5\n",
      "step = 9700: loss = 25483.3281\tstep = 9800: loss = 468.7646\tstep = 9900: loss = 238.4153\tstep = 10000: loss = 708.3715\tstep = 10000: average return = -630.600 cullsteps = 1.1\n",
      "step = 10100: loss = 0.0055\tstep = 10200: loss = 10785.3564\tstep = 10300: loss = 12977.4619\tstep = 10400: loss = 25088.8477\tstep = 10400: average return = -1264.800 cullsteps = 14.8\n",
      "step = 10500: loss = 1641.1346\tstep = 10600: loss = 1409.1636\tstep = 10700: loss = 932.2652\tstep = 10800: loss = 0.0068\tstep = 10800: average return = -935.200 cullsteps = 6.6\n",
      "step = 10900: loss = 18124.4180\tstep = 11000: loss = 1855.2959\tstep = 11100: loss = 8805.3721\tstep = 11200: loss = 22083.8301\tstep = 11200: average return = -654.400 cullsteps = 2.7\n",
      "step = 11300: loss = 0.0057\tstep = 11400: loss = 1924.9193\tstep = 11500: loss = 236.8060\tstep = 11600: loss = 697.9683\tstep = 11600: average return = -776.500 cullsteps = 3.8\n",
      "step = 11700: loss = 3690.7439\tstep = 11800: loss = 238.0842\tstep = 11900: loss = 1169.9890\tstep = 12000: loss = 235.2276\tstep = 12000: average return = -698.800 cullsteps = 3.3\n",
      "step = 12100: loss = 3546.5508\tstep = 12200: loss = 22225.6055\tstep = 12300: loss = 1410.6747\tstep = 12400: loss = 1164.8821\tstep = 12400: average return = -891.000 cullsteps = 8.0\n",
      "step = 12500: loss = 20599.1680\tstep = 12600: loss = 30032.8203\tstep = 12700: loss = 233.1058\tstep = 12800: loss = 1858.8146\tstep = 12800: average return = -701.500 cullsteps = 2.7\n",
      "step = 12900: loss = 711.3201\tstep = 13000: loss = 233.9127\tstep = 13100: loss = 230.7885\tstep = 13200: loss = 1163.3789\tstep = 13200: average return = -693.700 cullsteps = 2.7\n",
      "step = 13300: loss = 231.2003\tstep = 13400: loss = 23486.8359\tstep = 13500: loss = 13248.9395\tstep = 13600: loss = 34990.5703\tstep = 13600: average return = -745.400 cullsteps = 4.7\n",
      "step = 13700: loss = 467.5415\tstep = 13800: loss = 1178.8629\tstep = 13900: loss = 21238.3047\tstep = 14000: loss = 22292.0410\tstep = 14000: average return = -584.400 cullsteps = 1.4\n",
      "step = 14100: loss = 463.4338\tstep = 14200: loss = 0.0112\tstep = 14300: loss = 238.7870\tstep = 14400: loss = 926.5759\tstep = 14400: average return = -878.300 cullsteps = 7.8\n",
      "step = 14500: loss = 239.4837\tstep = 14600: loss = 479.5000\tstep = 14700: loss = 30594.5469\tstep = 14800: loss = 21871.8027\tstep = 14800: average return = -578.400 cullsteps = 1.1\n",
      "step = 14900: loss = 1118.9948\tstep = 15000: loss = 37460.8203\tstep = 15100: loss = 0.0120\tstep = 15200: loss = 8021.0156\tstep = 15200: average return = -593.800 cullsteps = 2.9\n",
      "step = 15300: loss = 0.0133\tstep = 15400: loss = 465.6269\tstep = 15500: loss = 1702.9969\tstep = 15600: loss = 0.0125\tstep = 15600: average return = -554.800 cullsteps = 0.9\n",
      "step = 15700: loss = 0.0145\tstep = 15800: loss = 0.0136\tstep = 15900: loss = 235.0448\tstep = 16000: loss = 697.6070\tstep = 16000: average return = -645.400 cullsteps = 3.7\n",
      "step = 16100: loss = 10894.4629\tstep = 16200: loss = 231.4373\tstep = 16300: loss = 6921.1631\tstep = 16400: loss = 236.4523\tstep = 16400: average return = -899.800 cullsteps = 6.8\n",
      "step = 16500: loss = 0.0156\tstep = 16600: loss = 15351.7939\tstep = 16700: loss = 33823.3906\tstep = 16800: loss = 949.4639\tstep = 16800: average return = -595.000 cullsteps = 1.2\n",
      "step = 16900: loss = 9934.2246\tstep = 17000: loss = 0.0158\tstep = 17100: loss = 0.0150\tstep = 17200: loss = 240.5452\tstep = 17200: average return = -709.000 cullsteps = 4.0\n",
      "step = 17300: loss = 1095.0662\tstep = 17400: loss = 467.4748\tstep = 17500: loss = 235.5058\tstep = 17600: loss = 0.0167\tstep = 17600: average return = -488.800 cullsteps = 0.9\n",
      "step = 17700: loss = 25379.0801\tstep = 17800: loss = 21756.6250\tstep = 17900: loss = 0.0165\tstep = 18000: loss = 469.0783\tstep = 18000: average return = -595.800 cullsteps = 4.9\n",
      "step = 18100: loss = 698.7743\tstep = 18200: loss = 16318.5479\tstep = 18300: loss = 239.3774\tstep = 18400: loss = 1840.1217\tstep = 18400: average return = -676.000 cullsteps = 3.1\n",
      "step = 18500: loss = 19210.4629\tstep = 18600: loss = 15564.0938\tstep = 18700: loss = 237.7810\tstep = 18800: loss = 233.3285\tstep = 18800: average return = -726.900 cullsteps = 2.1\n",
      "step = 18900: loss = 46996.6250\tstep = 19000: loss = 910.0829\tstep = 19100: loss = 29715.7969\tstep = 19200: loss = 1823.7996\tstep = 19200: average return = -697.600 cullsteps = 3.0\n",
      "step = 19300: loss = 1840.6166\tstep = 19400: loss = 33276.8984\tstep = 19500: loss = 236.9774\tstep = 19600: loss = 0.0213\tstep = 19600: average return = -640.800 cullsteps = 1.4\n",
      "step = 19700: loss = 234.9776\tstep = 19800: loss = 1079.1235\tstep = 19900: loss = 913.2930\tstep = 20000: loss = 0.0224\tstep = 20000: average return = -581.200 cullsteps = 2.6\n",
      "step = 20100: loss = 236.6588\tstep = 20200: loss = 699.8178\tstep = 20300: loss = 0.0227\tstep = 20400: loss = 47707.1133\tstep = 20400: average return = -630.600 cullsteps = 1.6\n",
      "step = 20500: loss = 26129.6621\tstep = 20600: loss = 0.0229\tstep = 20700: loss = 1630.9471\tstep = 20800: loss = 1391.5319\tstep = 20800: average return = -995.600 cullsteps = 7.6\n",
      "step = 20900: loss = 11849.1250\tstep = 21000: loss = 902.3251\tstep = 21100: loss = 0.0264\tstep = 21200: loss = 0.0266\tstep = 21200: average return = -719.200 cullsteps = 1.1\n",
      "step = 21300: loss = 40769.5000\tstep = 21400: loss = 31706.0625\tstep = 21500: loss = 234.3904\tstep = 21600: loss = 707.1379\tstep = 21600: average return = -589.000 cullsteps = 1.8\n",
      "step = 21700: loss = 465.4421\tstep = 21800: loss = 0.0259\tstep = 21900: loss = 16181.7666\tstep = 22000: loss = 58.6045\tstep = 22000: average return = -1044.600 cullsteps = 10.3\n",
      "step = 22100: loss = 0.0279\tstep = 22200: loss = 239.9415\tstep = 22300: loss = 710.7721\tstep = 22400: loss = 3237.9749\tstep = 22400: average return = -672.600 cullsteps = 1.6\n",
      "step = 22500: loss = 0.0299\tstep = 22600: loss = 16158.4854\tstep = 22700: loss = 0.0306\tstep = 22800: loss = 0.0264\tstep = 22800: average return = -765.000 cullsteps = 6.0\n",
      "step = 22900: loss = 715.5511\tstep = 23000: loss = 456.3860\tstep = 23100: loss = 239.5991\tstep = 23200: loss = 21627.9473\tstep = 23200: average return = -584.800 cullsteps = 1.9\n",
      "step = 23300: loss = 377.4622\tstep = 23400: loss = 5959.2529\tstep = 23500: loss = 1374.1057\tstep = 23600: loss = 987.9965\tstep = 23600: average return = -595.300 cullsteps = 1.5\n",
      "step = 23700: loss = 0.0301\tstep = 23800: loss = 229.4964\tstep = 23900: loss = 0.0342\tstep = 24000: loss = 240.4115\tstep = 24000: average return = -623.200 cullsteps = 1.1\n",
      "step = 24100: loss = 236.5531\tstep = 24200: loss = 21046.3164\tstep = 24300: loss = 47062.7305\tstep = 24400: loss = 29105.2637\tstep = 24400: average return = -686.900 cullsteps = 4.5\n",
      "step = 24500: loss = 228.5476\tstep = 24600: loss = 234.3113\tstep = 24700: loss = 902.6404\tstep = 24800: loss = 0.0356\tstep = 24800: average return = -1288.700 cullsteps = 14.3\n",
      "step = 24900: loss = 17320.3516\tstep = 25000: loss = 1365.9974\tstep = 25100: loss = 237.4803\tstep = 25200: loss = 240.9627\tstep = 25200: average return = -1232.100 cullsteps = 13.6\n",
      "step = 25300: loss = 953.4498\tstep = 25400: loss = 233.0171\tstep = 25500: loss = 23034.9512\tstep = 25600: loss = 882.0684\tstep = 25600: average return = -694.300 cullsteps = 3.1\n",
      "step = 25700: loss = 876.7745\tstep = 25800: loss = 0.0388\tstep = 25900: loss = 474.0072\tstep = 26000: loss = 699.3917\tstep = 26000: average return = -638.800 cullsteps = 2.6\n",
      "step = 26100: loss = 1121.4386\tstep = 26200: loss = 1786.9343\tstep = 26300: loss = 13864.3770\tstep = 26400: loss = 1121.1508\tstep = 26400: average return = -604.400 cullsteps = 0.8\n",
      "step = 26500: loss = 1124.9974\tstep = 26600: loss = 0.0342\tstep = 26700: loss = 892.2401\tstep = 26800: loss = 939.0752\tstep = 26800: average return = -1565.800 cullsteps = 19.1\n",
      "step = 26900: loss = 10816.5518\tstep = 27000: loss = 19479.5645\tstep = 27100: loss = 893.7247\tstep = 27200: loss = 53449.3594\tstep = 27200: average return = -682.400 cullsteps = 0.8\n",
      "step = 27300: loss = 241.0881\tstep = 27400: loss = 761.1702\tstep = 27500: loss = 955.8477\tstep = 27600: loss = 230.2887\tstep = 27600: average return = -684.000 cullsteps = 2.8\n",
      "step = 27700: loss = 0.0372\tstep = 27800: loss = 9603.7031\tstep = 27900: loss = 226.4402\tstep = 28000: loss = 7160.7754\tstep = 28000: average return = -594.400 cullsteps = 1.6\n",
      "step = 28100: loss = 19938.0664\tstep = 28200: loss = 1111.0732\tstep = 28300: loss = 470.7006\tstep = 28400: loss = 21507.7031\tstep = 28400: average return = -747.500 cullsteps = 2.6\n",
      "step = 28500: loss = 235.2891\tstep = 28600: loss = 1126.6896\tstep = 28700: loss = 0.0493\tstep = 28800: loss = 0.0453\tstep = 28800: average return = -612.700 cullsteps = 3.4\n",
      "step = 28900: loss = 897.0338\tstep = 29000: loss = 1118.8700\tstep = 29100: loss = 241.1564\tstep = 29200: loss = 4033.7607\tstep = 29200: average return = -686.800 cullsteps = 3.0\n",
      "step = 29300: loss = 22822.0684\tstep = 29400: loss = 0.0511\tstep = 29500: loss = 711.8978\tstep = 29600: loss = 21026.4043\tstep = 29600: average return = -736.800 cullsteps = 3.2\n",
      "step = 29700: loss = 6199.7842\tstep = 29800: loss = 10307.4863\tstep = 29900: loss = 5844.3462\tstep = 30000: loss = 42115.5117\tstep = 30000: average return = -689.000 cullsteps = 1.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step <= 1 or step % log_interval == 0:\n",
    "    print('step = {0:4>}: loss = {1:.4f}'.format(step, train_loss), end=\"\\t\")\n",
    "\n",
    "  if step <= 1 or (step < 100 and step % 20 == 0) or step % eval_interval == 0:\n",
    "    avg_return, cullsteps = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: average return = {1:.3f} cullsteps = {2:.1f}'.format(step, avg_return.numpy().item(), cullsteps))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward of learned policy:  [-673.6] culleps= 1.25\n"
     ]
    }
   ],
   "source": [
    "learned_reward, culleps = compute_avg_return(eval_env, agent.policy, num_episodes=100)\n",
    "print (\"reward of learned policy: \", learned_reward.numpy(), \"culleps=\", culleps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ts = eval_env.reset()\n",
    "\n",
    "def get_action(obs):\n",
    "    \"\"\" execute the learned policy network \n",
    "       obs:  one float for each herd - the time since last culling\n",
    "    \"\"\"\n",
    "    _ts = TimeStep(tf.constant([0.]),\n",
    "                   tf.constant([0.]),\n",
    "                   tf.constant([1]),\n",
    "                   tf.constant([obs]))\n",
    "    # a = agent.collect_policy.action(_ts) # just to see how much is explored versus exploited\n",
    "    a = agent.policy.action(_ts)\n",
    "    return a.action.numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what the learned policy does on a grid of observations (5 steps per row&col)\n",
    "A = [[get_action([x,y])\n",
    " for y in np.arange(0.,1.,.05,np.float32)]\n",
    " for x in np.arange(0.,1.,.05,np.float32)]\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with parameters of manually designed q_network policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      " [[-11.623947 -11.389742 -11.561311 -10.807395]\n",
      " [-11.50551  -11.517058 -11.269588 -10.697644]] \n",
      "bias [-13.105408  -13.213329  -13.2072115 -13.504318 ]\n"
     ]
    }
   ],
   "source": [
    "W, b = agent.policy.trainable_variables\n",
    "W = W.numpy()\n",
    "b = b.numpy()\n",
    "print (\"weights\\n\", W, \"\\nbias\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn(obs):\n",
    "    y = np.dot(obs, W)+b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-24.62276173, -24.71765604, -24.50597219, -24.21293755])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn([0.1,.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
