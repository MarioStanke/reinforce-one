{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the environment `HerdEnv` of `herd_env.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "from tf_agents.trajectories import TimeStep\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from herd_env import HerdEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "henv_val = HerdEnv(herd_sizes = [64,64], rand_recovery_prob = 0.1, rand_infection_prob = 0.05)\n",
    "utils.validate_py_environment(henv_val, episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Herd Environment instance to be trained for\n",
    "max_episode_length=100\n",
    "num_herds=2\n",
    "henv = HerdEnv(herd_sizes = [32,32], expected_episode_length=50., max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.04, rand_infection_prob = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': 1.0,\n",
      " 'observation': array([0., 0.], dtype=float32),\n",
      " 'reward': 0.0,\n",
      " 'step_type': array(0, dtype=int32)})\n",
      "state:  [0 2 1 1] observation:  [0.01 0.01] \treward:  0.0\n",
      "state:  [0 4 2 2] observation:  [0.02 0.02] \treward:  0.0\n",
      "state:  [2 6 3 3] observation:  [0.03 0.03] \treward:  0.0\n",
      "state:  [4 9 4 4] observation:  [0.04 0.04] \treward:  0.0\n",
      "state:  [4 9 5 5] observation:  [0.05 0.05] \treward:  0.0\n",
      "state:  [ 5 10  6  6] observation:  [0.06 0.06] \treward:  -300.0\n",
      "Final Reward =  -300.0\n"
     ]
    }
   ],
   "source": [
    "# show interor values of environment\n",
    "time_step = henv.reset()\n",
    "print(time_step)\n",
    "cumulative_reward = time_step.reward\n",
    "finished = False\n",
    "\n",
    "while not finished:\n",
    "  time_step = henv.step(0) # do nothing\n",
    "  s = henv.get_state()\n",
    "  print(\"state: \", s, \"observation: \", time_step.observation, \"\\treward: \", time_step.reward)\n",
    "  cumulative_reward += time_step.reward\n",
    "  if time_step.step_type == StepType.LAST:\n",
    "    finished = True\n",
    "\n",
    "print('Final Reward = ', cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action spec:\n",
      " BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=3) \n",
      "\n",
      "time step spec:\n",
      " TimeStep(\n",
      "{'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
      " 'observation': BoundedArraySpec(shape=(2,), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=1.0),\n",
      " 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n",
      " 'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "action_spec = henv.action_spec()\n",
    "ts_spec = henv.time_step_spec()\n",
    "print(\"action spec:\\n\", action_spec, \"\\n\\ntime step spec:\\n\", ts_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scripted policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do nothing policy: cull never\n",
    "action_script0 = [(max_episode_length, 0)]\n",
    "\n",
    "# cull first herd every 10th step and second herd every 20th step\n",
    "action_script1 = [(9, 0), \n",
    "                 (1, 1),\n",
    "                 (9, 0), \n",
    "                 (1, 3)] * int(1+max_episode_length/20)\n",
    "\n",
    "manual_scripted_policy0 = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=ts_spec,\n",
    "    action_spec=action_spec,\n",
    "    action_script=action_script0)\n",
    "\n",
    "manual_scripted_policy1 = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=ts_spec,\n",
    "    action_spec=action_spec,\n",
    "    action_script=action_script1)\n",
    "\n",
    "init_policy_state = manual_scripted_policy0.get_initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action= 0 \tpolicy_state [0, 1]\n",
      "action= 0 \tpolicy_state [0, 2]\n",
      "action= 0 \tpolicy_state [0, 3]\n",
      "action= 0 \tpolicy_state [0, 4]\n",
      "action= 0 \tpolicy_state [0, 5]\n",
      "action= 0 \tpolicy_state [0, 6]\n",
      "action= 0 \tpolicy_state [0, 7]\n",
      "action= 0 \tpolicy_state [0, 8]\n",
      "action= 0 \tpolicy_state [0, 9]\n",
      "action= 1 \tpolicy_state [1, 1]\n",
      "action= 0 \tpolicy_state [2, 1]\n",
      "action= 0 \tpolicy_state [2, 2]\n",
      "action= 0 \tpolicy_state [2, 3]\n",
      "action= 0 \tpolicy_state [2, 4]\n",
      "action= 0 \tpolicy_state [2, 5]\n",
      "action= 0 \tpolicy_state [2, 6]\n",
      "action= 0 \tpolicy_state [2, 7]\n",
      "action= 0 \tpolicy_state [2, 8]\n",
      "action= 0 \tpolicy_state [2, 9]\n",
      "action= 3 \tpolicy_state [3, 1]\n",
      "action= 0 \tpolicy_state [4, 1]\n"
     ]
    }
   ],
   "source": [
    "policy_state =  init_policy_state\n",
    "ts0 = henv.reset()\n",
    "for _ in range(21):\n",
    "    action_step = manual_scripted_policy1.action(ts0, policy_state)\n",
    "    policy_state = action_step.state\n",
    "    print(\"action=\", action_step.action, \"\\tpolicy_state\", policy_state)\n",
    "policy_state = manual_scripted_policy1.get_initial_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_py_policy.RandomPyPolicy(time_step_spec=ts_spec, action_spec=action_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drive a rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=50, verbose=False):\n",
    "  total_return = 0.0\n",
    "  cullsteps = 0\n",
    "  for e in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "        policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "    else:\n",
    "        policy_state = None # other policies without memory\n",
    "    episode_return = 0.0\n",
    "    i=0\n",
    "    while not time_step.is_last():\n",
    "        i+=1\n",
    "        action_step = policy.action(time_step, policy_state)\n",
    "        if action_step.action > 0:\n",
    "            cullsteps += 1\n",
    "        policy_state = action_step.state\n",
    "        time_step = environment.step(action_step.action)\n",
    "        if isinstance(environment, HerdEnv):\n",
    "            state = environment.get_state()\n",
    "        else:\n",
    "            state = None # TF environment from wrapper does not have get_state()\n",
    "        episode_return += time_step.reward\n",
    "        if verbose:\n",
    "            print (f\"episode {e:>2} step{i:>3} action: \", action_step.action, \"state=\", state, \"obs=\", time_step.observation, \"reward=\", time_step.reward)\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  cullsteps /= num_episodes\n",
    "  return avg_return, cullsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average return of random policy: -2290.560 avg steps with culls per episode: 26.78\n"
     ]
    }
   ],
   "source": [
    "random_reward, cullsteps = compute_avg_return(henv, random_policy)\n",
    "print (f\"average return of random policy: {random_reward:.3f} avg steps with culls per episode: {cullsteps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 step  1 action:  0 state= [4 1 1 1] obs= [0.01 0.01] reward= 0.0\n",
      "episode  0 step  2 action:  0 state= [5 3 2 2] obs= [0.02 0.02] reward= 0.0\n",
      "episode  0 step  3 action:  0 state= [5 4 3 3] obs= [0.03 0.03] reward= 0.0\n",
      "episode  0 step  4 action:  0 state= [6 6 4 4] obs= [0.04 0.04] reward= 0.0\n",
      "episode  0 step  5 action:  0 state= [6 6 5 5] obs= [0.05 0.05] reward= 0.0\n",
      "episode  0 step  6 action:  0 state= [6 7 6 6] obs= [0.06 0.06] reward= 0.0\n",
      "episode  0 step  7 action:  0 state= [10  9  7  7] obs= [0.07 0.07] reward= 0.0\n",
      "episode  0 step  8 action:  0 state= [11  7  8  8] obs= [0.08 0.08] reward= 0.0\n",
      "episode  0 step  9 action:  0 state= [12  7  9  9] obs= [0.09 0.09] reward= 0.0\n",
      "episode  0 step 10 action:  1 state= [ 0  6  0 10] obs= [0.  0.1] reward= -62.0\n",
      "episode  0 step 11 action:  0 state= [ 1  8  1 11] obs= [0.01 0.11] reward= 0.0\n",
      "episode  0 step 12 action:  0 state= [ 1 10  2 12] obs= [0.02 0.12] reward= 0.0\n",
      "episode  0 step 13 action:  0 state= [ 3 11  3 13] obs= [0.03 0.13] reward= 0.0\n",
      "episode  0 step 14 action:  0 state= [ 4 10  4 14] obs= [0.04 0.14] reward= 0.0\n",
      "episode  0 step 15 action:  0 state= [ 6 10  5 15] obs= [0.05 0.15] reward= 0.0\n",
      "episode  0 step 16 action:  0 state= [ 7 13  6 16] obs= [0.06 0.16] reward= 0.0\n",
      "episode  0 step 17 action:  0 state= [ 8 14  7 17] obs= [0.07 0.17] reward= 0.0\n",
      "episode  0 step 18 action:  0 state= [ 7 13  8 18] obs= [0.08 0.18] reward= 0.0\n",
      "episode  0 step 19 action:  0 state= [ 8 15  9 19] obs= [0.09 0.19] reward= 0.0\n",
      "episode  0 step 20 action:  3 state= [0 0 0 0] obs= [0. 0.] reward= -124.0\n",
      "episode  0 step 21 action:  0 state= [1 0 1 1] obs= [0.01 0.01] reward= 0.0\n",
      "episode  0 step 22 action:  0 state= [2 1 2 2] obs= [0.02 0.02] reward= 0.0\n",
      "episode  0 step 23 action:  0 state= [4 2 3 3] obs= [0.03 0.03] reward= 0.0\n",
      "episode  0 step 24 action:  0 state= [9 3 4 4] obs= [0.04 0.04] reward= 0.0\n",
      "episode  0 step 25 action:  0 state= [13  5  5  5] obs= [0.05 0.05] reward= 0.0\n",
      "episode  0 step 26 action:  0 state= [13  7  6  6] obs= [0.06 0.06] reward= 0.0\n",
      "episode  0 step 27 action:  0 state= [13  9  7  7] obs= [0.07 0.07] reward= 0.0\n",
      "episode  0 step 28 action:  0 state= [14 10  8  8] obs= [0.08 0.08] reward= 0.0\n",
      "episode  0 step 29 action:  0 state= [15 12  9  9] obs= [0.09 0.09] reward= 0.0\n",
      "episode  0 step 30 action:  1 state= [ 0 16  0 10] obs= [0.  0.1] reward= -62.0\n",
      "episode  0 step 31 action:  0 state= [ 0 17  1 11] obs= [0.01 0.11] reward= 0.0\n",
      "episode  0 step 32 action:  0 state= [ 2 18  2 12] obs= [0.02 0.12] reward= 0.0\n",
      "episode  0 step 33 action:  0 state= [ 2 19  3 13] obs= [0.03 0.13] reward= 0.0\n",
      "episode  0 step 34 action:  0 state= [ 3 19  4 14] obs= [0.04 0.14] reward= 0.0\n",
      "episode  0 step 35 action:  0 state= [ 3 19  5 15] obs= [0.05 0.15] reward= 0.0\n",
      "episode  0 step 36 action:  0 state= [ 7 19  6 16] obs= [0.06 0.16] reward= 0.0\n",
      "episode  0 step 37 action:  0 state= [11 17  7 17] obs= [0.07 0.17] reward= 0.0\n",
      "episode  0 step 38 action:  0 state= [14 17  8 18] obs= [0.08 0.18] reward= 0.0\n",
      "episode  0 step 39 action:  0 state= [15 17  9 19] obs= [0.09 0.19] reward= 0.0\n",
      "episode  0 step 40 action:  3 state= [0 0 0 0] obs= [0. 0.] reward= -124.0\n",
      "episode  0 step 41 action:  0 state= [2 2 1 1] obs= [0.01 0.01] reward= 0.0\n",
      "episode  0 step 42 action:  0 state= [3 4 2 2] obs= [0.02 0.02] reward= 0.0\n",
      "episode  0 step 43 action:  0 state= [3 6 3 3] obs= [0.03 0.03] reward= 0.0\n",
      "episode  0 step 44 action:  0 state= [7 7 4 4] obs= [0.04 0.04] reward= 0.0\n",
      "episode  0 step 45 action:  0 state= [8 9 5 5] obs= [0.05 0.05] reward= 0.0\n",
      "episode  0 step 46 action:  0 state= [ 8 12  6  6] obs= [0.06 0.06] reward= 0.0\n",
      "episode  0 step 47 action:  0 state= [ 9 16  7  7] obs= [0.07 0.07] reward= 0.0\n",
      "episode  0 step 48 action:  0 state= [ 9 15  8  8] obs= [0.08 0.08] reward= 0.0\n",
      "episode  0 step 49 action:  0 state= [12 17  9  9] obs= [0.09 0.09] reward= 0.0\n",
      "episode  0 step 50 action:  1 state= [ 0 16  0 10] obs= [0.  0.1] reward= -62.0\n",
      "episode  0 step 51 action:  0 state= [ 0 18  1 11] obs= [0.01 0.11] reward= 0.0\n",
      "episode  0 step 52 action:  0 state= [ 0 19  2 12] obs= [0.02 0.12] reward= 0.0\n",
      "episode  0 step 53 action:  0 state= [ 2 17  3 13] obs= [0.03 0.13] reward= 0.0\n",
      "episode  0 step 54 action:  0 state= [ 7 17  4 14] obs= [0.04 0.14] reward= 0.0\n",
      "episode  0 step 55 action:  0 state= [ 7 16  5 15] obs= [0.05 0.15] reward= 0.0\n",
      "episode  0 step 56 action:  0 state= [ 6 14  6 16] obs= [0.06 0.16] reward= 0.0\n",
      "episode  0 step 57 action:  0 state= [ 8 14  7 17] obs= [0.07 0.17] reward= 0.0\n",
      "episode  0 step 58 action:  0 state= [11 15  8 18] obs= [0.08 0.18] reward= 0.0\n",
      "episode  0 step 59 action:  0 state= [13 15  9 19] obs= [0.09 0.19] reward= 0.0\n",
      "episode  0 step 60 action:  3 state= [0 0 0 0] obs= [0. 0.] reward= -124.0\n",
      "episode  0 step 61 action:  0 state= [1 0 1 1] obs= [0.01 0.01] reward= 0.0\n",
      "episode  0 step 62 action:  0 state= [2 3 2 2] obs= [0.02 0.02] reward= 0.0\n",
      "episode  0 step 63 action:  0 state= [3 5 3 3] obs= [0.03 0.03] reward= 0.0\n",
      "episode  0 step 64 action:  0 state= [5 5 4 4] obs= [0.04 0.04] reward= 0.0\n",
      "episode  0 step 65 action:  0 state= [4 6 5 5] obs= [0.05 0.05] reward= 0.0\n",
      "episode  0 step 66 action:  0 state= [4 9 6 6] obs= [0.06 0.06] reward= 0.0\n",
      "episode  0 step 67 action:  0 state= [7 9 7 7] obs= [0.07 0.07] reward= 0.0\n",
      "episode  0 step 68 action:  0 state= [ 8 10  8  8] obs= [0.08 0.08] reward= 0.0\n",
      "episode  0 step 69 action:  0 state= [ 9 12  9  9] obs= [0.09 0.09] reward= 0.0\n",
      "episode  0 step 70 action:  1 state= [ 0 14  0 10] obs= [0.  0.1] reward= -62.0\n",
      "episode  0 step 71 action:  0 state= [ 0 14  1 11] obs= [0.01 0.11] reward= 0.0\n",
      "episode  0 step 72 action:  0 state= [ 0 16  2 12] obs= [0.02 0.12] reward= 0.0\n",
      "episode  0 step 73 action:  0 state= [ 0 15  3 13] obs= [0.03 0.13] reward= 0.0\n",
      "episode  0 step 74 action:  0 state= [ 2 17  4 14] obs= [0.04 0.14] reward= 0.0\n",
      "episode  0 step 75 action:  0 state= [ 3 14  5 15] obs= [0.05 0.15] reward= 0.0\n",
      "episode  0 step 76 action:  0 state= [ 4 14  6 16] obs= [0.06 0.16] reward= 0.0\n",
      "episode  0 step 77 action:  0 state= [ 8 14  7 17] obs= [0.07 0.17] reward= 0.0\n",
      "episode  0 step 78 action:  0 state= [11 15  8 18] obs= [0.08 0.18] reward= 0.0\n",
      "episode  0 step 79 action:  0 state= [11 16  9 19] obs= [0.09 0.19] reward= 0.0\n",
      "episode  0 step 80 action:  3 state= [0 0 0 0] obs= [0. 0.] reward= -124.0\n",
      "episode  0 step 81 action:  0 state= [1 2 1 1] obs= [0.01 0.01] reward= 0.0\n",
      "episode  0 step 82 action:  0 state= [3 3 2 2] obs= [0.02 0.02] reward= 0.0\n",
      "episode  0 step 83 action:  0 state= [4 4 3 3] obs= [0.03 0.03] reward= 0.0\n",
      "episode  0 step 84 action:  0 state= [7 4 4 4] obs= [0.04 0.04] reward= 0.0\n",
      "episode  0 step 85 action:  0 state= [8 6 5 5] obs= [0.05 0.05] reward= 0.0\n",
      "episode  0 step 86 action:  0 state= [10  6  6  6] obs= [0.06 0.06] reward= 0.0\n",
      "episode  0 step 87 action:  0 state= [11  7  7  7] obs= [0.07 0.07] reward= 0.0\n",
      "episode  0 step 88 action:  0 state= [15  8  8  8] obs= [0.08 0.08] reward= 0.0\n",
      "episode  0 step 89 action:  0 state= [17  8  9  9] obs= [0.09 0.09] reward= 0.0\n",
      "episode  0 step 90 action:  1 state= [ 0  9  0 10] obs= [0.  0.1] reward= -62.0\n",
      "episode  0 step 91 action:  0 state= [ 2 11  1 11] obs= [0.01 0.11] reward= 0.0\n",
      "episode  0 step 92 action:  0 state= [ 3 12  2 12] obs= [0.02 0.12] reward= 0.0\n",
      "episode  0 step 93 action:  0 state= [ 3 16  3 13] obs= [0.03 0.13] reward= 0.0\n",
      "episode  0 step 94 action:  0 state= [ 2 17  4 14] obs= [0.04 0.14] reward= 0.0\n",
      "episode  0 step 95 action:  0 state= [ 4 17  5 15] obs= [0.05 0.15] reward= 0.0\n",
      "episode  0 step 96 action:  0 state= [ 5 17  6 16] obs= [0.06 0.16] reward= 0.0\n",
      "episode  0 step 97 action:  0 state= [ 7 19  7 17] obs= [0.07 0.17] reward= 0.0\n",
      "episode  0 step 98 action:  0 state= [ 9 21  8 18] obs= [0.08 0.18] reward= 0.0\n",
      "episode  0 step 99 action:  0 state= [ 9 21  9 19] obs= [0.09 0.19] reward= 0.0\n",
      "episode  0 step100 action:  3 state= [0 0 0 0] obs= [0. 0.] reward= -124.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-930.0, 10.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show states for one rollout of second scripted policy\n",
    "compute_avg_return(henv, manual_scripted_policy1, num_episodes=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average return of do-nothing-policy: -719.200 avg culls 0.0\n",
      "average return of manual policy: -640.452 avg culls 3.82\n"
     ]
    }
   ],
   "source": [
    "manual_reward0, cullsteps = compute_avg_return(henv, manual_scripted_policy0, num_episodes=500)\n",
    "print (f\"average return of do-nothing-policy: {manual_reward0:.3f} avg culls {cullsteps}\")\n",
    "manual_reward1, cullsteps = compute_avg_return(henv, manual_scripted_policy1, num_episodes=500)\n",
    "print (f\"average return of manual policy: {manual_reward1:.3f} avg culls {cullsteps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Deep-Q Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks.sequential import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 100000\n",
    "replay_buffer_max_length = 100000\n",
    "batch_size = 64\n",
    "num_eval_episodes = 100\n",
    "initial_collect_steps = 100\n",
    "collect_steps_per_iteration = 1\n",
    "log_interval = 200\n",
    "eval_interval = 1000\n",
    "target_update_period = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make actor network simple\n",
    "num_actions = 2**num_herds # this does not scale, obviously\n",
    "kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03)\n",
    "\n",
    "#q_net = Sequential([Dense(300, activation='tanh'),\n",
    "#                    Dense(4, activation=None,\n",
    "#                          kernel_initializer = kernel_initializer)])\n",
    "\n",
    "# from tutorial\n",
    "fc_layer_params = (100, 50)\n",
    "action_tensor_spec = tensor_spec.from_spec(henv.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# it's output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=kernel_initializer,\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(henv)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(henv)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    #boltzmann_temperature = 0.005,\n",
    "    #epsilon_greedy = 0.001,\n",
    "    #gamma=0.99,\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-3),\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "    #target_update_period=target_update_period)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually initialize a reasonably good policy: kill both herds if the sum of observations is large\n",
    "#W = np.array([[0, 3 ,0, 2],[0, 0, 3, 2,]])\n",
    "#b = np.array([1, 0, 0, 0])\n",
    "#q_net.layers[0].set_weights([W,b])\n",
    "#agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mario/.local/lib/python3.8/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mario/.local/lib/python3.8/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mario/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mario/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(\n",
       "{action: (64, 2),\n",
       " discount: (64, 2),\n",
       " next_step_type: (64, 2),\n",
       " observation: (64, 2, 2),\n",
       " policy_info: (),\n",
       " reward: (64, 2),\n",
       " step_type: (64, 2)}), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(\n",
       "{action: tf.int32,\n",
       " discount: tf.float32,\n",
       " next_step_type: tf.int32,\n",
       " observation: tf.float32,\n",
       " policy_info: (),\n",
       " reward: tf.float32,\n",
       " step_type: tf.int32}), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mario/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mario/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1: loss = 5115.5830\tstep 1: average return = -2991.6 cullsteps = 42.4\n",
      "step 20: average return = -737.8 cullsteps = 0.0\n",
      "step 40: average return = -766.8 cullsteps = 0.0\n",
      "step 60: average return = -707.2 cullsteps = 0.0\n",
      "step 80: average return = -695.6 cullsteps = 0.0\n",
      "step = 200: loss = 4256.4712\tstep = 400: loss = 1427.8286\tstep = 600: loss = 544.9510\tstep = 800: loss = 2465.5642\tstep = 1000: loss = 801.6234\tstep 1000: average return = -735.6 cullsteps = 0.0\n",
      "step = 1200: loss = 183.5703\tstep = 1400: loss = 5252.7793\tstep = 1600: loss = 1016.4890\tstep = 1800: loss = 4346.8276\tstep = 2000: loss = 2143.7234\tstep 2000: average return = -684.6 cullsteps = 0.0\n",
      "step = 2200: loss = 133.5474\tstep = 2400: loss = 112.6395\tstep = 2600: loss = 36.8141\tstep = 2800: loss = 3058.1575\tstep = 3000: loss = 166.9824\tstep 3000: average return = -720.0 cullsteps = 0.0\n",
      "step = 3200: loss = 87.5893\tstep = 3400: loss = 6549.0703\tstep = 3600: loss = 3991.1753\tstep = 3800: loss = 11.3276\tstep = 4000: loss = 837.5455\tstep 4000: average return = -679.8 cullsteps = 0.0\n",
      "step = 4200: loss = 3978.6401\tstep = 4400: loss = 3766.4514\tstep = 4600: loss = 140.0810\tstep = 4800: loss = 2556.2395\tstep = 5000: loss = 102.3175\tstep 5000: average return = -707.2 cullsteps = 0.0\n",
      "step = 5200: loss = 108.6931\tstep = 5400: loss = 219.5561\tstep = 5600: loss = 844.3907\tstep = 5800: loss = 3767.3979\tstep = 6000: loss = 2629.2876\tstep 6000: average return = -670.0 cullsteps = 0.0\n",
      "step = 6200: loss = 210.4031\tstep = 6400: loss = 68.3980\tstep = 6600: loss = 6548.0410\tstep = 6800: loss = 1308.8479\tstep = 7000: loss = 2583.5066\tstep 7000: average return = -752.4 cullsteps = 0.0\n",
      "step = 7200: loss = 837.0793\tstep = 7400: loss = 5910.4326\tstep = 7600: loss = 9574.3516\tstep = 7800: loss = 450.9033\tstep = 8000: loss = 49.0046\tstep 8000: average return = -699.4 cullsteps = 0.0\n",
      "step = 8200: loss = 2852.5288\tstep = 8400: loss = 5000.6465\tstep = 8600: loss = 1812.6769\tstep = 8800: loss = 4624.2881\tstep = 9000: loss = 578.9772\tstep 9000: average return = -694.4 cullsteps = 0.0\n",
      "step = 9200: loss = 44.2684\tstep = 9400: loss = 2201.4431\tstep = 9600: loss = 4318.4604\tstep = 9800: loss = 317.3435\tstep = 10000: loss = 81.6265\tstep 10000: average return = -694.8 cullsteps = 0.0\n",
      "step = 10200: loss = 3883.5481\tstep = 10400: loss = 3614.1519\tstep = 10600: loss = 5733.1143\tstep = 10800: loss = 62.9609\tstep = 11000: loss = 3930.1182\tstep 11000: average return = -722.6 cullsteps = 0.0\n",
      "step = 11200: loss = 4636.2559\tstep = 11400: loss = 996.0366\tstep = 11600: loss = 184.1822\tstep = 11800: loss = 457.2879\tstep = 12000: loss = 299.1270\tstep 12000: average return = -672.8 cullsteps = 0.0\n",
      "step = 12200: loss = 3631.6182\tstep = 12400: loss = 341.8179\tstep = 12600: loss = 389.6373\tstep = 12800: loss = 156.7279\tstep = 13000: loss = 3565.4614\tstep 13000: average return = -737.5 cullsteps = 0.2\n",
      "step = 13200: loss = 513.2780\tstep = 13400: loss = 67.8398\tstep = 13600: loss = 837.9203\tstep = 13800: loss = 362.5079\tstep = 14000: loss = 78.9528\tstep 14000: average return = -718.8 cullsteps = 0.0\n",
      "step = 14200: loss = 273.2810\tstep = 14400: loss = 7549.6514\tstep = 14600: loss = 6020.8311\tstep = 14800: loss = 299.8001\tstep = 15000: loss = 3037.7471\tstep 15000: average return = -682.2 cullsteps = 0.6\n",
      "step = 15200: loss = 146.4061\tstep = 15400: loss = 4858.3491\tstep = 15600: loss = 481.9756\tstep = 15800: loss = 2332.3923\tstep = 16000: loss = 2546.3586\tstep 16000: average return = -658.3 cullsteps = 0.8\n",
      "step = 16200: loss = 5703.3901\tstep = 16400: loss = 5771.6348\tstep = 16600: loss = 546.4495\tstep = 16800: loss = 577.4074\tstep = 17000: loss = 907.3649\tstep 17000: average return = -658.4 cullsteps = 0.8\n",
      "step = 17200: loss = 288.9101\tstep = 17400: loss = 69.2440\tstep = 17600: loss = 368.2837\tstep = 17800: loss = 716.2748\tstep = 18000: loss = 35.8898\tstep 18000: average return = -696.4 cullsteps = 1.6\n",
      "step = 18200: loss = 142.2183\tstep = 18400: loss = 57.6641\tstep = 18600: loss = 1590.4263\tstep = 18800: loss = 657.9960\tstep = 19000: loss = 1806.7375\tstep 19000: average return = -792.7 cullsteps = 4.2\n",
      "step = 19200: loss = 874.6661\tstep = 19400: loss = 68.2483\tstep = 19600: loss = 3297.4890\tstep = 19800: loss = 3712.5530\tstep = 20000: loss = 4042.6509\tstep 20000: average return = -855.4 cullsteps = 4.8\n",
      "step = 20200: loss = 5676.2524\tstep = 20400: loss = 1993.4836\tstep = 20600: loss = 1618.9263\tstep = 20800: loss = 93.1568\tstep = 21000: loss = 50.6606\tstep 21000: average return = -687.4 cullsteps = 0.8\n",
      "step = 21200: loss = 6254.2632\tstep = 21400: loss = 98.6338\tstep = 21600: loss = 1455.2418\tstep = 21800: loss = 1696.7052\tstep = 22000: loss = 404.6685\tstep 22000: average return = -685.3 cullsteps = 0.9\n",
      "step = 22200: loss = 25.7925\tstep = 22400: loss = 1943.6470\tstep = 22600: loss = 1024.0077\tstep = 22800: loss = 3312.2026\tstep = 23000: loss = 4039.0315\tstep 23000: average return = -647.4 cullsteps = 0.7\n",
      "step = 23200: loss = 39.5307\tstep = 23400: loss = 59.1975\tstep = 23600: loss = 4175.9688\tstep = 23800: loss = 5349.4790\tstep = 24000: loss = 1338.2069\tstep 24000: average return = -663.6 cullsteps = 0.7\n",
      "step = 24200: loss = 2728.0791\tstep = 24400: loss = 5561.5659\tstep = 24600: loss = 4696.7705\tstep = 24800: loss = 379.1816\tstep = 25000: loss = 2223.6157\tstep 25000: average return = -745.3 cullsteps = 2.6\n",
      "step = 25200: loss = 135.7014\tstep = 25400: loss = 3343.6926\tstep = 25600: loss = 145.5973\tstep = 25800: loss = 7307.8491\tstep = 26000: loss = 311.7620\tstep 26000: average return = -722.6 cullsteps = 0.9\n",
      "step = 26200: loss = 1561.9453\tstep = 26400: loss = 4037.3127\tstep = 26600: loss = 110.1917\tstep = 26800: loss = 2413.7822\tstep = 27000: loss = 1931.2024\tstep 27000: average return = -692.2 cullsteps = 0.8\n",
      "step = 27200: loss = 889.2502\tstep = 27400: loss = 36.7155\tstep = 27600: loss = 8233.9287\tstep = 27800: loss = 267.6523\tstep = 28000: loss = 37.1106\tstep 28000: average return = -701.3 cullsteps = 0.8\n",
      "step = 28200: loss = 1456.2264\tstep = 28400: loss = 44.6668\tstep = 28600: loss = 3646.3716\tstep = 28800: loss = 2406.8723\tstep = 29000: loss = 169.4002\tstep 29000: average return = -634.3 cullsteps = 0.7\n",
      "step = 29200: loss = 1630.5674\tstep = 29400: loss = 6528.7241\tstep = 29600: loss = 2944.7732\tstep = 29800: loss = 409.7296\tstep = 30000: loss = 4071.8228\tstep 30000: average return = -690.6 cullsteps = 0.7\n",
      "step = 30200: loss = 7247.8545\tstep = 30400: loss = 85.9886\tstep = 30600: loss = 32.6884\tstep = 30800: loss = 5139.1875\tstep = 31000: loss = 459.2781\tstep 31000: average return = -662.4 cullsteps = 0.6\n",
      "step = 31200: loss = 3805.5371\tstep = 31400: loss = 190.0707\tstep = 31600: loss = 3510.4014\tstep = 31800: loss = 4979.8662\tstep = 32000: loss = 1342.3890\tstep 32000: average return = -685.7 cullsteps = 0.8\n",
      "step = 32200: loss = 375.4305\tstep = 32400: loss = 1012.6230\tstep = 32600: loss = 32.0674\tstep = 32800: loss = 40.7703\tstep = 33000: loss = 1887.9182\tstep 33000: average return = -667.0 cullsteps = 0.7\n",
      "step = 33200: loss = 445.4357\tstep = 33400: loss = 9054.6562\tstep = 33600: loss = 234.1178\tstep = 33800: loss = 950.6127\tstep = 34000: loss = 3744.1458\tstep 34000: average return = -695.6 cullsteps = 0.6\n",
      "step = 34200: loss = 5894.0908\tstep = 34400: loss = 188.7401\tstep = 34600: loss = 613.1036\tstep = 34800: loss = 3239.5361\tstep = 35000: loss = 36.0491\tstep 35000: average return = -659.8 cullsteps = 0.6\n",
      "step = 35200: loss = 301.5631\tstep = 35400: loss = 8630.8828\tstep = 35600: loss = 99.5750\tstep = 35800: loss = 4202.3779\tstep = 36000: loss = 1374.0077\tstep 36000: average return = -695.8 cullsteps = 0.8\n",
      "step = 36200: loss = 3049.0757\tstep = 36400: loss = 547.2017\tstep = 36600: loss = 35.7188\tstep = 36800: loss = 115.1366\tstep = 37000: loss = 1219.9323\tstep 37000: average return = -617.3 cullsteps = 0.6\n",
      "step = 37200: loss = 2488.6724\tstep = 37400: loss = 575.4748\tstep = 37600: loss = 868.6849\tstep = 37800: loss = 3268.2329\tstep = 38000: loss = 55.7245\tstep 38000: average return = -619.7 cullsteps = 0.7\n",
      "step = 38200: loss = 2042.3481\tstep = 38400: loss = 4207.3970\tstep = 38600: loss = 6508.1948\tstep = 38800: loss = 438.7923\tstep = 39000: loss = 123.1320\tstep 39000: average return = -680.1 cullsteps = 0.8\n",
      "step = 39200: loss = 60.4330\tstep = 39400: loss = 1621.8146\tstep = 39600: loss = 47.4707\tstep = 39800: loss = 1337.5660\tstep = 40000: loss = 5772.8330\tstep 40000: average return = -676.0 cullsteps = 0.7\n",
      "step = 40200: loss = 46.6714\tstep = 40400: loss = 66.4451\tstep = 40600: loss = 5334.7690\tstep = 40800: loss = 73.4183\tstep = 41000: loss = 1031.2445\tstep 41000: average return = -629.6 cullsteps = 0.8\n",
      "step = 41200: loss = 2339.9175\tstep = 41400: loss = 310.9486\tstep = 41600: loss = 1170.6493\tstep = 41800: loss = 7077.0801\tstep = 42000: loss = 30.0646\tstep 42000: average return = -701.7 cullsteps = 0.6\n",
      "step = 42200: loss = 79.7466\tstep = 42400: loss = 3404.0515\tstep = 42600: loss = 47.1043\tstep = 42800: loss = 319.7144\tstep = 43000: loss = 1205.1294\tstep 43000: average return = -671.5 cullsteps = 0.8\n",
      "step = 43200: loss = 75.1685\tstep = 43400: loss = 2674.7468\tstep = 43600: loss = 1614.9744\tstep = 43800: loss = 2535.3220\tstep = 44000: loss = 4517.4546\tstep 44000: average return = -616.6 cullsteps = 0.9\n",
      "step = 44200: loss = 1683.7329\tstep = 44400: loss = 5190.5581\tstep = 44600: loss = 33.4460\tstep = 44800: loss = 45.2174\tstep = 45000: loss = 46.6170\tstep 45000: average return = -728.7 cullsteps = 0.8\n",
      "step = 45200: loss = 427.2086\tstep = 45400: loss = 9984.1084\tstep = 45600: loss = 5591.5068\tstep = 45800: loss = 3053.0261\tstep = 46000: loss = 422.6143\tstep 46000: average return = -690.9 cullsteps = 0.6\n",
      "step = 46200: loss = 1851.2222\tstep = 46400: loss = 56.7374\tstep = 46600: loss = 72.1739\tstep = 46800: loss = 1318.5835\tstep = 47000: loss = 2425.8394\tstep 47000: average return = -650.0 cullsteps = 0.7\n",
      "step = 47200: loss = 36.2311\tstep = 47400: loss = 46.9907\tstep = 47600: loss = 114.8741\tstep = 47800: loss = 27.2685\tstep = 48000: loss = 3436.7400\tstep 48000: average return = -707.8 cullsteps = 0.7\n",
      "step = 48200: loss = 2076.1919\tstep = 48400: loss = 4973.8320\tstep = 48600: loss = 141.2292\tstep = 48800: loss = 1352.0660\tstep = 49000: loss = 59.3964\tstep 49000: average return = -676.3 cullsteps = 0.7\n",
      "step = 49200: loss = 969.7917\tstep = 49400: loss = 1418.7327\tstep = 49600: loss = 36.0814\tstep = 49800: loss = 1420.0992\tstep = 50000: loss = 45.5900\tstep 50000: average return = -660.8 cullsteps = 0.7\n",
      "step = 50200: loss = 508.2237\tstep = 50400: loss = 59.4968\tstep = 50600: loss = 3928.3748\tstep = 50800: loss = 1510.0557\tstep = 51000: loss = 4337.0825\tstep 51000: average return = -644.5 cullsteps = 0.6\n",
      "step = 51200: loss = 326.0998\tstep = 51400: loss = 4312.8862\tstep = 51600: loss = 142.1052\tstep = 51800: loss = 2998.4854\tstep = 52000: loss = 26.5909\tstep 52000: average return = -655.0 cullsteps = 0.8\n",
      "step = 52200: loss = 97.8387\tstep = 52400: loss = 38.4919\tstep = 52600: loss = 2100.8423\tstep = 52800: loss = 2169.1794\tstep = 53000: loss = 142.6430\tstep 53000: average return = -733.0 cullsteps = 0.7\n",
      "step = 53200: loss = 59.2358\tstep = 53400: loss = 276.8520\tstep = 53600: loss = 86.8198\tstep = 53800: loss = 156.2986\tstep = 54000: loss = 698.8091\tstep 54000: average return = -694.4 cullsteps = 0.8\n",
      "step = 54200: loss = 48.7780\tstep = 54400: loss = 838.8889\tstep = 54600: loss = 101.8067\tstep = 54800: loss = 5233.6948\tstep = 55000: loss = 2821.8076\tstep 55000: average return = -663.2 cullsteps = 0.8\n",
      "step = 55200: loss = 1131.8463\tstep = 55400: loss = 27.2911\tstep = 55600: loss = 115.8452\tstep = 55800: loss = 5448.7251\tstep = 56000: loss = 4068.9153\tstep 56000: average return = -650.0 cullsteps = 0.8\n",
      "step = 56200: loss = 35.7377\tstep = 56400: loss = 1351.9923\tstep = 56600: loss = 7355.8477\tstep = 56800: loss = 173.4109\tstep = 57000: loss = 4382.3491\tstep 57000: average return = -681.9 cullsteps = 0.8\n",
      "step = 57200: loss = 89.5325\tstep = 57400: loss = 1100.0314\tstep = 57600: loss = 12428.8066\tstep = 57800: loss = 165.0019\tstep = 58000: loss = 235.2999\tstep 58000: average return = -641.1 cullsteps = 0.6\n",
      "step = 58200: loss = 29.8241\tstep = 58400: loss = 3658.1174\tstep = 58600: loss = 766.1620\tstep = 58800: loss = 3295.3286\tstep = 59000: loss = 37.8285\tstep 59000: average return = -714.8 cullsteps = 0.7\n",
      "step = 59200: loss = 68.1499\tstep = 59400: loss = 1035.4882\tstep = 59600: loss = 62.5387\tstep = 59800: loss = 367.8351\tstep = 60000: loss = 25.6761\tstep 60000: average return = -632.1 cullsteps = 0.5\n",
      "step = 60200: loss = 57.8064\tstep = 60400: loss = 3208.7505\tstep = 60600: loss = 1354.8521\tstep = 60800: loss = 54.4774\tstep = 61000: loss = 35.0163\tstep 61000: average return = -606.1 cullsteps = 1.0\n",
      "step = 61200: loss = 709.8096\tstep = 61400: loss = 1799.3433\tstep = 61600: loss = 68.9496\tstep = 61800: loss = 58.4046\tstep = 62000: loss = 6658.1357\tstep 62000: average return = -689.9 cullsteps = 0.6\n",
      "step = 62200: loss = 2091.6250\tstep = 62400: loss = 201.6186\tstep = 62600: loss = 6354.9229\tstep = 62800: loss = 36.9269\tstep = 63000: loss = 1265.9646\tstep 63000: average return = -647.9 cullsteps = 0.7\n",
      "step = 63200: loss = 59.0812\tstep = 63400: loss = 3021.4038\tstep = 63600: loss = 33.7902\tstep = 63800: loss = 4264.7764\tstep = 64000: loss = 4034.2678\tstep 64000: average return = -656.5 cullsteps = 0.7\n",
      "step = 64200: loss = 355.0164\tstep = 64400: loss = 2446.7822\tstep = 64600: loss = 2813.6631\tstep = 64800: loss = 252.6030\tstep = 65000: loss = 634.3301\tstep 65000: average return = -711.0 cullsteps = 0.9\n",
      "step = 65200: loss = 38.6299\tstep = 65400: loss = 1149.0344\tstep = 65600: loss = 79.8381\tstep = 65800: loss = 4800.9658\tstep = 66000: loss = 1264.1705\tstep 66000: average return = -627.5 cullsteps = 0.9\n",
      "step = 66200: loss = 6113.5820\tstep = 66400: loss = 3701.8623\tstep = 66600: loss = 164.6851\tstep = 66800: loss = 1604.2872\tstep = 67000: loss = 1546.3312\tstep 67000: average return = -715.5 cullsteps = 0.7\n",
      "step = 67200: loss = 33.3936\tstep = 67400: loss = 529.1198\tstep = 67600: loss = 4404.9697\tstep = 67800: loss = 6295.6973\tstep = 68000: loss = 3028.9771\tstep 68000: average return = -582.8 cullsteps = 1.1\n",
      "step = 68200: loss = 1912.8782\tstep = 68400: loss = 146.3684\tstep = 68600: loss = 457.4273\tstep = 68800: loss = 2185.5232\tstep = 69000: loss = 687.9884\tstep 69000: average return = -697.2 cullsteps = 0.6\n",
      "step = 69200: loss = 34.0430\tstep = 69400: loss = 2478.7891\tstep = 69600: loss = 3836.3232\tstep = 69800: loss = 2094.1194\tstep = 70000: loss = 63.2138\tstep 70000: average return = -617.6 cullsteps = 1.1\n",
      "step = 70200: loss = 39.2063\tstep = 70400: loss = 4229.7739\tstep = 70600: loss = 6191.8872\tstep = 70800: loss = 46.5182\tstep = 71000: loss = 171.8229\tstep 71000: average return = -655.3 cullsteps = 1.2\n",
      "step = 71200: loss = 5013.9761\tstep = 71400: loss = 746.0499\tstep = 71600: loss = 1775.1814\tstep = 71800: loss = 52.1151\tstep = 72000: loss = 31.3583\tstep 72000: average return = -693.9 cullsteps = 0.7\n",
      "step = 72200: loss = 49.4481\tstep = 72400: loss = 4360.9136\tstep = 72600: loss = 504.5847\tstep = 72800: loss = 2889.5215\tstep = 73000: loss = 2530.0334\tstep 73000: average return = -648.2 cullsteps = 0.7\n",
      "step = 73200: loss = 176.1254\tstep = 73400: loss = 67.3512\tstep = 73600: loss = 1016.0706\tstep = 73800: loss = 5062.9541\tstep = 74000: loss = 63.3062\tstep 74000: average return = -707.9 cullsteps = 0.7\n",
      "step = 74200: loss = 4335.4126\tstep = 74400: loss = 974.7223\tstep = 74600: loss = 2675.1533\tstep = 74800: loss = 4038.5754\tstep = 75000: loss = 4337.5420\tstep 75000: average return = -637.4 cullsteps = 1.0\n",
      "step = 75200: loss = 511.3400\tstep = 75400: loss = 368.6386\tstep = 75600: loss = 995.9824\tstep = 75800: loss = 558.4739\tstep = 76000: loss = 1447.1316\tstep 76000: average return = -673.7 cullsteps = 1.1\n",
      "step = 76200: loss = 477.7025\tstep = 76400: loss = 1955.1364\tstep = 76600: loss = 41.3404\tstep = 76800: loss = 1040.9536\tstep = 77000: loss = 78.3458\tstep 77000: average return = -589.6 cullsteps = 1.0\n",
      "step = 77200: loss = 3136.4998\tstep = 77400: loss = 277.1767\tstep = 77600: loss = 3102.9495\tstep = 77800: loss = 6360.6113\tstep = 78000: loss = 156.4045\tstep 78000: average return = -616.8 cullsteps = 0.9\n",
      "step = 78200: loss = 4552.7710\tstep = 78400: loss = 2536.0781\tstep = 78600: loss = 1838.5234\tstep = 78800: loss = 343.3014\tstep = 79000: loss = 4302.4302\tstep 79000: average return = -646.5 cullsteps = 0.6\n",
      "step = 79200: loss = 3533.9353\tstep = 79400: loss = 251.5468\tstep = 79600: loss = 3035.6370\tstep = 79800: loss = 314.4105\tstep = 80000: loss = 4982.3198\tstep 80000: average return = -682.2 cullsteps = 0.7\n",
      "step = 80200: loss = 1653.9733\tstep = 80400: loss = 2579.8931\tstep = 80600: loss = 3531.9153\tstep = 80800: loss = 11158.9707\tstep = 81000: loss = 6638.9604\tstep 81000: average return = -640.8 cullsteps = 0.7\n",
      "step = 81200: loss = 2045.1299\tstep = 81400: loss = 777.1741\tstep = 81600: loss = 303.3944\tstep = 81800: loss = 4208.6035\tstep = 82000: loss = 745.0662\tstep 82000: average return = -592.1 cullsteps = 1.2\n",
      "step = 82200: loss = 3248.9951\tstep = 82400: loss = 171.0866\tstep = 82600: loss = 571.5538\tstep = 82800: loss = 47.3741\tstep = 83000: loss = 1387.9504\tstep 83000: average return = -668.6 cullsteps = 0.7\n",
      "step = 83200: loss = 333.5696\tstep = 83400: loss = 984.5186\tstep = 83600: loss = 2448.5051\tstep = 83800: loss = 3119.0217\tstep = 84000: loss = 3425.4431\tstep 84000: average return = -581.2 cullsteps = 0.9\n",
      "step = 84200: loss = 8296.7539\tstep = 84400: loss = 367.6451\tstep = 84600: loss = 245.4380\tstep = 84800: loss = 8731.8984\tstep = 85000: loss = 30.5972\tstep 85000: average return = -649.8 cullsteps = 0.8\n",
      "step = 85200: loss = 5246.8389\tstep = 85400: loss = 72.5777\tstep = 85600: loss = 1510.6550\tstep = 85800: loss = 275.8676\tstep = 86000: loss = 699.2637\tstep 86000: average return = -610.9 cullsteps = 0.9\n",
      "step = 86200: loss = 288.2076\tstep = 86400: loss = 35.0648\tstep = 86600: loss = 386.1092\tstep = 86800: loss = 4824.2808\tstep = 87000: loss = 60.4079\tstep 87000: average return = -641.8 cullsteps = 1.1\n",
      "step = 87200: loss = 43.1077\tstep = 87400: loss = 1036.5815\tstep = 87600: loss = 1951.7394\tstep = 87800: loss = 42.3451\tstep = 88000: loss = 675.2390\tstep 88000: average return = -656.6 cullsteps = 1.0\n",
      "step = 88200: loss = 9442.4082\tstep = 88400: loss = 1821.6467\tstep = 88600: loss = 198.6436\tstep = 88800: loss = 1825.9614\tstep = 89000: loss = 213.4405\tstep 89000: average return = -683.4 cullsteps = 1.1\n",
      "step = 89200: loss = 882.9537\tstep = 89400: loss = 2348.9363\tstep = 89600: loss = 2250.0498\tstep = 89800: loss = 80.2677\tstep = 90000: loss = 4564.6787\tstep 90000: average return = -723.7 cullsteps = 0.7\n",
      "step = 90200: loss = 33.7157\tstep = 90400: loss = 1794.6819\tstep = 90600: loss = 1206.0521\tstep = 90800: loss = 947.5577\tstep = 91000: loss = 172.5512\tstep 91000: average return = -721.2 cullsteps = 0.7\n",
      "step = 91200: loss = 2060.3157\tstep = 91400: loss = 6449.8696\tstep = 91600: loss = 895.1641\tstep = 91800: loss = 1537.7358\tstep = 92000: loss = 70.5896\tstep 92000: average return = -630.0 cullsteps = 1.1\n",
      "step = 92200: loss = 1537.5063\tstep = 92400: loss = 2948.4297\tstep = 92600: loss = 331.5319\tstep = 92800: loss = 2033.7753\tstep = 93000: loss = 7712.5552\tstep 93000: average return = -614.6 cullsteps = 0.9\n",
      "step = 93200: loss = 2083.0249\tstep = 93400: loss = 6623.3501\tstep = 93600: loss = 26.7766\tstep = 93800: loss = 335.7442\tstep = 94000: loss = 51.5774\tstep 94000: average return = -681.7 cullsteps = 1.1\n",
      "step = 94200: loss = 43.7779\tstep = 94400: loss = 46.8038\tstep = 94600: loss = 1906.4443\tstep = 94800: loss = 2572.4478\tstep = 95000: loss = 58.2160\tstep 95000: average return = -604.4 cullsteps = 1.1\n",
      "step = 95200: loss = 2395.6313\tstep = 95400: loss = 958.9655\tstep = 95600: loss = 1783.7498\tstep = 95800: loss = 644.9393\tstep = 96000: loss = 38.8443\tstep 96000: average return = -686.1 cullsteps = 0.7\n",
      "step = 96200: loss = 28.0566\tstep = 96400: loss = 43.9891\tstep = 96600: loss = 30.3327\tstep = 96800: loss = 602.2629\tstep = 97000: loss = 66.6417\tstep 97000: average return = -668.0 cullsteps = 0.9\n",
      "step = 97200: loss = 4093.9536\tstep = 97400: loss = 60.9413\tstep = 97600: loss = 1599.4900\tstep = 97800: loss = 39.8855\tstep = 98000: loss = 1827.5754\tstep 98000: average return = -663.6 cullsteps = 1.0\n",
      "step = 98200: loss = 31.3004\tstep = 98400: loss = 7253.7803\tstep = 98600: loss = 494.6267\tstep = 98800: loss = 2946.2522\tstep = 99000: loss = 103.3152\tstep 99000: average return = -716.1 cullsteps = 0.8\n",
      "step = 99200: loss = 2812.7449\tstep = 99400: loss = 135.7595\tstep = 99600: loss = 126.4306\tstep = 99800: loss = 1173.5042\tstep = 100000: loss = 5297.6191\tstep 100000: average return = -658.0 cullsteps = 0.8\n"
     ]
    }
   ],
   "source": [
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step <= 1 or step % log_interval == 0:\n",
    "    print('step = {0:4>}: loss = {1:.4f}'.format(step, train_loss), end=\"\\t\")\n",
    "\n",
    "  if step <= 1 or (step < 100 and step % 20 == 0) or step % eval_interval == 0:\n",
    "    avg_return, cullsteps = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step {0}: average return = {1:.1f} cullsteps = {2:.1f}'.format(step, avg_return.numpy().item(), cullsteps))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward of learned policy:  [-683.936] cullsteps= 0.744\n"
     ]
    }
   ],
   "source": [
    "learned_reward, culleps = compute_avg_return(eval_env, agent.policy, num_episodes=500)\n",
    "print (\"reward of learned policy: \", learned_reward.numpy(), \"cullsteps=\", culleps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ts = eval_env.reset()\n",
    "\n",
    "def get_action(obs):\n",
    "    \"\"\" execute the learned policy network \n",
    "       obs:  one float for each herd - the time since last culling\n",
    "    \"\"\"\n",
    "    _ts = TimeStep(tf.constant([0.]),\n",
    "                   tf.constant([0.]),\n",
    "                   tf.constant([1]),\n",
    "                   tf.constant([obs]))\n",
    "    # a = agent.collect_policy.action(_ts) # just to see how much is explored versus exploited\n",
    "    a = agent.policy.action(_ts)\n",
    "    return a.action.numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.greedy_policy.GreedyPolicy at 0x7f5c380e6a00>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what the learned policy does on a grid of observations (5 steps per row&col)\n",
    "A = [[get_action([x,y])\n",
    " for y in np.arange(0.,1.,.05,np.float32)]\n",
    " for x in np.arange(0.,1.,.05,np.float32)]\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with parameters of manually designed q_network policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W, b = agent.policy.trainable_variables\n",
    "# W = W.numpy()\n",
    "# b = b.numpy()\n",
    "# print (\"weights\\n\", W, \"\\nbias\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nn(obs):\n",
    "#    y = np.dot(obs, W)+b\n",
    "#    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn([0.5,.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
