{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the environment `HerdEnv` of `herd_env.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "from tf_agents.trajectories import TimeStep\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from herd_env_ddpg import HerdEnv_DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "henv_val = HerdEnv_DDPG(herd_sizes = [64,64], rand_recovery_prob = 0.1, rand_infection_prob = 0.05)\n",
    "utils.validate_py_environment(henv_val, episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Herd Environment instance to be trained for\n",
    "max_episode_length=100\n",
    "num_herds=2\n",
    "henv = HerdEnv_DDPG(herd_sizes = [32,32], expected_episode_length=-1, max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.04, rand_infection_prob = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': 1.0,\n",
      " 'observation': array([0., 0., 0.], dtype=float32),\n",
      " 'reward': 0.0,\n",
      " 'step_type': array(0, dtype=int32)})\n",
      "state:  [1 2 1 2 1 1] observation:  [0.01 0.01 0.01] \treward:  0.0\n",
      "state:  [3 2 4 4 2 2] observation:  [0.02 0.02 0.02] \treward:  0.0\n",
      "state:  [3 3 7 7 3 3] observation:  [0.03 0.03 0.03] \treward:  0.0\n",
      "state:  [ 3  6 10 13  4  4] observation:  [0.04 0.04 0.04] \treward:  0.0\n",
      "state:  [ 3  7 13 20  5  5] observation:  [0.05 0.05 0.05] \treward:  0.0\n",
      "state:  [ 5  9 18 29  6  6] observation:  [0.06 0.06 0.06] \treward:  0.0\n",
      "state:  [ 5 10 23 39  7  7] observation:  [0.07 0.07 0.07] \treward:  0.0\n",
      "state:  [ 6 11 29 50  8  8] observation:  [0.08 0.08 0.08] \treward:  0.0\n",
      "state:  [ 9 11 38 61  9  9] observation:  [0.09 0.09 0.09] \treward:  0.0\n",
      "state:  [11 11 49 72 10 10] observation:  [0.1 0.1 0.1] \treward:  0.0\n",
      "state:  [11 13 60 85 11 11] observation:  [0.11 0.11 0.11] \treward:  0.0\n",
      "state:  [11 13 71 98 12 12] observation:  [0.12 0.12 0.12] \treward:  0.0\n",
      "state:  [ 11  12  82 110  13  13] observation:  [0.13 0.13 0.13] \treward:  0.0\n",
      "state:  [ 13  12  95 122  14  14] observation:  [0.14 0.14 0.14] \treward:  0.0\n",
      "state:  [ 14  12 109 134  15  15] observation:  [0.15 0.15 0.15] \treward:  0.0\n",
      "state:  [ 14  13 123 147  16  16] observation:  [0.16 0.16 0.16] \treward:  0.0\n",
      "state:  [ 15  13 138 160  17  17] observation:  [0.17 0.17 0.17] \treward:  0.0\n",
      "state:  [ 16  12 154 172  18  18] observation:  [0.18 0.18 0.18] \treward:  0.0\n",
      "state:  [ 15  12 169 184  19  19] observation:  [0.19 0.19 0.19] \treward:  0.0\n",
      "state:  [ 15  14 184 198  20  20] observation:  [0.2 0.2 0.2] \treward:  0.0\n",
      "state:  [ 14  15 198 213  21  21] observation:  [0.21 0.21 0.21] \treward:  0.0\n",
      "state:  [ 15  16 213 229  22  22] observation:  [0.22 0.22 0.22] \treward:  0.0\n",
      "state:  [ 15  17 228 246  23  23] observation:  [0.23 0.23 0.23] \treward:  0.0\n",
      "state:  [ 15  16 243 262  24  24] observation:  [0.24 0.24 0.24] \treward:  0.0\n",
      "state:  [ 17  16 260 278  25  25] observation:  [0.25 0.25 0.25] \treward:  0.0\n",
      "state:  [ 16  17 276 295  26  26] observation:  [0.26 0.26 0.26] \treward:  0.0\n",
      "state:  [ 20  16 296 311  27  27] observation:  [0.27 0.27 0.27] \treward:  0.0\n",
      "state:  [ 21  14 317 325  28  28] observation:  [0.28 0.28 0.28] \treward:  0.0\n",
      "state:  [ 19  16 336 341  29  29] observation:  [0.29 0.29 0.29] \treward:  0.0\n",
      "state:  [ 18  18 354 359  30  30] observation:  [0.3 0.3 0.3] \treward:  0.0\n",
      "state:  [ 19  18 373 377  31  31] observation:  [0.31 0.31 0.31] \treward:  0.0\n",
      "state:  [ 19  19 392 396  32  32] observation:  [0.32 0.32 0.32] \treward:  0.0\n",
      "state:  [ 21  18 413 414  33  33] observation:  [0.33 0.33 0.33] \treward:  0.0\n",
      "state:  [ 24  20 437 434  34  34] observation:  [0.34 0.34 0.34] \treward:  0.0\n",
      "state:  [ 23  19 460 453  35  35] observation:  [0.35 0.35 0.35] \treward:  0.0\n",
      "state:  [ 24  18 484 471  36  36] observation:  [0.36 0.36 0.36] \treward:  0.0\n",
      "state:  [ 23  18 507 489  37  37] observation:  [0.37 0.37 0.37] \treward:  0.0\n",
      "state:  [ 24  16 531 505  38  38] observation:  [0.38 0.38 0.38] \treward:  0.0\n",
      "state:  [ 24  18 555 523  39  39] observation:  [0.39 0.39 0.39] \treward:  0.0\n",
      "state:  [ 26  19 581 542  40  40] observation:  [0.4 0.4 0.4] \treward:  0.0\n",
      "state:  [ 28  18 609 560  41  41] observation:  [0.41 0.41 0.41] \treward:  0.0\n",
      "state:  [ 27  18 636 578  42  42] observation:  [0.42 0.42 0.42] \treward:  0.0\n",
      "state:  [ 25  16 661 594  43  43] observation:  [0.43 0.43 0.43] \treward:  0.0\n",
      "state:  [ 27  15 688 609  44  44] observation:  [0.44 0.44 0.44] \treward:  0.0\n",
      "state:  [ 26  16 714 625  45  45] observation:  [0.45 0.45 0.45] \treward:  0.0\n",
      "state:  [ 25  15 739 640  46  46] observation:  [0.46 0.46 0.46] \treward:  0.0\n",
      "state:  [ 26  16 765 656  47  47] observation:  [0.47 0.47 0.47] \treward:  0.0\n",
      "state:  [ 26  17 791 673  48  48] observation:  [0.48 0.48 0.48] \treward:  0.0\n",
      "state:  [ 26  17 817 690  49  49] observation:  [0.49 0.49 0.49] \treward:  0.0\n",
      "state:  [ 25  17 842 707  50  50] observation:  [0.5 0.5 0.5] \treward:  0.0\n",
      "state:  [ 25  19 867 726  51  51] observation:  [0.51 0.51 0.51] \treward:  0.0\n",
      "state:  [ 26  19 893 745  52  52] observation:  [0.52 0.52 0.52] \treward:  0.0\n",
      "state:  [ 25  18 918 763  53  53] observation:  [0.53 0.53 0.53] \treward:  0.0\n",
      "state:  [ 25  19 943 782  54  54] observation:  [0.54 0.54 0.54] \treward:  0.0\n",
      "state:  [ 23  20 966 802  55  55] observation:  [0.55 0.55 0.55] \treward:  0.0\n",
      "state:  [ 23  19 989 821  56  56] observation:  [0.56 0.56 0.56] \treward:  0.0\n",
      "state:  [  24   19 1013  840   57   57] observation:  [0.57 0.57 0.57] \treward:  0.0\n",
      "state:  [  22   21 1035  861   58   58] observation:  [0.58 0.58 0.58] \treward:  0.0\n",
      "state:  [  23   18 1058  879   59   59] observation:  [0.59 0.59 0.59] \treward:  0.0\n",
      "state:  [  23   20 1081  899   60   60] observation:  [0.6 0.6 0.6] \treward:  0.0\n",
      "state:  [  22   20 1103  919   61   61] observation:  [0.61 0.61 0.61] \treward:  0.0\n",
      "state:  [  24   20 1127  939   62   62] observation:  [0.62 0.62 0.62] \treward:  0.0\n",
      "state:  [  24   20 1151  959   63   63] observation:  [0.63 0.63 0.63] \treward:  0.0\n",
      "state:  [  22   22 1173  981   64   64] observation:  [0.64 0.64 0.64] \treward:  0.0\n",
      "state:  [  22   23 1195 1004   65   65] observation:  [0.65 0.65 0.65] \treward:  0.0\n",
      "state:  [  22   22 1217 1026   66   66] observation:  [0.66 0.66 0.66] \treward:  0.0\n",
      "state:  [  21   22 1238 1048   67   67] observation:  [0.67 0.67 0.67] \treward:  0.0\n",
      "state:  [  22   21 1260 1069   68   68] observation:  [0.68 0.68 0.68] \treward:  0.0\n",
      "state:  [  21   20 1281 1089   69   69] observation:  [0.69 0.69 0.69] \treward:  0.0\n",
      "state:  [  20   19 1301 1108   70   70] observation:  [0.7 0.7 0.7] \treward:  0.0\n",
      "state:  [  23   18 1324 1126   71   71] observation:  [0.71 0.71 0.71] \treward:  0.0\n",
      "state:  [  23   17 1347 1143   72   72] observation:  [0.72 0.72 0.72] \treward:  0.0\n",
      "state:  [  23   18 1370 1161   73   73] observation:  [0.73 0.73 0.73] \treward:  0.0\n",
      "state:  [  23   17 1393 1178   74   74] observation:  [0.74 0.74 0.74] \treward:  0.0\n",
      "state:  [  22   15 1415 1193   75   75] observation:  [0.75 0.75 0.75] \treward:  0.0\n",
      "state:  [  21   18 1436 1211   76   76] observation:  [0.76 0.76 0.76] \treward:  0.0\n",
      "state:  [  24   17 1460 1228   77   77] observation:  [0.77 0.77 0.77] \treward:  0.0\n",
      "state:  [  24   18 1484 1246   78   78] observation:  [0.78 0.78 0.78] \treward:  0.0\n",
      "state:  [  24   19 1508 1265   79   79] observation:  [0.79 0.79 0.79] \treward:  0.0\n",
      "state:  [  27   18 1535 1283   80   80] observation:  [0.8 0.8 0.8] \treward:  0.0\n",
      "state:  [  26   20 1561 1303   81   81] observation:  [0.81 0.81 0.81] \treward:  0.0\n",
      "state:  [  27   19 1588 1322   82   82] observation:  [0.82 0.82 0.82] \treward:  0.0\n",
      "state:  [  27   19 1615 1341   83   83] observation:  [0.83 0.83 0.83] \treward:  0.0\n",
      "state:  [  27   21 1642 1362   84   84] observation:  [0.84 0.84 0.84] \treward:  0.0\n",
      "state:  [  27   19 1669 1381   85   85] observation:  [0.85 0.85 0.85] \treward:  0.0\n",
      "state:  [  27   19 1696 1400   86   86] observation:  [0.86 0.86 0.86] \treward:  0.0\n",
      "state:  [  25   20 1721 1420   87   87] observation:  [0.87 0.87 0.87] \treward:  0.0\n",
      "state:  [  25   21 1746 1441   88   88] observation:  [0.88 0.88 0.88] \treward:  0.0\n",
      "state:  [  27   23 1773 1464   89   89] observation:  [0.89 0.89 0.89] \treward:  0.0\n",
      "state:  [  26   22 1799 1486   90   90] observation:  [0.9 0.9 0.9] \treward:  0.0\n",
      "state:  [  22   21 1821 1507   91   91] observation:  [0.91 0.91 0.91] \treward:  0.0\n",
      "state:  [  22   22 1843 1529   92   92] observation:  [0.92 0.92 0.92] \treward:  0.0\n",
      "state:  [  22   20 1865 1549   93   93] observation:  [0.93 0.93 0.93] \treward:  0.0\n",
      "state:  [  24   21 1889 1570   94   94] observation:  [0.94 0.94 0.94] \treward:  0.0\n",
      "state:  [  24   22 1913 1592   95   95] observation:  [0.95 0.95 0.95] \treward:  0.0\n",
      "state:  [  24   24 1937 1616   96   96] observation:  [0.96 0.96 0.96] \treward:  0.0\n",
      "state:  [  23   21 1960 1637   97   97] observation:  [0.97 0.97 0.97] \treward:  0.0\n",
      "state:  [  22   19 1982 1656   98   98] observation:  [0.98 0.98 0.98] \treward:  0.0\n",
      "state:  [  22   20 2004 1676   99   99] observation:  [0.99 0.99 0.99] \treward:  0.0\n",
      "state:  [  25   20 2029 1696  100  100] observation:  [1. 1. 1.] \treward:  -1862.5\n",
      "Final Reward =  -1862.5\n"
     ]
    }
   ],
   "source": [
    "# show interor values of environment\n",
    "time_step = henv.reset()\n",
    "print(time_step)\n",
    "cumulative_reward = time_step.reward\n",
    "finished = False\n",
    "\n",
    "while not finished:\n",
    "  time_step = henv.step([0.,0.]) # do nothing\n",
    "  s = henv.get_state()\n",
    "  print(\"state: \", s, \"observation: \", time_step.observation, \"\\treward: \", time_step.reward)\n",
    "  cumulative_reward += time_step.reward\n",
    "  if time_step.step_type == StepType.LAST:\n",
    "    finished = True\n",
    "\n",
    "print('Final Reward = ', cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action spec:\n",
      " BoundedArraySpec(shape=(2,), dtype=dtype('float32'), name='action', minimum=0.0, maximum=1.0) \n",
      "\n",
      "time step spec:\n",
      " TimeStep(\n",
      "{'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
      " 'observation': BoundedArraySpec(shape=(3,), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=1.0),\n",
      " 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n",
      " 'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "action_spec = henv.action_spec()\n",
    "ts_spec = henv.time_step_spec()\n",
    "print(\"action spec:\\n\", action_spec, \"\\n\\ntime step spec:\\n\", ts_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scripted policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do nothing policy: cull never\n",
    "action_script0 = [(max_episode_length, [0,0])]\n",
    "\n",
    "# cull first herd every 20th step and second herd every 20th step\n",
    "action_script1 = [(19, [0,0]), \n",
    "                 (1, [1,0]),\n",
    "                 (19, [0,0]), \n",
    "                 (1, [0,1])] * int(1+max_episode_length/40)\n",
    "\n",
    "manual_scripted_policy0 = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=ts_spec,\n",
    "    action_spec=action_spec,\n",
    "    action_script=action_script0)\n",
    "\n",
    "manual_scripted_policy1 = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=ts_spec,\n",
    "    action_spec=action_spec,\n",
    "    action_script=action_script1)\n",
    "\n",
    "init_policy_state = manual_scripted_policy0.get_initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action= [0. 0.] \tpolicy_state [0, 1]\n",
      "action= [0. 0.] \tpolicy_state [0, 2]\n",
      "action= [0. 0.] \tpolicy_state [0, 3]\n",
      "action= [0. 0.] \tpolicy_state [0, 4]\n",
      "action= [0. 0.] \tpolicy_state [0, 5]\n",
      "action= [0. 0.] \tpolicy_state [0, 6]\n",
      "action= [0. 0.] \tpolicy_state [0, 7]\n",
      "action= [0. 0.] \tpolicy_state [0, 8]\n",
      "action= [0. 0.] \tpolicy_state [0, 9]\n",
      "action= [0. 0.] \tpolicy_state [0, 10]\n",
      "action= [0. 0.] \tpolicy_state [0, 11]\n",
      "action= [0. 0.] \tpolicy_state [0, 12]\n",
      "action= [0. 0.] \tpolicy_state [0, 13]\n",
      "action= [0. 0.] \tpolicy_state [0, 14]\n",
      "action= [0. 0.] \tpolicy_state [0, 15]\n",
      "action= [0. 0.] \tpolicy_state [0, 16]\n",
      "action= [0. 0.] \tpolicy_state [0, 17]\n",
      "action= [0. 0.] \tpolicy_state [0, 18]\n",
      "action= [0. 0.] \tpolicy_state [0, 19]\n",
      "action= [1. 0.] \tpolicy_state [1, 1]\n",
      "action= [0. 0.] \tpolicy_state [2, 1]\n"
     ]
    }
   ],
   "source": [
    "policy_state =  init_policy_state\n",
    "ts0 = henv.reset()\n",
    "for _ in range(21):\n",
    "    action_step = manual_scripted_policy1.action(ts0, policy_state)\n",
    "    policy_state = action_step.state\n",
    "    print(\"action=\", action_step.action, \"\\tpolicy_state\", policy_state)\n",
    "policy_state = manual_scripted_policy1.get_initial_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_policy = random_py_policy.RandomPyPolicy(time_step_spec=ts_spec, action_spec=action_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drive a rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=50, verbose=False):\n",
    "  total_return = 0.0\n",
    "  cullsteps = 0\n",
    "  for e in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "        policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "    else:\n",
    "        policy_state = None # other policies without memory\n",
    "    episode_return = 0.0\n",
    "    i=0\n",
    "    while not time_step.is_last():\n",
    "        i+=1\n",
    "        action_step = policy.action(time_step, policy_state)\n",
    "        if action_step.action[0] > 0 or action_step.action[1] > 0:\n",
    "            cullsteps += 1\n",
    "        policy_state = action_step.state\n",
    "        time_step = environment.step(action_step.action)\n",
    "        if isinstance(environment, HerdEnv_DDPG):\n",
    "            state = environment.get_state()\n",
    "        else:\n",
    "            state = None # TF environment from wrapper does not have get_state()\n",
    "        episode_return += time_step.reward\n",
    "        if verbose:\n",
    "            print (f\"episode {e:>2} step{i:>4} action: \", action_step.action, \"state=\", state, \"obs=\", time_step.observation, \"reward=\", time_step.reward)\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  cullsteps /= num_episodes\n",
    "  return avg_return, cullsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average return of random policy: -3348.500 avg steps with culls per episode: 100.0\n"
     ]
    }
   ],
   "source": [
    "random_reward, cullsteps = compute_avg_return(henv, random_policy)\n",
    "print (f\"average return of random policy: {random_reward:.3f} avg steps with culls per episode: {cullsteps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 step   1 action:  [0. 0.] state= [3 2 3 2 1 1] obs= [0.01 0.01 0.01] reward= 0.0\n",
      "episode  0 step   2 action:  [0. 0.] state= [4 3 7 5 2 2] obs= [0.02 0.02 0.02] reward= 0.0\n",
      "episode  0 step   3 action:  [0. 0.] state= [ 7  4 14  9  3  3] obs= [0.03 0.03 0.03] reward= 0.0\n",
      "episode  0 step   4 action:  [0. 0.] state= [ 7  4 21 13  4  4] obs= [0.04 0.04 0.04] reward= 0.0\n",
      "episode  0 step   5 action:  [0. 0.] state= [10  5 31 18  5  5] obs= [0.05 0.05 0.05] reward= 0.0\n",
      "episode  0 step   6 action:  [0. 0.] state= [11  8 42 26  6  6] obs= [0.06 0.06 0.06] reward= 0.0\n",
      "episode  0 step   7 action:  [0. 0.] state= [13 13 55 39  7  7] obs= [0.07 0.07 0.07] reward= 0.0\n",
      "episode  0 step   8 action:  [0. 0.] state= [15 13 70 52  8  8] obs= [0.08 0.08 0.08] reward= 0.0\n",
      "episode  0 step   9 action:  [0. 0.] state= [14 14 84 66  9  9] obs= [0.09 0.09 0.09] reward= 0.0\n",
      "episode  0 step  10 action:  [0. 0.] state= [15 14 99 80 10 10] obs= [0.1 0.1 0.1] reward= 0.0\n",
      "episode  0 step  11 action:  [0. 0.] state= [ 16  18 115  98  11  11] obs= [0.11 0.11 0.11] reward= 0.0\n",
      "episode  0 step  12 action:  [0. 0.] state= [ 15  17 130 115  12  12] obs= [0.12 0.12 0.12] reward= 0.0\n",
      "episode  0 step  13 action:  [0. 0.] state= [ 17  16 147 131  13  13] obs= [0.13 0.13 0.13] reward= 0.0\n",
      "episode  0 step  14 action:  [0. 0.] state= [ 19  17 166 148  14  14] obs= [0.14 0.14 0.14] reward= 0.0\n",
      "episode  0 step  15 action:  [0. 0.] state= [ 17  17 183 165  15  15] obs= [0.15 0.15 0.15] reward= 0.0\n",
      "episode  0 step  16 action:  [0. 0.] state= [ 18  17 201 182  16  16] obs= [0.16 0.16 0.16] reward= 0.0\n",
      "episode  0 step  17 action:  [0. 0.] state= [ 18  18 219 200  17  17] obs= [0.17 0.17 0.17] reward= 0.0\n",
      "episode  0 step  18 action:  [0. 0.] state= [ 19  21 238 221  18  18] obs= [0.18 0.18 0.18] reward= 0.0\n",
      "episode  0 step  19 action:  [0. 0.] state= [ 19  21 257 242  19  19] obs= [0.19 0.19 0.19] reward= 0.0\n",
      "episode  0 step  20 action:  [1. 0.] state= [  0  21 257 263   0  20] obs= [0.2 0.  0.2] reward= -32.0\n",
      "episode  0 step  21 action:  [0. 0.] state= [  0  21 257 284   1  21] obs= [0.21 0.01 0.21] reward= 0.0\n",
      "episode  0 step  22 action:  [0. 0.] state= [  1  23 258 307   2  22] obs= [0.22 0.02 0.22] reward= 0.0\n",
      "episode  0 step  23 action:  [0. 0.] state= [  2  24 260 331   3  23] obs= [0.23 0.03 0.23] reward= 0.0\n",
      "episode  0 step  24 action:  [0. 0.] state= [  4  23 264 354   4  24] obs= [0.24 0.04 0.24] reward= 0.0\n",
      "episode  0 step  25 action:  [0. 0.] state= [  9  22 273 376   5  25] obs= [0.25 0.05 0.25] reward= 0.0\n",
      "episode  0 step  26 action:  [0. 0.] state= [  7  23 280 399   6  26] obs= [0.26 0.06 0.26] reward= 0.0\n",
      "episode  0 step  27 action:  [0. 0.] state= [  7  21 287 420   7  27] obs= [0.27 0.07 0.27] reward= 0.0\n",
      "episode  0 step  28 action:  [0. 0.] state= [  8  21 295 441   8  28] obs= [0.28 0.08 0.28] reward= 0.0\n",
      "episode  0 step  29 action:  [0. 0.] state= [ 10  20 305 461   9  29] obs= [0.29 0.09 0.29] reward= 0.0\n",
      "episode  0 step  30 action:  [0. 0.] state= [ 12  18 317 479  10  30] obs= [0.3 0.1 0.3] reward= 0.0\n",
      "episode  0 step  31 action:  [0. 0.] state= [ 13  17 330 496  11  31] obs= [0.31 0.11 0.31] reward= 0.0\n",
      "episode  0 step  32 action:  [0. 0.] state= [ 16  17 346 513  12  32] obs= [0.32 0.12 0.32] reward= 0.0\n",
      "episode  0 step  33 action:  [0. 0.] state= [ 15  18 361 531  13  33] obs= [0.33 0.13 0.33] reward= 0.0\n",
      "episode  0 step  34 action:  [0. 0.] state= [ 15  17 376 548  14  34] obs= [0.34 0.14 0.34] reward= 0.0\n",
      "episode  0 step  35 action:  [0. 0.] state= [ 15  18 391 566  15  35] obs= [0.35 0.15 0.35] reward= 0.0\n",
      "episode  0 step  36 action:  [0. 0.] state= [ 16  18 407 584  16  36] obs= [0.36 0.16 0.36] reward= 0.0\n",
      "episode  0 step  37 action:  [0. 0.] state= [ 17  19 424 603  17  37] obs= [0.37 0.17 0.37] reward= 0.0\n",
      "episode  0 step  38 action:  [0. 0.] state= [ 18  18 442 621  18  38] obs= [0.38 0.18 0.38] reward= 0.0\n",
      "episode  0 step  39 action:  [0. 0.] state= [ 17  17 459 638  19  39] obs= [0.39 0.19 0.39] reward= 0.0\n",
      "episode  0 step  40 action:  [0. 1.] state= [ 16   0 475 638  20   0] obs= [0.4 0.2 0. ] reward= -32.0\n",
      "episode  0 step  41 action:  [0. 0.] state= [ 17   1 492 639  21   1] obs= [0.41 0.21 0.01] reward= 0.0\n",
      "episode  0 step  42 action:  [0. 0.] state= [ 17   3 509 642  22   2] obs= [0.42 0.22 0.02] reward= 0.0\n",
      "episode  0 step  43 action:  [0. 0.] state= [ 18   6 527 648  23   3] obs= [0.43 0.23 0.03] reward= 0.0\n",
      "episode  0 step  44 action:  [0. 0.] state= [ 18   7 545 655  24   4] obs= [0.44 0.24 0.04] reward= 0.0\n",
      "episode  0 step  45 action:  [0. 0.] state= [ 18  10 563 665  25   5] obs= [0.45 0.25 0.05] reward= 0.0\n",
      "episode  0 step  46 action:  [0. 0.] state= [ 18  10 581 675  26   6] obs= [0.46 0.26 0.06] reward= 0.0\n",
      "episode  0 step  47 action:  [0. 0.] state= [ 18  11 599 686  27   7] obs= [0.47 0.27 0.07] reward= 0.0\n",
      "episode  0 step  48 action:  [0. 0.] state= [ 19  11 618 697  28   8] obs= [0.48 0.28 0.08] reward= 0.0\n",
      "episode  0 step  49 action:  [0. 0.] state= [ 18  12 636 709  29   9] obs= [0.49 0.29 0.09] reward= 0.0\n",
      "episode  0 step  50 action:  [0. 0.] state= [ 18  15 654 724  30  10] obs= [0.5 0.3 0.1] reward= 0.0\n",
      "episode  0 step  51 action:  [0. 0.] state= [ 18  18 672 742  31  11] obs= [0.51 0.31 0.11] reward= 0.0\n",
      "episode  0 step  52 action:  [0. 0.] state= [ 17  21 689 763  32  12] obs= [0.52 0.32 0.12] reward= 0.0\n",
      "episode  0 step  53 action:  [0. 0.] state= [ 16  23 705 786  33  13] obs= [0.53 0.33 0.13] reward= 0.0\n",
      "episode  0 step  54 action:  [0. 0.] state= [ 18  22 723 808  34  14] obs= [0.54 0.34 0.14] reward= 0.0\n",
      "episode  0 step  55 action:  [0. 0.] state= [ 17  21 740 829  35  15] obs= [0.55 0.35 0.15] reward= 0.0\n",
      "episode  0 step  56 action:  [0. 0.] state= [ 15  20 755 849  36  16] obs= [0.56 0.36 0.16] reward= 0.0\n",
      "episode  0 step  57 action:  [0. 0.] state= [ 16  21 771 870  37  17] obs= [0.57 0.37 0.17] reward= 0.0\n",
      "episode  0 step  58 action:  [0. 0.] state= [ 16  23 787 893  38  18] obs= [0.58 0.38 0.18] reward= 0.0\n",
      "episode  0 step  59 action:  [0. 0.] state= [ 15  22 802 915  39  19] obs= [0.59 0.39 0.19] reward= 0.0\n",
      "episode  0 step  60 action:  [1. 0.] state= [  0  23 802 938   0  20] obs= [0.6 0.  0.2] reward= -32.0\n",
      "episode  0 step  61 action:  [0. 0.] state= [  1  24 803 962   1  21] obs= [0.61 0.01 0.21] reward= 0.0\n",
      "episode  0 step  62 action:  [0. 0.] state= [  3  24 806 986   2  22] obs= [0.62 0.02 0.22] reward= 0.0\n",
      "episode  0 step  63 action:  [0. 0.] state= [   7   23  813 1009    3   23] obs= [0.63 0.03 0.23] reward= 0.0\n",
      "episode  0 step  64 action:  [0. 0.] state= [  10   20  823 1029    4   24] obs= [0.64 0.04 0.24] reward= 0.0\n",
      "episode  0 step  65 action:  [0. 0.] state= [  11   22  834 1051    5   25] obs= [0.65 0.05 0.25] reward= 0.0\n",
      "episode  0 step  66 action:  [0. 0.] state= [  11   21  845 1072    6   26] obs= [0.66 0.06 0.26] reward= 0.0\n",
      "episode  0 step  67 action:  [0. 0.] state= [  11   21  856 1093    7   27] obs= [0.67 0.07 0.27] reward= 0.0\n",
      "episode  0 step  68 action:  [0. 0.] state= [  12   21  868 1114    8   28] obs= [0.68 0.08 0.28] reward= 0.0\n",
      "episode  0 step  69 action:  [0. 0.] state= [  14   22  882 1136    9   29] obs= [0.69 0.09 0.29] reward= 0.0\n",
      "episode  0 step  70 action:  [0. 0.] state= [  14   21  896 1157   10   30] obs= [0.7 0.1 0.3] reward= 0.0\n",
      "episode  0 step  71 action:  [0. 0.] state= [  16   21  912 1178   11   31] obs= [0.71 0.11 0.31] reward= 0.0\n",
      "episode  0 step  72 action:  [0. 0.] state= [  17   23  929 1201   12   32] obs= [0.72 0.12 0.32] reward= 0.0\n",
      "episode  0 step  73 action:  [0. 0.] state= [  17   22  946 1223   13   33] obs= [0.73 0.13 0.33] reward= 0.0\n",
      "episode  0 step  74 action:  [0. 0.] state= [  15   22  961 1245   14   34] obs= [0.74 0.14 0.34] reward= 0.0\n",
      "episode  0 step  75 action:  [0. 0.] state= [  15   23  976 1268   15   35] obs= [0.75 0.15 0.35] reward= 0.0\n",
      "episode  0 step  76 action:  [0. 0.] state= [  15   26  991 1294   16   36] obs= [0.76 0.16 0.36] reward= 0.0\n",
      "episode  0 step  77 action:  [0. 0.] state= [  13   25 1004 1319   17   37] obs= [0.77 0.17 0.37] reward= 0.0\n",
      "episode  0 step  78 action:  [0. 0.] state= [  13   26 1017 1345   18   38] obs= [0.78 0.18 0.38] reward= 0.0\n",
      "episode  0 step  79 action:  [0. 0.] state= [  12   27 1029 1372   19   39] obs= [0.79 0.19 0.39] reward= 0.0\n",
      "episode  0 step  80 action:  [0. 1.] state= [  12    0 1041 1372   20    0] obs= [0.8 0.2 0. ] reward= -32.0\n",
      "episode  0 step  81 action:  [0. 0.] state= [  12    0 1053 1372   21    1] obs= [0.81 0.21 0.01] reward= 0.0\n",
      "episode  0 step  82 action:  [0. 0.] state= [  13    1 1066 1373   22    2] obs= [0.82 0.22 0.02] reward= 0.0\n",
      "episode  0 step  83 action:  [0. 0.] state= [  13    2 1079 1375   23    3] obs= [0.83 0.23 0.03] reward= 0.0\n",
      "episode  0 step  84 action:  [0. 0.] state= [  13    2 1092 1377   24    4] obs= [0.84 0.24 0.04] reward= 0.0\n",
      "episode  0 step  85 action:  [0. 0.] state= [  15    5 1107 1382   25    5] obs= [0.85 0.25 0.05] reward= 0.0\n",
      "episode  0 step  86 action:  [0. 0.] state= [  17    5 1124 1387   26    6] obs= [0.86 0.26 0.06] reward= 0.0\n",
      "episode  0 step  87 action:  [0. 0.] state= [  16    7 1140 1394   27    7] obs= [0.87 0.27 0.07] reward= 0.0\n",
      "episode  0 step  88 action:  [0. 0.] state= [  19   11 1159 1405   28    8] obs= [0.88 0.28 0.08] reward= 0.0\n",
      "episode  0 step  89 action:  [0. 0.] state= [  19   12 1178 1417   29    9] obs= [0.89 0.29 0.09] reward= 0.0\n",
      "episode  0 step  90 action:  [0. 0.] state= [  21   12 1199 1429   30   10] obs= [0.9 0.3 0.1] reward= 0.0\n",
      "episode  0 step  91 action:  [0. 0.] state= [  20   13 1219 1442   31   11] obs= [0.91 0.31 0.11] reward= 0.0\n",
      "episode  0 step  92 action:  [0. 0.] state= [  19   16 1238 1458   32   12] obs= [0.92 0.32 0.12] reward= 0.0\n",
      "episode  0 step  93 action:  [0. 0.] state= [  18   16 1256 1474   33   13] obs= [0.93 0.33 0.13] reward= 0.0\n",
      "episode  0 step  94 action:  [0. 0.] state= [  20   17 1276 1491   34   14] obs= [0.94 0.34 0.14] reward= 0.0\n",
      "episode  0 step  95 action:  [0. 0.] state= [  19   18 1295 1509   35   15] obs= [0.95 0.35 0.15] reward= 0.0\n",
      "episode  0 step  96 action:  [0. 0.] state= [  19   19 1314 1528   36   16] obs= [0.96 0.36 0.16] reward= 0.0\n",
      "episode  0 step  97 action:  [0. 0.] state= [  19   18 1333 1546   37   17] obs= [0.97 0.37 0.17] reward= 0.0\n",
      "episode  0 step  98 action:  [0. 0.] state= [  17   17 1350 1563   38   18] obs= [0.98 0.38 0.18] reward= 0.0\n",
      "episode  0 step  99 action:  [0. 0.] state= [  18   16 1368 1579   39   19] obs= [0.99 0.39 0.19] reward= 0.0\n",
      "episode  0 step 100 action:  [1. 0.] state= [   0   16 1368 1595    0   20] obs= [1.  0.  0.2] reward= -1513.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1641.5, 5.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show states for one rollout of second scripted policy\n",
    "compute_avg_return(henv, manual_scripted_policy1, num_episodes=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average return of do-nothing-policy: -1942.939 avg culls 0.0\n",
      "average return of manual policy: -1632.058 avg culls 5.0\n"
     ]
    }
   ],
   "source": [
    "manual_reward0, cullsteps = compute_avg_return(henv, manual_scripted_policy0, num_episodes=500)\n",
    "print (f\"average return of do-nothing-policy: {manual_reward0:.3f} avg culls {cullsteps}\")\n",
    "manual_reward1, cullsteps = compute_avg_return(henv, manual_scripted_policy1, num_episodes=500)\n",
    "print (f\"average return of manual policy: {manual_reward1:.3f} avg culls {cullsteps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Deep-Q Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Made it work to here, implement ddpg agent next.'''\n",
    "\n",
    "\n",
    "from tf_agents.networks.sequential import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 30000\n",
    "replay_buffer_max_length = 10000\n",
    "batch_size = 64\n",
    "num_eval_episodes = 100\n",
    "initial_collect_steps = 100\n",
    "collect_steps_per_iteration = 100\n",
    "log_interval = 200\n",
    "eval_interval = 1000\n",
    "target_update_period = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make actor network simple\n",
    "num_actions = 2**num_herds # this does not scale, obviously\n",
    "kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03)\n",
    "\n",
    "#q_net = Sequential([Dense(300, activation='tanh'),\n",
    "#                    Dense(4, activation=None,\n",
    "#                          kernel_initializer = kernel_initializer)])\n",
    "\n",
    "# from tutorial\n",
    "fc_layer_params = (100, 50)\n",
    "action_tensor_spec = tensor_spec.from_spec(henv.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# it's output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=kernel_initializer,\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(henv)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(henv)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    #boltzmann_temperature = 0.005,\n",
    "    #epsilon_greedy = 0.001,\n",
    "    #gamma=0.99,\n",
    "    optimizer=tf.keras.optimizers.RMSprop(), # tf.keras.optimizers.Adam(learning_rate = 1e-3),\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "    #target_update_period=target_update_period)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually initialize a reasonably good policy: kill both herds if the sum of observations is large\n",
    "#W = np.array([[0, 3 ,0, 2],[0, 0, 3, 2,]])\n",
    "#b = np.array([1, 0, 0, 0])\n",
    "#q_net.layers[0].set_weights([W,b])\n",
    "#agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step <= 1 or step % log_interval == 0:\n",
    "    print('step = {0:4>}: loss = {1:.4f}'.format(step, train_loss), end=\"\\t\")\n",
    "\n",
    "  if step <= 1 or (step <= 100 and step % 50 == 0) or step % eval_interval == 0:\n",
    "    avg_return, cullsteps = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step {0}: average return = {1:.1f} cullsteps = {2:.1f}'.format(step, avg_return.numpy().item(), cullsteps))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_reward, culleps = compute_avg_return(eval_env, agent.policy, num_episodes=500)\n",
    "print (\"reward of learned policy: \", learned_reward.numpy(), \"cullsteps=\", culleps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ts = eval_env.reset()\n",
    "\n",
    "def get_action(obs):\n",
    "    \"\"\" execute the learned policy network \n",
    "       obs:  one float for global time, one float for each herd - the time since last culling\n",
    "    \"\"\"\n",
    "    _ts = TimeStep(tf.constant([0.]),\n",
    "                   tf.constant([0.]),\n",
    "                   tf.constant([1]),\n",
    "                   tf.constant([obs]))\n",
    "    # a = agent.collect_policy.action(_ts) # just to see how much is explored versus exploited\n",
    "    a = agent.policy.action(_ts)\n",
    "    return a.action.numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what the learned policy does on a grid of observations (5 steps per row&col)\n",
    "A = [[get_action([.0, x,y])\n",
    " for y in np.arange(0.,1.,.05,np.float32)]\n",
    " for x in np.arange(0.,1.,.05,np.float32)]\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with parameters of manually designed q_network policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W, b = agent.policy.trainable_variables\n",
    "# W = W.numpy()\n",
    "# b = b.numpy()\n",
    "# print (\"weights\\n\", W, \"\\nbias\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nn(obs):\n",
    "#    y = np.dot(obs, W)+b\n",
    "#    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn([0.5,.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
