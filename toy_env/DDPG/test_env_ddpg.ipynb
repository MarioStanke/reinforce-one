{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the environment `HerdEnv` of `herd_env.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "from tf_agents.trajectories import TimeStep\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from herd_env_ddpg import HerdEnv_DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "henv_val = HerdEnv_DDPG(herd_sizes = [64,64], rand_recovery_prob = 0.1, rand_infection_prob = 0.05)\n",
    "utils.validate_py_environment(henv_val, episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Herd Environment instance to be trained for\n",
    "max_episode_length=100\n",
    "num_herds=2\n",
    "henv = HerdEnv_DDPG(herd_sizes = [32,32], expected_episode_length=-1, max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.04, rand_infection_prob = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': 1.0,\n",
      " 'observation': array([0., 0., 0.], dtype=float32),\n",
      " 'reward': 0.0,\n",
      " 'step_type': array(0, dtype=int32)})\n",
      "state:  [2 1 2 1 1 1] observation:  [0.01 0.01 0.01] \treward:  0.0\n",
      "state:  [4 2 6 3 2 2] observation:  [0.02 0.02 0.02] \treward:  0.0\n",
      "state:  [ 4  2 10  5  3  3] observation:  [0.03 0.03 0.03] \treward:  0.0\n",
      "state:  [ 4  6 14 11  4  4] observation:  [0.04 0.04 0.04] \treward:  0.0\n",
      "state:  [ 9  5 23 16  5  5] observation:  [0.05 0.05 0.05] \treward:  0.0\n",
      "state:  [10  8 33 24  6  6] observation:  [0.06 0.06 0.06] \treward:  0.0\n",
      "state:  [13 11 46 35  7  7] observation:  [0.07 0.07 0.07] \treward:  0.0\n",
      "state:  [13 15 59 50  8  8] observation:  [0.08 0.08 0.08] \treward:  0.0\n",
      "state:  [15 17 74 67  9  9] observation:  [0.09 0.09 0.09] \treward:  0.0\n",
      "state:  [15 19 89 86 10 10] observation:  [0.1 0.1 0.1] \treward:  0.0\n",
      "state:  [ 15  20 104 106  11  11] observation:  [0.11 0.11 0.11] \treward:  0.0\n",
      "state:  [ 17  16 121 122  12  12] observation:  [0.12 0.12 0.12] \treward:  0.0\n",
      "state:  [ 18  17 139 139  13  13] observation:  [0.13 0.13 0.13] \treward:  0.0\n",
      "state:  [ 19  17 158 156  14  14] observation:  [0.14 0.14 0.14] \treward:  0.0\n",
      "state:  [ 20  15 178 171  15  15] observation:  [0.15 0.15 0.15] \treward:  0.0\n",
      "state:  [ 22  16 200 187  16  16] observation:  [0.16 0.16 0.16] \treward:  0.0\n",
      "state:  [ 22  17 222 204  17  17] observation:  [0.17 0.17 0.17] \treward:  0.0\n",
      "state:  [ 21  13 243 217  18  18] observation:  [0.18 0.18 0.18] \treward:  0.0\n",
      "state:  [ 21  12 264 229  19  19] observation:  [0.19 0.19 0.19] \treward:  0.0\n",
      "state:  [ 21  12 285 241  20  20] observation:  [0.2 0.2 0.2] \treward:  0.0\n",
      "state:  [ 22  13 307 254  21  21] observation:  [0.21 0.21 0.21] \treward:  0.0\n",
      "state:  [ 20  14 327 268  22  22] observation:  [0.22 0.22 0.22] \treward:  0.0\n",
      "state:  [ 19  17 346 285  23  23] observation:  [0.23 0.23 0.23] \treward:  0.0\n",
      "state:  [ 19  18 365 303  24  24] observation:  [0.24 0.24 0.24] \treward:  0.0\n",
      "state:  [ 19  19 384 322  25  25] observation:  [0.25 0.25 0.25] \treward:  0.0\n",
      "state:  [ 19  19 403 341  26  26] observation:  [0.26 0.26 0.26] \treward:  0.0\n",
      "state:  [ 20  19 423 360  27  27] observation:  [0.27 0.27 0.27] \treward:  0.0\n",
      "state:  [ 20  20 443 380  28  28] observation:  [0.28 0.28 0.28] \treward:  0.0\n",
      "state:  [ 20  21 463 401  29  29] observation:  [0.29 0.29 0.29] \treward:  0.0\n",
      "state:  [ 21  20 484 421  30  30] observation:  [0.3 0.3 0.3] \treward:  0.0\n",
      "state:  [ 22  19 506 440  31  31] observation:  [0.31 0.31 0.31] \treward:  0.0\n",
      "state:  [ 19  20 525 460  32  32] observation:  [0.32 0.32 0.32] \treward:  0.0\n",
      "state:  [ 22  21 547 481  33  33] observation:  [0.33 0.33 0.33] \treward:  0.0\n",
      "state:  [ 23  16 570 497  34  34] observation:  [0.34 0.34 0.34] \treward:  0.0\n",
      "state:  [ 22  18 592 515  35  35] observation:  [0.35 0.35 0.35] \treward:  0.0\n",
      "state:  [ 22  19 614 534  36  36] observation:  [0.36 0.36 0.36] \treward:  0.0\n",
      "state:  [ 22  20 636 554  37  37] observation:  [0.37 0.37 0.37] \treward:  0.0\n",
      "state:  [ 22  22 658 576  38  38] observation:  [0.38 0.38 0.38] \treward:  0.0\n",
      "state:  [ 23  23 681 599  39  39] observation:  [0.39 0.39 0.39] \treward:  0.0\n",
      "state:  [ 22  23 703 622  40  40] observation:  [0.4 0.4 0.4] \treward:  0.0\n",
      "state:  [ 22  24 725 646  41  41] observation:  [0.41 0.41 0.41] \treward:  0.0\n",
      "state:  [ 24  26 749 672  42  42] observation:  [0.42 0.42 0.42] \treward:  0.0\n",
      "state:  [ 24  25 773 697  43  43] observation:  [0.43 0.43 0.43] \treward:  0.0\n",
      "state:  [ 26  23 799 720  44  44] observation:  [0.44 0.44 0.44] \treward:  0.0\n",
      "state:  [ 25  24 824 744  45  45] observation:  [0.45 0.45 0.45] \treward:  0.0\n",
      "state:  [ 24  24 848 768  46  46] observation:  [0.46 0.46 0.46] \treward:  0.0\n",
      "state:  [ 22  22 870 790  47  47] observation:  [0.47 0.47 0.47] \treward:  0.0\n",
      "state:  [ 20  23 890 813  48  48] observation:  [0.48 0.48 0.48] \treward:  0.0\n",
      "state:  [ 20  23 910 836  49  49] observation:  [0.49 0.49 0.49] \treward:  0.0\n",
      "state:  [ 22  22 932 858  50  50] observation:  [0.5 0.5 0.5] \treward:  0.0\n",
      "state:  [ 22  23 954 881  51  51] observation:  [0.51 0.51 0.51] \treward:  0.0\n",
      "state:  [ 22  24 976 905  52  52] observation:  [0.52 0.52 0.52] \treward:  0.0\n",
      "state:  [ 23  22 999 927  53  53] observation:  [0.53 0.53 0.53] \treward:  0.0\n",
      "state:  [  22   20 1021  947   54   54] observation:  [0.54 0.54 0.54] \treward:  0.0\n",
      "state:  [  22   19 1043  966   55   55] observation:  [0.55 0.55 0.55] \treward:  0.0\n",
      "state:  [  24   19 1067  985   56   56] observation:  [0.56 0.56 0.56] \treward:  0.0\n",
      "state:  [  23   19 1090 1004   57   57] observation:  [0.57 0.57 0.57] \treward:  0.0\n",
      "state:  [  21   19 1111 1023   58   58] observation:  [0.58 0.58 0.58] \treward:  0.0\n",
      "state:  [  22   21 1133 1044   59   59] observation:  [0.59 0.59 0.59] \treward:  0.0\n",
      "state:  [  22   21 1155 1065   60   60] observation:  [0.6 0.6 0.6] \treward:  0.0\n",
      "state:  [  20   22 1175 1087   61   61] observation:  [0.61 0.61 0.61] \treward:  0.0\n",
      "state:  [  21   22 1196 1109   62   62] observation:  [0.62 0.62 0.62] \treward:  0.0\n",
      "state:  [  21   21 1217 1130   63   63] observation:  [0.63 0.63 0.63] \treward:  0.0\n",
      "state:  [  20   22 1237 1152   64   64] observation:  [0.64 0.64 0.64] \treward:  0.0\n",
      "state:  [  18   23 1255 1175   65   65] observation:  [0.65 0.65 0.65] \treward:  0.0\n",
      "state:  [  19   23 1274 1198   66   66] observation:  [0.66 0.66 0.66] \treward:  0.0\n",
      "state:  [  18   21 1292 1219   67   67] observation:  [0.67 0.67 0.67] \treward:  0.0\n",
      "state:  [  17   20 1309 1239   68   68] observation:  [0.68 0.68 0.68] \treward:  0.0\n",
      "state:  [  17   21 1326 1260   69   69] observation:  [0.69 0.69 0.69] \treward:  0.0\n",
      "state:  [  20   21 1346 1281   70   70] observation:  [0.7 0.7 0.7] \treward:  0.0\n",
      "state:  [  21   20 1367 1301   71   71] observation:  [0.71 0.71 0.71] \treward:  0.0\n",
      "state:  [  20   19 1387 1320   72   72] observation:  [0.72 0.72 0.72] \treward:  0.0\n",
      "state:  [  20   18 1407 1338   73   73] observation:  [0.73 0.73 0.73] \treward:  0.0\n",
      "state:  [  17   18 1424 1356   74   74] observation:  [0.74 0.74 0.74] \treward:  0.0\n",
      "state:  [  18   19 1442 1375   75   75] observation:  [0.75 0.75 0.75] \treward:  0.0\n",
      "state:  [  20   19 1462 1394   76   76] observation:  [0.76 0.76 0.76] \treward:  0.0\n",
      "state:  [  21   21 1483 1415   77   77] observation:  [0.77 0.77 0.77] \treward:  0.0\n",
      "state:  [  22   23 1505 1438   78   78] observation:  [0.78 0.78 0.78] \treward:  0.0\n",
      "state:  [  21   21 1526 1459   79   79] observation:  [0.79 0.79 0.79] \treward:  0.0\n",
      "state:  [  21   22 1547 1481   80   80] observation:  [0.8 0.8 0.8] \treward:  0.0\n",
      "state:  [  21   21 1568 1502   81   81] observation:  [0.81 0.81 0.81] \treward:  0.0\n",
      "state:  [  22   20 1590 1522   82   82] observation:  [0.82 0.82 0.82] \treward:  0.0\n",
      "state:  [  23   20 1613 1542   83   83] observation:  [0.83 0.83 0.83] \treward:  0.0\n",
      "state:  [  22   21 1635 1563   84   84] observation:  [0.84 0.84 0.84] \treward:  0.0\n",
      "state:  [  20   20 1655 1583   85   85] observation:  [0.85 0.85 0.85] \treward:  0.0\n",
      "state:  [  18   21 1673 1604   86   86] observation:  [0.86 0.86 0.86] \treward:  0.0\n",
      "state:  [  21   23 1694 1627   87   87] observation:  [0.87 0.87 0.87] \treward:  0.0\n",
      "state:  [  21   21 1715 1648   88   88] observation:  [0.88 0.88 0.88] \treward:  0.0\n",
      "state:  [  21   20 1736 1668   89   89] observation:  [0.89 0.89 0.89] \treward:  0.0\n",
      "state:  [  23   24 1759 1692   90   90] observation:  [0.9 0.9 0.9] \treward:  0.0\n",
      "state:  [  22   23 1781 1715   91   91] observation:  [0.91 0.91 0.91] \treward:  0.0\n",
      "state:  [  23   22 1804 1737   92   92] observation:  [0.92 0.92 0.92] \treward:  0.0\n",
      "state:  [  23   20 1827 1757   93   93] observation:  [0.93 0.93 0.93] \treward:  0.0\n",
      "state:  [  23   19 1850 1776   94   94] observation:  [0.94 0.94 0.94] \treward:  0.0\n",
      "state:  [  21   19 1871 1795   95   95] observation:  [0.95 0.95 0.95] \treward:  0.0\n",
      "state:  [  21   18 1892 1813   96   96] observation:  [0.96 0.96 0.96] \treward:  0.0\n",
      "state:  [  22   19 1914 1832   97   97] observation:  [0.97 0.97 0.97] \treward:  0.0\n",
      "state:  [  22   17 1936 1849   98   98] observation:  [0.98 0.98 0.98] \treward:  0.0\n",
      "state:  [  24   17 1960 1866   99   99] observation:  [0.99 0.99 0.99] \treward:  0.0\n",
      "state:  [  22   19 1982 1885  100  100] observation:  [1. 1. 1.] \treward:  -1933.5\n",
      "Final Reward =  -1933.5\n"
     ]
    }
   ],
   "source": [
    "# show interor values of environment\n",
    "time_step = henv.reset()\n",
    "print(time_step)\n",
    "cumulative_reward = time_step.reward\n",
    "finished = False\n",
    "\n",
    "while not finished:\n",
    "  time_step = henv.step([0.,0.]) # do nothing\n",
    "  s = henv.get_state()\n",
    "  print(\"state: \", s, \"observation: \", time_step.observation, \"\\treward: \", time_step.reward)\n",
    "  cumulative_reward += time_step.reward\n",
    "  if time_step.step_type == StepType.LAST:\n",
    "    finished = True\n",
    "\n",
    "print('Final Reward = ', cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action spec:\n",
      " BoundedArraySpec(shape=(2,), dtype=dtype('float32'), name='action', minimum=0.0, maximum=1.0) \n",
      "\n",
      "time step spec:\n",
      " TimeStep(\n",
      "{'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
      " 'observation': BoundedArraySpec(shape=(3,), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=1.0),\n",
      " 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n",
      " 'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "action_spec = henv.action_spec()\n",
    "ts_spec = henv.time_step_spec()\n",
    "print(\"action spec:\\n\", action_spec, \"\\n\\ntime step spec:\\n\", ts_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scripted policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do nothing policy: cull never\n",
    "action_script0 = [(max_episode_length, [0,0])]\n",
    "\n",
    "# cull first herd every 20th step and second herd every 20th step\n",
    "action_script1 = [(19, [0,0]), \n",
    "                 (1, [1,0]),\n",
    "                 (19, [0,0]), \n",
    "                 (1, [0,1])] * int(1+max_episode_length/40)\n",
    "\n",
    "manual_scripted_policy0 = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=ts_spec,\n",
    "    action_spec=action_spec,\n",
    "    action_script=action_script0)\n",
    "\n",
    "manual_scripted_policy1 = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=ts_spec,\n",
    "    action_spec=action_spec,\n",
    "    action_script=action_script1)\n",
    "\n",
    "init_policy_state = manual_scripted_policy0.get_initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action= [0. 0.] \tpolicy_state [0, 1]\n",
      "action= [0. 0.] \tpolicy_state [0, 2]\n",
      "action= [0. 0.] \tpolicy_state [0, 3]\n",
      "action= [0. 0.] \tpolicy_state [0, 4]\n",
      "action= [0. 0.] \tpolicy_state [0, 5]\n",
      "action= [0. 0.] \tpolicy_state [0, 6]\n",
      "action= [0. 0.] \tpolicy_state [0, 7]\n",
      "action= [0. 0.] \tpolicy_state [0, 8]\n",
      "action= [0. 0.] \tpolicy_state [0, 9]\n",
      "action= [0. 0.] \tpolicy_state [0, 10]\n",
      "action= [0. 0.] \tpolicy_state [0, 11]\n",
      "action= [0. 0.] \tpolicy_state [0, 12]\n",
      "action= [0. 0.] \tpolicy_state [0, 13]\n",
      "action= [0. 0.] \tpolicy_state [0, 14]\n",
      "action= [0. 0.] \tpolicy_state [0, 15]\n",
      "action= [0. 0.] \tpolicy_state [0, 16]\n",
      "action= [0. 0.] \tpolicy_state [0, 17]\n",
      "action= [0. 0.] \tpolicy_state [0, 18]\n",
      "action= [0. 0.] \tpolicy_state [0, 19]\n",
      "action= [1. 0.] \tpolicy_state [1, 1]\n",
      "action= [0. 0.] \tpolicy_state [2, 1]\n"
     ]
    }
   ],
   "source": [
    "policy_state =  init_policy_state\n",
    "ts0 = henv.reset()\n",
    "for _ in range(21):\n",
    "    action_step = manual_scripted_policy1.action(ts0, policy_state)\n",
    "    policy_state = action_step.state\n",
    "    print(\"action=\", action_step.action, \"\\tpolicy_state\", policy_state)\n",
    "policy_state = manual_scripted_policy1.get_initial_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_policy = random_py_policy.RandomPyPolicy(time_step_spec=ts_spec, action_spec=action_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drive a rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=50, verbose=False):\n",
    "  total_return = 0.0\n",
    "  cullsteps = 0\n",
    "  for e in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "        policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "    else:\n",
    "        policy_state = None # other policies without memory\n",
    "    episode_return = 0.0\n",
    "    i=0\n",
    "    while not time_step.is_last():\n",
    "        i+=1\n",
    "        action_step = policy.action(time_step, policy_state)\n",
    "        if action_step.action[0] > 0 or action_step.action[1] > 0:\n",
    "            cullsteps += 1\n",
    "        policy_state = action_step.state\n",
    "        time_step = environment.step(action_step.action)\n",
    "        if isinstance(environment, HerdEnv_DDPG):\n",
    "            state = environment.get_state()\n",
    "        else:\n",
    "            state = None # TF environment from wrapper does not have get_state()\n",
    "        episode_return += time_step.reward\n",
    "        if verbose:\n",
    "            print (f\"episode {e:>2} step{i:>4} action: \", action_step.action, \"state=\", state, \"obs=\", time_step.observation, \"reward=\", time_step.reward)\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  cullsteps /= num_episodes\n",
    "  return avg_return, cullsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average return of random policy: -1752.800 avg steps with culls per episode: 100.0\n"
     ]
    }
   ],
   "source": [
    "random_reward, cullsteps = compute_avg_return(henv, random_policy)\n",
    "print (f\"average return of random policy: {random_reward:.3f} avg steps with culls per episode: {cullsteps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 step   1 action:  [0. 0.] state= [1 0 1 0 1 1] obs= [0.01 0.01 0.01] reward= 0.0\n",
      "episode  0 step   2 action:  [0. 0.] state= [4 0 5 0 2 2] obs= [0.02 0.02 0.02] reward= 0.0\n",
      "episode  0 step   3 action:  [0. 0.] state= [ 6  3 11  3  3  3] obs= [0.03 0.03 0.03] reward= 0.0\n",
      "episode  0 step   4 action:  [0. 0.] state= [ 7  4 18  7  4  4] obs= [0.04 0.04 0.04] reward= 0.0\n",
      "episode  0 step   5 action:  [0. 0.] state= [10  6 28 13  5  5] obs= [0.05 0.05 0.05] reward= 0.0\n",
      "episode  0 step   6 action:  [0. 0.] state= [11  7 39 20  6  6] obs= [0.06 0.06 0.06] reward= 0.0\n",
      "episode  0 step   7 action:  [0. 0.] state= [11  9 50 29  7  7] obs= [0.07 0.07 0.07] reward= 0.0\n",
      "episode  0 step   8 action:  [0. 0.] state= [13 13 63 42  8  8] obs= [0.08 0.08 0.08] reward= 0.0\n",
      "episode  0 step   9 action:  [0. 0.] state= [13 13 76 55  9  9] obs= [0.09 0.09 0.09] reward= 0.0\n",
      "episode  0 step  10 action:  [0. 0.] state= [14 14 90 69 10 10] obs= [0.1 0.1 0.1] reward= 0.0\n",
      "episode  0 step  11 action:  [0. 0.] state= [ 16  18 106  87  11  11] obs= [0.11 0.11 0.11] reward= 0.0\n",
      "episode  0 step  12 action:  [0. 0.] state= [ 15  19 121 106  12  12] obs= [0.12 0.12 0.12] reward= 0.0\n",
      "episode  0 step  13 action:  [0. 0.] state= [ 15  19 136 125  13  13] obs= [0.13 0.13 0.13] reward= 0.0\n",
      "episode  0 step  14 action:  [0. 0.] state= [ 15  17 151 142  14  14] obs= [0.14 0.14 0.14] reward= 0.0\n",
      "episode  0 step  15 action:  [0. 0.] state= [ 19  16 170 158  15  15] obs= [0.15 0.15 0.15] reward= 0.0\n",
      "episode  0 step  16 action:  [0. 0.] state= [ 19  17 189 175  16  16] obs= [0.16 0.16 0.16] reward= 0.0\n",
      "episode  0 step  17 action:  [0. 0.] state= [ 18  18 207 193  17  17] obs= [0.17 0.17 0.17] reward= 0.0\n",
      "episode  0 step  18 action:  [0. 0.] state= [ 19  19 226 212  18  18] obs= [0.18 0.18 0.18] reward= 0.0\n",
      "episode  0 step  19 action:  [0. 0.] state= [ 18  16 244 228  19  19] obs= [0.19 0.19 0.19] reward= 0.0\n",
      "episode  0 step  20 action:  [1. 0.] state= [  0  16 244 244   0  20] obs= [0.2 0.  0.2] reward= -16.0\n",
      "episode  0 step  21 action:  [0. 0.] state= [  3  17 247 261   1  21] obs= [0.21 0.01 0.21] reward= 0.0\n",
      "episode  0 step  22 action:  [0. 0.] state= [  3  18 250 279   2  22] obs= [0.22 0.02 0.22] reward= 0.0\n",
      "episode  0 step  23 action:  [0. 0.] state= [  3  17 253 296   3  23] obs= [0.23 0.03 0.23] reward= 0.0\n",
      "episode  0 step  24 action:  [0. 0.] state= [  4  16 257 312   4  24] obs= [0.24 0.04 0.24] reward= 0.0\n",
      "episode  0 step  25 action:  [0. 0.] state= [  4  18 261 330   5  25] obs= [0.25 0.05 0.25] reward= 0.0\n",
      "episode  0 step  26 action:  [0. 0.] state= [  7  17 268 347   6  26] obs= [0.26 0.06 0.26] reward= 0.0\n",
      "episode  0 step  27 action:  [0. 0.] state= [  8  18 276 365   7  27] obs= [0.27 0.07 0.27] reward= 0.0\n",
      "episode  0 step  28 action:  [0. 0.] state= [ 10  21 286 386   8  28] obs= [0.28 0.08 0.28] reward= 0.0\n",
      "episode  0 step  29 action:  [0. 0.] state= [ 11  21 297 407   9  29] obs= [0.29 0.09 0.29] reward= 0.0\n",
      "episode  0 step  30 action:  [0. 0.] state= [ 13  23 310 430  10  30] obs= [0.3 0.1 0.3] reward= 0.0\n",
      "episode  0 step  31 action:  [0. 0.] state= [ 12  24 322 454  11  31] obs= [0.31 0.11 0.31] reward= 0.0\n",
      "episode  0 step  32 action:  [0. 0.] state= [ 13  26 335 480  12  32] obs= [0.32 0.12 0.32] reward= 0.0\n",
      "episode  0 step  33 action:  [0. 0.] state= [ 16  27 351 507  13  33] obs= [0.33 0.13 0.33] reward= 0.0\n",
      "episode  0 step  34 action:  [0. 0.] state= [ 16  26 367 533  14  34] obs= [0.34 0.14 0.34] reward= 0.0\n",
      "episode  0 step  35 action:  [0. 0.] state= [ 17  26 384 559  15  35] obs= [0.35 0.15 0.35] reward= 0.0\n",
      "episode  0 step  36 action:  [0. 0.] state= [ 18  27 402 586  16  36] obs= [0.36 0.16 0.36] reward= 0.0\n",
      "episode  0 step  37 action:  [0. 0.] state= [ 17  24 419 610  17  37] obs= [0.37 0.17 0.37] reward= 0.0\n",
      "episode  0 step  38 action:  [0. 0.] state= [ 18  24 437 634  18  38] obs= [0.38 0.18 0.38] reward= 0.0\n",
      "episode  0 step  39 action:  [0. 0.] state= [ 20  25 457 659  19  39] obs= [0.39 0.19 0.39] reward= 0.0\n",
      "episode  0 step  40 action:  [0. 1.] state= [ 20   0 477 659  20   0] obs= [0.4 0.2 0. ] reward= -16.0\n",
      "episode  0 step  41 action:  [0. 0.] state= [ 18   0 495 659  21   1] obs= [0.41 0.21 0.01] reward= 0.0\n",
      "episode  0 step  42 action:  [0. 0.] state= [ 19   0 514 659  22   2] obs= [0.42 0.22 0.02] reward= 0.0\n",
      "episode  0 step  43 action:  [0. 0.] state= [ 17   6 531 665  23   3] obs= [0.43 0.23 0.03] reward= 0.0\n",
      "episode  0 step  44 action:  [0. 0.] state= [ 18   6 549 671  24   4] obs= [0.44 0.24 0.04] reward= 0.0\n",
      "episode  0 step  45 action:  [0. 0.] state= [ 16   6 565 677  25   5] obs= [0.45 0.25 0.05] reward= 0.0\n",
      "episode  0 step  46 action:  [0. 0.] state= [ 15   7 580 684  26   6] obs= [0.46 0.26 0.06] reward= 0.0\n",
      "episode  0 step  47 action:  [0. 0.] state= [ 15   8 595 692  27   7] obs= [0.47 0.27 0.07] reward= 0.0\n",
      "episode  0 step  48 action:  [0. 0.] state= [ 19   9 614 701  28   8] obs= [0.48 0.28 0.08] reward= 0.0\n",
      "episode  0 step  49 action:  [0. 0.] state= [ 20  10 634 711  29   9] obs= [0.49 0.29 0.09] reward= 0.0\n",
      "episode  0 step  50 action:  [0. 0.] state= [ 19  11 653 722  30  10] obs= [0.5 0.3 0.1] reward= 0.0\n",
      "episode  0 step  51 action:  [0. 0.] state= [ 20  12 673 734  31  11] obs= [0.51 0.31 0.11] reward= 0.0\n",
      "episode  0 step  52 action:  [0. 0.] state= [ 21  13 694 747  32  12] obs= [0.52 0.32 0.12] reward= 0.0\n",
      "episode  0 step  53 action:  [0. 0.] state= [ 22  15 716 762  33  13] obs= [0.53 0.33 0.13] reward= 0.0\n",
      "episode  0 step  54 action:  [0. 0.] state= [ 22  16 738 778  34  14] obs= [0.54 0.34 0.14] reward= 0.0\n",
      "episode  0 step  55 action:  [0. 0.] state= [ 25  17 763 795  35  15] obs= [0.55 0.35 0.15] reward= 0.0\n",
      "episode  0 step  56 action:  [0. 0.] state= [ 25  17 788 812  36  16] obs= [0.56 0.36 0.16] reward= 0.0\n",
      "episode  0 step  57 action:  [0. 0.] state= [ 24  18 812 830  37  17] obs= [0.57 0.37 0.17] reward= 0.0\n",
      "episode  0 step  58 action:  [0. 0.] state= [ 23  20 835 850  38  18] obs= [0.58 0.38 0.18] reward= 0.0\n",
      "episode  0 step  59 action:  [0. 0.] state= [ 21  19 856 869  39  19] obs= [0.59 0.39 0.19] reward= 0.0\n",
      "episode  0 step  60 action:  [1. 0.] state= [  0  19 856 888   0  20] obs= [0.6 0.  0.2] reward= -16.0\n",
      "episode  0 step  61 action:  [0. 0.] state= [  1  19 857 907   1  21] obs= [0.61 0.01 0.21] reward= 0.0\n",
      "episode  0 step  62 action:  [0. 0.] state= [  1  18 858 925   2  22] obs= [0.62 0.02 0.22] reward= 0.0\n",
      "episode  0 step  63 action:  [0. 0.] state= [  2  18 860 943   3  23] obs= [0.63 0.03 0.23] reward= 0.0\n",
      "episode  0 step  64 action:  [0. 0.] state= [  4  19 864 962   4  24] obs= [0.64 0.04 0.24] reward= 0.0\n",
      "episode  0 step  65 action:  [0. 0.] state= [  5  18 869 980   5  25] obs= [0.65 0.05 0.25] reward= 0.0\n",
      "episode  0 step  66 action:  [0. 0.] state= [   6   21  875 1001    6   26] obs= [0.66 0.06 0.26] reward= 0.0\n",
      "episode  0 step  67 action:  [0. 0.] state= [   7   19  882 1020    7   27] obs= [0.67 0.07 0.27] reward= 0.0\n",
      "episode  0 step  68 action:  [0. 0.] state= [   6   21  888 1041    8   28] obs= [0.68 0.08 0.28] reward= 0.0\n",
      "episode  0 step  69 action:  [0. 0.] state= [   7   20  895 1061    9   29] obs= [0.69 0.09 0.29] reward= 0.0\n",
      "episode  0 step  70 action:  [0. 0.] state= [  11   21  906 1082   10   30] obs= [0.7 0.1 0.3] reward= 0.0\n",
      "episode  0 step  71 action:  [0. 0.] state= [  11   21  917 1103   11   31] obs= [0.71 0.11 0.31] reward= 0.0\n",
      "episode  0 step  72 action:  [0. 0.] state= [  13   19  930 1122   12   32] obs= [0.72 0.12 0.32] reward= 0.0\n",
      "episode  0 step  73 action:  [0. 0.] state= [  14   16  944 1138   13   33] obs= [0.73 0.13 0.33] reward= 0.0\n",
      "episode  0 step  74 action:  [0. 0.] state= [  13   13  957 1151   14   34] obs= [0.74 0.14 0.34] reward= 0.0\n",
      "episode  0 step  75 action:  [0. 0.] state= [  13   13  970 1164   15   35] obs= [0.75 0.15 0.35] reward= 0.0\n",
      "episode  0 step  76 action:  [0. 0.] state= [  15   13  985 1177   16   36] obs= [0.76 0.16 0.36] reward= 0.0\n",
      "episode  0 step  77 action:  [0. 0.] state= [  15   16 1000 1193   17   37] obs= [0.77 0.17 0.37] reward= 0.0\n",
      "episode  0 step  78 action:  [0. 0.] state= [  16   16 1016 1209   18   38] obs= [0.78 0.18 0.38] reward= 0.0\n",
      "episode  0 step  79 action:  [0. 0.] state= [  16   15 1032 1224   19   39] obs= [0.79 0.19 0.39] reward= 0.0\n",
      "episode  0 step  80 action:  [0. 1.] state= [  17    0 1049 1224   20    0] obs= [0.8 0.2 0. ] reward= -16.0\n",
      "episode  0 step  81 action:  [0. 0.] state= [  19    1 1068 1225   21    1] obs= [0.81 0.21 0.01] reward= 0.0\n",
      "episode  0 step  82 action:  [0. 0.] state= [  18    4 1086 1229   22    2] obs= [0.82 0.22 0.02] reward= 0.0\n",
      "episode  0 step  83 action:  [0. 0.] state= [  19    9 1105 1238   23    3] obs= [0.83 0.23 0.03] reward= 0.0\n",
      "episode  0 step  84 action:  [0. 0.] state= [  18   10 1123 1248   24    4] obs= [0.84 0.24 0.04] reward= 0.0\n",
      "episode  0 step  85 action:  [0. 0.] state= [  21   10 1144 1258   25    5] obs= [0.85 0.25 0.05] reward= 0.0\n",
      "episode  0 step  86 action:  [0. 0.] state= [  20   12 1164 1270   26    6] obs= [0.86 0.26 0.06] reward= 0.0\n",
      "episode  0 step  87 action:  [0. 0.] state= [  19   12 1183 1282   27    7] obs= [0.87 0.27 0.07] reward= 0.0\n",
      "episode  0 step  88 action:  [0. 0.] state= [  18   13 1201 1295   28    8] obs= [0.88 0.28 0.08] reward= 0.0\n",
      "episode  0 step  89 action:  [0. 0.] state= [  19   14 1220 1309   29    9] obs= [0.89 0.29 0.09] reward= 0.0\n",
      "episode  0 step  90 action:  [0. 0.] state= [  19   13 1239 1322   30   10] obs= [0.9 0.3 0.1] reward= 0.0\n",
      "episode  0 step  91 action:  [0. 0.] state= [  18   14 1257 1336   31   11] obs= [0.91 0.31 0.11] reward= 0.0\n",
      "episode  0 step  92 action:  [0. 0.] state= [  18   15 1275 1351   32   12] obs= [0.92 0.32 0.12] reward= 0.0\n",
      "episode  0 step  93 action:  [0. 0.] state= [  20   15 1295 1366   33   13] obs= [0.93 0.33 0.13] reward= 0.0\n",
      "episode  0 step  94 action:  [0. 0.] state= [  19   15 1314 1381   34   14] obs= [0.94 0.34 0.14] reward= 0.0\n",
      "episode  0 step  95 action:  [0. 0.] state= [  20   16 1334 1397   35   15] obs= [0.95 0.35 0.15] reward= 0.0\n",
      "episode  0 step  96 action:  [0. 0.] state= [  20   16 1354 1413   36   16] obs= [0.96 0.36 0.16] reward= 0.0\n",
      "episode  0 step  97 action:  [0. 0.] state= [  21   18 1375 1431   37   17] obs= [0.97 0.37 0.17] reward= 0.0\n",
      "episode  0 step  98 action:  [0. 0.] state= [  24   19 1399 1450   38   18] obs= [0.98 0.38 0.18] reward= 0.0\n",
      "episode  0 step  99 action:  [0. 0.] state= [  26   17 1425 1467   39   19] obs= [0.99 0.39 0.19] reward= 0.0\n",
      "episode  0 step 100 action:  [1. 0.] state= [   0   17 1425 1484    0   20] obs= [1.  0.  0.2] reward= -1470.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1534.5, 5.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show states for one rollout of second scripted policy\n",
    "compute_avg_return(henv, manual_scripted_policy1, num_episodes=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average return of do-nothing-policy: -1928.560 avg culls 0.0\n",
      "average return of manual policy: -1551.621 avg culls 5.0\n"
     ]
    }
   ],
   "source": [
    "manual_reward0, cullsteps = compute_avg_return(henv, manual_scripted_policy0, num_episodes=500)\n",
    "print (f\"average return of do-nothing-policy: {manual_reward0:.3f} avg culls {cullsteps}\")\n",
    "manual_reward1, cullsteps = compute_avg_return(henv, manual_scripted_policy1, num_episodes=500)\n",
    "print (f\"average return of manual policy: {manual_reward1:.3f} avg culls {cullsteps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=50, verbose=False):\n",
    "  total_return = 0.0\n",
    "  cullsteps = 0\n",
    "  for e in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "        policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "    else:\n",
    "        policy_state = None # other policies without memory\n",
    "    episode_return = 0.0\n",
    "    i=0\n",
    "    while not time_step.is_last():\n",
    "        i+=1\n",
    "        action_step = policy.action(time_step, policy_state)\n",
    "        if action_step.action[0][0] > 0 or action_step.action[0][1] > 0:\n",
    "            cullsteps += 1\n",
    "        policy_state = action_step.state\n",
    "        time_step = environment.step(action_step.action)\n",
    "        if isinstance(environment, HerdEnv_DDPG):\n",
    "            state = environment.get_state()\n",
    "        else:\n",
    "            state = None # TF environment from wrapper does not have get_state()\n",
    "        episode_return += time_step.reward\n",
    "        if verbose:\n",
    "            print (f\"episode {e:>2} step{i:>4} action: \", action_step.action, \"state=\", state, \"obs=\", time_step.observation, \"reward=\", time_step.reward)\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  cullsteps /= num_episodes\n",
    "  return avg_return, cullsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Made it work to here, implement ddpg agent next.'''\n",
    "\n",
    "import functools\n",
    "from tf_agents.networks import nest_map\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.keras_layers import inner_reshape\n",
    "from tf_agents.networks.sequential import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000\n",
    "replay_buffer_max_length = 20000\n",
    "batch_size = 64\n",
    "num_eval_episodes = 100\n",
    "initial_collect_steps = 2000\n",
    "collect_steps_per_iteration = 5\n",
    "log_interval = 500\n",
    "eval_interval = 500\n",
    "target_update_period = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(HerdEnv_DDPG)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(HerdEnv_DDPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03)\n",
    "\n",
    "#Make critic and actor net for ddpg\n",
    "\n",
    "dense = functools.partial(\n",
    "    tf.keras.layers.Dense,\n",
    "    activation=tf.keras.activations.relu,\n",
    "    kernel_initializer=tf.compat.v1.variance_scaling_initializer(\n",
    "        scale=1./ 3.0, mode='fan_in', distribution='uniform'))\n",
    "\n",
    "\n",
    "def create_identity_layer():\n",
    "    return tf.keras.layers.Lambda(lambda x: x)\n",
    "\n",
    "\n",
    "def create_fc_network(layer_units):\n",
    "    return sequential.Sequential([dense(num_units) for num_units in layer_units])\n",
    "\n",
    "\n",
    "def create_actor_network(fc_layer_units, action_spec):\n",
    "    \"\"\"Create an actor network for DDPG.\"\"\"\n",
    "    flat_action_spec = tf.nest.flatten(action_spec)\n",
    "    if len(flat_action_spec) > 1:\n",
    "        raise ValueError('Only a single action tensor is supported by this network')\n",
    "    flat_action_spec = flat_action_spec[0]\n",
    "\n",
    "    fc_layers = [dense(num_units) for num_units in fc_layer_units]\n",
    "\n",
    "    num_actions = flat_action_spec.shape.num_elements()\n",
    "    action_fc_layer = tf.keras.layers.Dense(\n",
    "        num_actions,\n",
    "        activation=tf.keras.activations.tanh,\n",
    "        kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "            minval=-0.003, maxval=0.003))\n",
    "\n",
    "    scaling_layer = tf.keras.layers.Lambda(\n",
    "        lambda x: common.scale_to_spec(x, flat_action_spec))\n",
    "    return sequential.Sequential(fc_layers + [action_fc_layer, scaling_layer])\n",
    "\n",
    "\n",
    "def create_critic_network(obs_fc_layer_units,\n",
    "                          action_fc_layer_units,\n",
    "                          joint_fc_layer_units):\n",
    "    \"\"\"Create a critic network for DDPG.\"\"\"\n",
    "\n",
    "    def split_inputs(inputs):\n",
    "        return {'observation': inputs[0], 'action': inputs[1]}\n",
    "\n",
    "    obs_network = create_fc_network(\n",
    "        obs_fc_layer_units) if obs_fc_layer_units else create_identity_layer()\n",
    "    action_network = create_fc_network(\n",
    "        action_fc_layer_units\n",
    "        ) if action_fc_layer_units else create_identity_layer()\n",
    "    joint_network = create_fc_network(\n",
    "        joint_fc_layer_units) if joint_fc_layer_units else create_identity_layer()\n",
    "    \n",
    "    value_fc_layer = tf.keras.layers.Dense(\n",
    "        1,\n",
    "        activation=None,\n",
    "        kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "            minval=-0.003, maxval=0.003))\n",
    "\n",
    "    return sequential.Sequential([\n",
    "        tf.keras.layers.Lambda(split_inputs),\n",
    "        nest_map.NestMap({\n",
    "            'observation': obs_network,\n",
    "            'action': action_network\n",
    "        }),\n",
    "        nest_map.NestFlatten(),\n",
    "        tf.keras.layers.Concatenate(),\n",
    "        joint_network,\n",
    "        value_fc_layer,\n",
    "        inner_reshape.InnerReshape([1], [])\n",
    "    ])\n",
    "\n",
    "actor_fc_layers=(400, 300)\n",
    "critic_obs_fc_layers=(400,)\n",
    "critic_action_fc_layers=None\n",
    "critic_joint_fc_layers=(300,)\n",
    "\n",
    "actor_net = create_actor_network(actor_fc_layers, train_env.action_spec())\n",
    "critic_net = create_critic_network(critic_obs_fc_layers,\n",
    "                                       critic_action_fc_layers,\n",
    "                                       critic_joint_fc_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_counter = tf.Variable(0)\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "agent = ddpg_agent.DdpgAgent(\n",
    "        train_env.time_step_spec(),\n",
    "        train_env.action_spec(),\n",
    "        actor_network=actor_net,\n",
    "        critic_network=critic_net,\n",
    "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=1e-4),\n",
    "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=1e-3),\n",
    "        ou_stddev=0.3,\n",
    "        ou_damping=0.15,\n",
    "        target_update_tau=0.05,\n",
    "        target_update_period=5,\n",
    "        #dqda_clipping=dqda_clipping,\n",
    "        td_errors_loss_fn=tf.compat.v1.losses.huber_loss,\n",
    "        gamma=0.999,\n",
    "        #reward_scale_factor=reward_scale_factor,\n",
    "        #gradient_clipping=gradient_clipping,\n",
    "        #debug_summaries=debug_summaries,\n",
    "        #summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "        train_step_counter=global_step)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually initialize a reasonably good policy: kill both herds if the sum of observations is large\n",
    "#W = np.array([[0, 3 ,0, 2],[0, 0, 3, 2,]])\n",
    "#b = np.array([1, 0, 0, 0])\n",
    "#q_net.layers[0].set_weights([W,b])\n",
    "#agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(\n",
       "{action: (64, 2, 2),\n",
       " discount: (64, 2),\n",
       " next_step_type: (64, 2),\n",
       " observation: (64, 2, 3),\n",
       " policy_info: (),\n",
       " reward: (64, 2),\n",
       " step_type: (64, 2)}), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(\n",
       "{action: tf.float32,\n",
       " discount: tf.float32,\n",
       " next_step_type: tf.int32,\n",
       " observation: tf.float32,\n",
       " policy_info: (),\n",
       " reward: tf.float32,\n",
       " step_type: tf.int32}), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1: loss = 15.1171\tstep 1: average return = -3640.0 cullsteps = 222.9\n",
      "step 50: average return = -3114.6 cullsteps = 223.2\n",
      "step 100: average return = -1943.0 cullsteps = 198.0\n",
      "step = 500: loss = 120.2619\tstep 500: average return = -5372.1 cullsteps = 83.5\n",
      "step = 1000: loss = 285.4420\tstep 1000: average return = -6065.9 cullsteps = 45.2\n",
      "step = 1500: loss = 244.3414\tstep 1500: average return = -5312.4 cullsteps = 32.2\n",
      "step = 2000: loss = 236.2220\tstep 2000: average return = -4989.1 cullsteps = 25.8\n",
      "step = 2500: loss = 197.7295\tstep 2500: average return = -5250.3 cullsteps = 21.1\n",
      "step = 3000: loss = 153.5066\tstep 3000: average return = -5262.1 cullsteps = 18.8\n",
      "step = 3500: loss = 138.1592\tstep 3500: average return = -5417.1 cullsteps = 15.8\n",
      "step = 4000: loss = 129.6016\tstep 4000: average return = -5710.5 cullsteps = 14.8\n",
      "step = 4500: loss = 122.1865\tstep 4500: average return = -5257.8 cullsteps = 12.7\n",
      "step = 5000: loss = 97.6707\tstep 5000: average return = -4903.2 cullsteps = 11.6\n",
      "step = 5500: loss = 126.2777\tstep 5500: average return = -5396.3 cullsteps = 9.8\n",
      "step = 6000: loss = 135.7774\tstep 6000: average return = -5798.4 cullsteps = 8.9\n",
      "step = 6500: loss = 101.3754\tstep 6500: average return = -5576.4 cullsteps = 6.9\n",
      "step = 7000: loss = 90.4364\tstep 7000: average return = -4816.3 cullsteps = 6.0\n",
      "step = 7500: loss = 149.7432\tstep 7500: average return = -6270.5 cullsteps = 5.0\n",
      "step = 8000: loss = 94.0890\tstep 8000: average return = -5029.1 cullsteps = 4.0\n",
      "step = 8500: loss = 126.5894\tstep 8500: average return = -5272.6 cullsteps = 3.0\n",
      "step = 9000: loss = 122.0633\tstep 9000: average return = -4275.9 cullsteps = 2.0\n",
      "step = 9500: loss = 103.5101\tstep 9500: average return = -6097.2 cullsteps = 1.0\n",
      "step = 10000: loss = 150.2081\tstep 10000: average return = -5041.9 cullsteps = 1.0\n",
      "step = 10500: loss = 110.0396\tstep 10500: average return = -5366.0 cullsteps = 0.0\n",
      "step = 11000: loss = 104.9721\tstep 11000: average return = -5122.6 cullsteps = 0.0\n",
      "step = 11500: loss = 120.1480\tstep 11500: average return = -5813.8 cullsteps = 0.0\n",
      "step = 12000: loss = 106.5797\tstep 12000: average return = -5859.2 cullsteps = 0.0\n",
      "step = 12500: loss = 96.4151\tstep 12500: average return = -5250.6 cullsteps = 0.0\n",
      "step = 13000: loss = 120.6455\tstep 13000: average return = -6013.5 cullsteps = 0.0\n",
      "step = 13500: loss = 118.2779\tstep 13500: average return = -4717.7 cullsteps = 0.0\n",
      "step = 14000: loss = 91.6821\tstep 14000: average return = -5504.9 cullsteps = 0.0\n",
      "step = 14500: loss = 112.0403\tstep 14500: average return = -5733.1 cullsteps = 0.0\n",
      "step = 15000: loss = 102.6924\tstep 15000: average return = -5749.6 cullsteps = 0.0\n",
      "step = 15500: loss = 132.9075\tstep 15500: average return = -5096.1 cullsteps = 0.0\n",
      "step = 16000: loss = 96.6257\tstep 16000: average return = -5685.3 cullsteps = 0.0\n",
      "step = 16500: loss = 93.4642\tstep 16500: average return = -5459.9 cullsteps = 0.0\n",
      "step = 17000: loss = 99.8429\tstep 17000: average return = -4886.8 cullsteps = 0.0\n",
      "step = 17500: loss = 90.8674\tstep 17500: average return = -4804.7 cullsteps = 0.0\n",
      "step = 18000: loss = 77.0842\tstep 18000: average return = -5791.0 cullsteps = 0.0\n",
      "step = 18500: loss = 112.1905\tstep 18500: average return = -5760.2 cullsteps = 0.0\n",
      "step = 19000: loss = 102.8376\tstep 19000: average return = -4927.1 cullsteps = 0.0\n",
      "step = 19500: loss = 82.7433\tstep 19500: average return = -4917.2 cullsteps = 0.0\n",
      "step = 20000: loss = 97.5408\tstep 20000: average return = -6026.4 cullsteps = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step <= 1 or step % log_interval == 0:\n",
    "    print('step = {0:4>}: loss = {1:.4f}'.format(step, train_loss), end=\"\\t\")\n",
    "\n",
    "  if step <= 1 or (step <= 100 and step % 50 == 0) or step % eval_interval == 0:\n",
    "    avg_return, cullsteps = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step {0}: average return = {1:.1f} cullsteps = {2:.1f}'.format(step, avg_return.numpy().item(), cullsteps))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward of learned policy:  [-5426.526] cullsteps= 0.0\n"
     ]
    }
   ],
   "source": [
    "learned_reward, culleps = compute_avg_return(eval_env, agent.policy, num_episodes=500)\n",
    "print (\"reward of learned policy: \", learned_reward.numpy(), \"cullsteps=\", culleps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ts = eval_env.reset()\n",
    "\n",
    "def get_action(obs):\n",
    "    \"\"\" execute the learned policy network \n",
    "       obs:  one float for global time, one float for each herd - the time since last culling\n",
    "    \"\"\"\n",
    "    _ts = TimeStep(tf.constant([0.]),\n",
    "                   tf.constant([0.]),\n",
    "                   tf.constant([1]),\n",
    "                   tf.constant([obs]))\n",
    "    # a = agent.collect_policy.action(_ts) # just to see how much is explored versus exploited\n",
    "    a = agent.policy.action(_ts)\n",
    "    return a.action.numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.policies.actor_policy.ActorPolicy at 0x7fc93b6c57c0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can only convert an array of size 1 to a Python scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-20f92965fead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# what the learned policy does on a grid of observations (5 steps per row&col)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m A = [[get_action([.0, x,y])\n\u001b[0m\u001b[1;32m      3\u001b[0m  for y in np.arange(0.,1.,.05,np.float32)]\n\u001b[1;32m      4\u001b[0m  for x in np.arange(0.,1.,.05,np.float32)]\n\u001b[1;32m      5\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-20f92965fead>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# what the learned policy does on a grid of observations (5 steps per row&col)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m A = [[get_action([.0, x,y])\n\u001b[0m\u001b[1;32m      3\u001b[0m  for y in np.arange(0.,1.,.05,np.float32)]\n\u001b[1;32m      4\u001b[0m  for x in np.arange(0.,1.,.05,np.float32)]\n\u001b[1;32m      5\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-20f92965fead>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# what the learned policy does on a grid of observations (5 steps per row&col)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m A = [[get_action([.0, x,y])\n\u001b[0m\u001b[1;32m      3\u001b[0m  for y in np.arange(0.,1.,.05,np.float32)]\n\u001b[1;32m      4\u001b[0m  for x in np.arange(0.,1.,.05,np.float32)]\n\u001b[1;32m      5\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-2d94b6695cf0>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(obs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# a = agent.collect_policy.action(_ts) # just to see how much is explored versus exploited\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: can only convert an array of size 1 to a Python scalar"
     ]
    }
   ],
   "source": [
    "# what the learned policy does on a grid of observations (5 steps per row&col)\n",
    "A = [[get_action([.0, x,y])\n",
    " for y in np.arange(0.,1.,.05,np.float32)]\n",
    " for x in np.arange(0.,1.,.05,np.float32)]\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with parameters of manually designed q_network policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W, b = agent.policy.trainable_variables\n",
    "# W = W.numpy()\n",
    "# b = b.numpy()\n",
    "# print (\"weights\\n\", W, \"\\nbias\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nn(obs):\n",
    "#    y = np.dot(obs, W)+b\n",
    "#    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn([0.5,.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
