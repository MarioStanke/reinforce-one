{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0a87bfa-df1a-4239-b926-afae212223c1",
   "metadata": {},
   "source": [
    "## RNN DDPG  \n",
    "This code is from tf-agents library with minor alterations.  \n",
    "PC1 stands for parameter configuration 1, more suited to our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9deede77-1eee-4990-b9f0-1bab787a9f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The TF-Agents Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Lint as: python2, python3\n",
    "r\"\"\"Train and Eval DDPG.\n",
    "\n",
    "To run:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir $HOME/tmp/ddpg_rnn/dm/CartPole-Balance/ --port 2223 &\n",
    "\n",
    "python tf_agents/agents/ddpg/examples/v2/train_eval_rnn.py \\\n",
    "  --root_dir=$HOME/tmp/ddpg_rnn/dm/CartPole-Balance/ \\\n",
    "  --num_iterations=100000 \\\n",
    "  --alsologtostderr\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "root_dir = '~/Masterarbeit/rnn'\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "\n",
    "from absl import app\n",
    "from absl import logging\n",
    "\n",
    "import gin\n",
    "from six.moves import range\n",
    "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
    "\n",
    "from tf_agents.agents.ddpg import actor_rnn_network\n",
    "from tf_agents.agents.ddpg import critic_rnn_network\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.environments import suite_dm_control\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import numpy \n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "from tf_agents.trajectories import TimeStep\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import sequential\n",
    "\n",
    "from RNN_Env import FREnv\n",
    "max_episode_length=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c799e852-cb44-467a-8163-36ba7e236a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=50, verbose=False):\n",
    "  total_return = 0.0\n",
    "  cullsteps = 0 \n",
    "  total_culls = 0\n",
    "  for e in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "        policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "    else:\n",
    "        #print(policy.get_initial_state(batch_size=train_env.batch_size()))\n",
    "        policy_state = policy.get_initial_state(batch_size=1) # other policies without memory\n",
    "    episode_return = 0.0\n",
    "    i=0\n",
    "    while not time_step.is_last():\n",
    "        i+=1\n",
    "        action_step = policy.action(time_step, policy_state)\n",
    "        if action_step.action[0][0] > 0 or action_step.action[0][1] > 0:\n",
    "            cullsteps += 1\n",
    "            total_culls += 1\n",
    "        elif action_step.action[0][0] > 0 and action_step.action[0][1] > 0:\n",
    "            total_culls += 2\n",
    "        policy_state = action_step.state\n",
    "        time_step = environment.step(action_step.action)\n",
    "        if isinstance(environment, FREnv):\n",
    "            state = environment.get_state()\n",
    "        else:\n",
    "            state = None # TF environment from wrapper does not have get_state()\n",
    "        episode_return += time_step.reward\n",
    "        if verbose:\n",
    "            print (f\"episode {e:>2} step{i:>4} action: \", action_step.action, \n",
    "                   \"state=\", state, \"obs=\", time_step.observation, \"reward=\", time_step.reward)\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  cullsteps /= num_episodes\n",
    "  total_culls /= num_episodes\n",
    "  cull_percent = cullsteps / total_culls\n",
    "  return avg_return, cullsteps, cull_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce0c5642-0e58-4bc2-873c-2c26ecff20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "@gin.configurable\n",
    "def train_eval(\n",
    "    root_dir,\n",
    "    env_name='cartpole',\n",
    "    task_name='balance',\n",
    "    observations_allowlist='position',\n",
    "    num_iterations=10000,\n",
    "    actor_fc_layers=(400, 300),\n",
    "    actor_output_fc_layers=(100,),\n",
    "    actor_lstm_size=(40,),\n",
    "    critic_obs_fc_layers=(400,),\n",
    "    critic_action_fc_layers=None,\n",
    "    critic_joint_fc_layers=(300,),\n",
    "    critic_output_fc_layers=(100,),\n",
    "    critic_lstm_size=(40,),\n",
    "    # Params for collect\n",
    "    initial_collect_episodes=100,    #1\n",
    "    collect_episodes_per_iteration=4,    #1\n",
    "    replay_buffer_capacity=100000,\n",
    "    ou_stddev=0.2,\n",
    "    ou_damping=0.15,\n",
    "    # Params for target update\n",
    "    target_update_tau=0.05,\n",
    "    target_update_period=5,\n",
    "    # Params for train\n",
    "    # Params for train\n",
    "    train_steps_per_iteration=100,    #200\n",
    "    batch_size=64,\n",
    "    train_sequence_length=50,    #10\n",
    "    actor_learning_rate=1e-4,\n",
    "    critic_learning_rate=1e-3,\n",
    "    dqda_clipping=None,\n",
    "    td_errors_loss_fn=None,\n",
    "    gamma=0.99,    #.995\n",
    "    reward_scale_factor=1.0,\n",
    "    gradient_clipping=None,\n",
    "    use_tf_functions=True,\n",
    "    # Params for eval\n",
    "    num_eval_episodes=100,    #10\n",
    "    eval_interval=2000,    #1000\n",
    "    # Params for checkpoints, summaries, and logging\n",
    "    log_interval=1000,\n",
    "    summary_interval=1000,\n",
    "    summaries_flush_secs=10,\n",
    "    debug_summaries=True,\n",
    "    summarize_grads_and_vars=True,\n",
    "    eval_metrics_callback=None):\n",
    "\n",
    "  \"\"\"A simple train and eval for DDPG.\"\"\"\n",
    "\n",
    "  best_return = -10000\n",
    "  root_dir = os.path.expanduser(root_dir)\n",
    "  train_dir = os.path.join(root_dir, 'train')\n",
    "  eval_dir = os.path.join(root_dir, 'eval')\n",
    "  policy_dir = os.path.join(root_dir, 'policy')\n",
    "\n",
    "  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "      train_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "  train_summary_writer.set_as_default()\n",
    "\n",
    "  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "      eval_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "  eval_metrics = [\n",
    "      tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n",
    "      tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n",
    "  ]\n",
    "\n",
    "  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "  with tf.compat.v2.summary.record_if(\n",
    "      lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
    "    if observations_allowlist is not None:\n",
    "      env_wrappers = [\n",
    "          functools.partial(\n",
    "              wrappers.FlattenObservationsWrapper,\n",
    "              observations_allowlist=[observations_allowlist])\n",
    "      ]\n",
    "    else:\n",
    "      env_wrappers = []\n",
    "\n",
    "    tf_env = tf_py_environment.TFPyEnvironment(\n",
    "        FREnv(herd_sizes = [32,32], expected_episode_length=100, max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.02, rand_infection_prob = 0.06))\n",
    "    eval_tf_env = tf_py_environment.TFPyEnvironment(\n",
    "        FREnv(herd_sizes = [32,32], expected_episode_length=100, max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.02, rand_infection_prob = 0.06))\n",
    "\n",
    "    actor_net = actor_rnn_network.ActorRnnNetwork(\n",
    "        tf_env.time_step_spec().observation,\n",
    "        tf_env.action_spec(),\n",
    "        input_fc_layer_params=actor_fc_layers,\n",
    "        lstm_size=actor_lstm_size,\n",
    "        output_fc_layer_params=actor_output_fc_layers)\n",
    "\n",
    "    critic_net_input_specs = (tf_env.time_step_spec().observation,\n",
    "                              tf_env.action_spec())\n",
    "\n",
    "    critic_net = critic_rnn_network.CriticRnnNetwork(\n",
    "        critic_net_input_specs,\n",
    "        observation_fc_layer_params=critic_obs_fc_layers,\n",
    "        action_fc_layer_params=critic_action_fc_layers,\n",
    "        joint_fc_layer_params=critic_joint_fc_layers,\n",
    "        lstm_size=critic_lstm_size,\n",
    "        output_fc_layer_params=critic_output_fc_layers,\n",
    "    )\n",
    "\n",
    "    tf_agent = ddpg_agent.DdpgAgent(\n",
    "        tf_env.time_step_spec(),\n",
    "        tf_env.action_spec(),\n",
    "        actor_network=actor_net,\n",
    "        critic_network=critic_net,\n",
    "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=actor_learning_rate),\n",
    "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=critic_learning_rate),\n",
    "        ou_stddev=ou_stddev,\n",
    "        ou_damping=ou_damping,\n",
    "        target_update_tau=target_update_tau,\n",
    "        target_update_period=target_update_period,\n",
    "        dqda_clipping=dqda_clipping,\n",
    "        td_errors_loss_fn=td_errors_loss_fn,\n",
    "        gamma=gamma,\n",
    "        reward_scale_factor=reward_scale_factor,\n",
    "        gradient_clipping=gradient_clipping,\n",
    "        debug_summaries=debug_summaries,\n",
    "        summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "        train_step_counter=global_step)\n",
    "    tf_agent.initialize()\n",
    "\n",
    "    train_metrics = [\n",
    "        tf_metrics.NumberOfEpisodes(),\n",
    "        tf_metrics.EnvironmentSteps(),\n",
    "        tf_metrics.AverageReturnMetric(),\n",
    "        tf_metrics.AverageEpisodeLengthMetric(),\n",
    "    ]\n",
    "\n",
    "    eval_policy = tf_agent.policy\n",
    "    collect_policy = tf_agent.collect_policy\n",
    "    \n",
    "    saver = policy_saver.PolicySaver(eval_policy)\n",
    "\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        tf_agent.collect_data_spec,\n",
    "        batch_size=tf_env.batch_size,\n",
    "        max_length=replay_buffer_capacity)\n",
    "\n",
    "    initial_collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "        tf_env,\n",
    "        collect_policy,\n",
    "        observers=[replay_buffer.add_batch] + train_metrics,\n",
    "        num_episodes=initial_collect_episodes)\n",
    "\n",
    "    collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "        tf_env,\n",
    "        collect_policy,\n",
    "        observers=[replay_buffer.add_batch] + train_metrics,\n",
    "        num_episodes=collect_episodes_per_iteration)\n",
    "\n",
    "    if use_tf_functions:\n",
    "      initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "      collect_driver.run = common.function(collect_driver.run)\n",
    "      tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "    # Collect initial replay data.\n",
    "    logging.info(\n",
    "        'Initializing replay buffer by collecting experience for %d episodes '\n",
    "        'with a random policy.', initial_collect_episodes)\n",
    "    initial_collect_driver.run()\n",
    "\n",
    "    results = metric_utils.eager_compute(\n",
    "        eval_metrics,\n",
    "        eval_tf_env,\n",
    "        eval_policy,\n",
    "        num_episodes=num_eval_episodes,\n",
    "        train_step=global_step,\n",
    "        summary_writer=eval_summary_writer,\n",
    "        summary_prefix='Metrics',\n",
    "    )\n",
    "    if eval_metrics_callback is not None:\n",
    "      eval_metrics_callback(results, global_step.numpy())\n",
    "    metric_utils.log_metrics(eval_metrics)\n",
    "\n",
    "    time_step = None\n",
    "    policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n",
    "\n",
    "    timed_at_step = global_step.numpy()\n",
    "    time_acc = 0\n",
    "\n",
    "    # Dataset generates trajectories with shape [BxTx...]\n",
    "    dataset = replay_buffer.as_dataset(\n",
    "        num_parallel_calls=3,\n",
    "        sample_batch_size=batch_size,\n",
    "        num_steps=train_sequence_length + 1).prefetch(3)\n",
    "    iterator = iter(dataset)\n",
    "\n",
    "    def train_step():\n",
    "      experience, _ = next(iterator)\n",
    "      return tf_agent.train(experience)\n",
    "\n",
    "    if use_tf_functions:\n",
    "      train_step = common.function(train_step)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "      start_time = time.time()\n",
    "      time_step, policy_state = collect_driver.run(\n",
    "          time_step=time_step,\n",
    "          policy_state=policy_state,\n",
    "      )\n",
    "      for _ in range(train_steps_per_iteration):\n",
    "        train_loss = train_step()\n",
    "      time_acc += time.time() - start_time\n",
    "\n",
    "      if global_step.numpy() % log_interval == 0:\n",
    "        logging.info('step = %d, loss = %f', global_step.numpy(),\n",
    "                     train_loss.loss)\n",
    "        steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n",
    "        logging.info('%.3f steps/sec', steps_per_sec)\n",
    "        tf.compat.v2.summary.scalar(\n",
    "            name='global_steps_per_sec', data=steps_per_sec, step=global_step)\n",
    "        timed_at_step = global_step.numpy()\n",
    "        time_acc = 0\n",
    "\n",
    "      for train_metric in train_metrics:\n",
    "        train_metric.tf_summaries(\n",
    "            train_step=global_step, step_metrics=train_metrics[:2])\n",
    "\n",
    "      if global_step.numpy() % eval_interval == 0:\n",
    "        results = metric_utils.eager_compute(\n",
    "            eval_metrics,\n",
    "            eval_tf_env,\n",
    "            eval_policy,\n",
    "            num_episodes=num_eval_episodes,\n",
    "            train_step=global_step,\n",
    "            summary_writer=eval_summary_writer,\n",
    "            summary_prefix='Metrics',\n",
    "        )\n",
    "        if eval_metrics_callback is not None:\n",
    "          eval_metrics_callback(results, global_step.numpy())\n",
    "        metric_utils.log_metrics(eval_metrics)\n",
    "        avg_return, cullsteps, cull_percent = compute_avg_return(eval_tf_env, eval_policy, num_episodes=100, verbose=False)\n",
    "        print('step {0}: average return = {1:.1f} cullsteps = {2:.1f} cull_percent = {3:.1f}'.format(global_step.numpy(), \n",
    "                                                                                                     avg_return.numpy().item(), \n",
    "                                                                                                     cullsteps, cull_percent))\n",
    "        if avg_return > best_return:\n",
    "            if avg_return > -2000:\n",
    "                best_return = avg_return\n",
    "                print('Final best return: ', best_return)\n",
    "                saver.save(os.path.join(policy_dir, str(global_step.numpy())))\n",
    "                break\n",
    "            else:\n",
    "                best_return = avg_return\n",
    "                print('New best return: ', best_return)\n",
    "                saver.save(os.path.join(policy_dir, str(global_step.numpy())))\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c098712-8656-4607-9014-a1a1d4710bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n",
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation, 1/0, 1/1 with unsupported characters which will be renamed to step_type, reward, discount, observation, unknown, unknown_0 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000: average return = -7691.6 cullsteps = 100.2 cull_percent = 1.0\n",
      "New best return:  tf.Tensor([-7691.5786], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as ActorRnnNetwork_layer_call_and_return_conditional_losses, ActorRnnNetwork_layer_call_fn, dynamic_unroll_layer_call_and_return_conditional_losses, dynamic_unroll_layer_call_fn, ActorRnnNetwork_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/rnn/policy/2000/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/rnn/policy/2000/assets\n",
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation, 1/0, 1/1 with unsupported characters which will be renamed to step_type, reward, discount, observation, unknown, unknown_0 in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as ActorRnnNetwork_layer_call_and_return_conditional_losses, ActorRnnNetwork_layer_call_fn, dynamic_unroll_layer_call_and_return_conditional_losses, dynamic_unroll_layer_call_fn, ActorRnnNetwork_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000: average return = -4718.6 cullsteps = 108.8 cull_percent = 1.0\n",
      "New best return:  tf.Tensor([-4718.605], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/rnn/policy/4000/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/rnn/policy/4000/assets\n",
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation, 1/0, 1/1 with unsupported characters which will be renamed to step_type, reward, discount, observation, unknown, unknown_0 in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as ActorRnnNetwork_layer_call_and_return_conditional_losses, ActorRnnNetwork_layer_call_fn, dynamic_unroll_layer_call_and_return_conditional_losses, dynamic_unroll_layer_call_fn, ActorRnnNetwork_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6000: average return = -4497.9 cullsteps = 107.4 cull_percent = 1.0\n",
      "New best return:  tf.Tensor([-4497.897], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/rnn/policy/6000/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/rnn/policy/6000/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8000: average return = -4719.1 cullsteps = 108.5 cull_percent = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation, 1/0, 1/1 with unsupported characters which will be renamed to step_type, reward, discount, observation, unknown, unknown_0 in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as ActorRnnNetwork_layer_call_and_return_conditional_losses, ActorRnnNetwork_layer_call_fn, dynamic_unroll_layer_call_and_return_conditional_losses, dynamic_unroll_layer_call_fn, ActorRnnNetwork_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10000: average return = -3104.9 cullsteps = 89.9 cull_percent = 1.0\n",
      "New best return:  tf.Tensor([-3104.9333], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/rnn/policy/10000/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/rnn/policy/10000/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 12000: average return = -4600.9 cullsteps = 98.3 cull_percent = 1.0\n",
      "step 14000: average return = -3400.8 cullsteps = 95.2 cull_percent = 1.0\n",
      "step 16000: average return = -5502.5 cullsteps = 108.1 cull_percent = 1.0\n",
      "step 18000: average return = -3879.8 cullsteps = 102.9 cull_percent = 1.0\n",
      "step 20000: average return = -4578.2 cullsteps = 104.6 cull_percent = 1.0\n",
      "step 22000: average return = -5651.5 cullsteps = 115.7 cull_percent = 1.0\n",
      "step 24000: average return = -4494.1 cullsteps = 101.4 cull_percent = 1.0\n",
      "step 26000: average return = -3774.2 cullsteps = 93.8 cull_percent = 1.0\n",
      "step 28000: average return = -4392.8 cullsteps = 103.3 cull_percent = 1.0\n",
      "step 30000: average return = -6136.2 cullsteps = 107.1 cull_percent = 1.0\n",
      "step 32000: average return = -3677.4 cullsteps = 97.3 cull_percent = 1.0\n",
      "step 34000: average return = -7046.6 cullsteps = 111.3 cull_percent = 1.0\n",
      "step 36000: average return = -4079.8 cullsteps = 92.4 cull_percent = 1.0\n",
      "step 38000: average return = -4736.5 cullsteps = 92.1 cull_percent = 1.0\n",
      "step 40000: average return = -5705.6 cullsteps = 100.4 cull_percent = 1.0\n",
      "step 42000: average return = -4513.9 cullsteps = 89.7 cull_percent = 1.0\n",
      "step 44000: average return = -4690.4 cullsteps = 90.6 cull_percent = 1.0\n",
      "step 46000: average return = -4743.4 cullsteps = 86.0 cull_percent = 1.0\n",
      "step 48000: average return = -3866.5 cullsteps = 75.2 cull_percent = 1.0\n",
      "step 50000: average return = -5294.3 cullsteps = 86.5 cull_percent = 1.0\n",
      "step 52000: average return = -3822.1 cullsteps = 81.1 cull_percent = 1.0\n",
      "step 54000: average return = -3276.9 cullsteps = 71.5 cull_percent = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation, 1/0, 1/1 with unsupported characters which will be renamed to step_type, reward, discount, observation, unknown, unknown_0 in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as ActorRnnNetwork_layer_call_and_return_conditional_losses, ActorRnnNetwork_layer_call_fn, dynamic_unroll_layer_call_and_return_conditional_losses, dynamic_unroll_layer_call_fn, ActorRnnNetwork_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 56000: average return = -3100.2 cullsteps = 71.6 cull_percent = 1.0\n",
      "New best return:  tf.Tensor([-3100.1653], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/rnn/policy/56000/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/rnn/policy/56000/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 58000: average return = -5303.5 cullsteps = 90.5 cull_percent = 1.0\n",
      "step 60000: average return = -4501.4 cullsteps = 83.7 cull_percent = 1.0\n",
      "step 62000: average return = -3889.4 cullsteps = 80.0 cull_percent = 1.0\n",
      "step 64000: average return = -6032.1 cullsteps = 108.3 cull_percent = 1.0\n",
      "step 66000: average return = -5207.3 cullsteps = 86.1 cull_percent = 1.0\n",
      "step 68000: average return = -4386.6 cullsteps = 80.4 cull_percent = 1.0\n",
      "step 70000: average return = -3382.2 cullsteps = 76.7 cull_percent = 1.0\n",
      "step 72000: average return = -3964.0 cullsteps = 72.9 cull_percent = 1.0\n",
      "step 74000: average return = -3923.2 cullsteps = 79.5 cull_percent = 1.0\n",
      "step 76000: average return = -4471.5 cullsteps = 81.7 cull_percent = 1.0\n",
      "step 78000: average return = -4715.0 cullsteps = 82.7 cull_percent = 1.0\n",
      "step 80000: average return = -6991.3 cullsteps = 97.6 cull_percent = 1.0\n",
      "step 82000: average return = -4244.8 cullsteps = 80.3 cull_percent = 1.0\n",
      "step 84000: average return = -5390.9 cullsteps = 78.8 cull_percent = 1.0\n",
      "step 86000: average return = -4164.1 cullsteps = 74.0 cull_percent = 1.0\n",
      "step 88000: average return = -3177.7 cullsteps = 57.6 cull_percent = 1.0\n",
      "step 90000: average return = -5657.8 cullsteps = 81.3 cull_percent = 1.0\n",
      "step 92000: average return = -3687.9 cullsteps = 62.2 cull_percent = 1.0\n",
      "step 94000: average return = -3542.9 cullsteps = 70.2 cull_percent = 1.0\n",
      "step 96000: average return = -5672.8 cullsteps = 80.5 cull_percent = 1.0\n",
      "step 98000: average return = -3741.7 cullsteps = 57.9 cull_percent = 1.0\n",
      "step 100000: average return = -4889.6 cullsteps = 63.0 cull_percent = 1.0\n",
      "step 102000: average return = -3515.8 cullsteps = 51.6 cull_percent = 1.0\n",
      "step 104000: average return = -4384.7 cullsteps = 58.0 cull_percent = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation, 1/0, 1/1 with unsupported characters which will be renamed to step_type, reward, discount, observation, unknown, unknown_0 in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as ActorRnnNetwork_layer_call_and_return_conditional_losses, ActorRnnNetwork_layer_call_fn, dynamic_unroll_layer_call_and_return_conditional_losses, dynamic_unroll_layer_call_fn, ActorRnnNetwork_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 106000: average return = -2692.1 cullsteps = 46.8 cull_percent = 1.0\n",
      "New best return:  tf.Tensor([-2692.087], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/rnn/policy/106000/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/rnn/policy/106000/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 108000: average return = -6974.0 cullsteps = 64.7 cull_percent = 1.0\n",
      "step 110000: average return = -4185.1 cullsteps = 50.0 cull_percent = 1.0\n",
      "step 112000: average return = -4569.9 cullsteps = 48.3 cull_percent = 1.0\n",
      "step 114000: average return = -4847.2 cullsteps = 43.9 cull_percent = 1.0\n",
      "step 116000: average return = -3779.0 cullsteps = 37.4 cull_percent = 1.0\n",
      "step 118000: average return = -5438.2 cullsteps = 44.0 cull_percent = 1.0\n",
      "step 120000: average return = -4903.9 cullsteps = 42.2 cull_percent = 1.0\n",
      "step 122000: average return = -5556.3 cullsteps = 46.0 cull_percent = 1.0\n",
      "step 124000: average return = -7225.3 cullsteps = 56.9 cull_percent = 1.0\n",
      "step 126000: average return = -3987.9 cullsteps = 42.2 cull_percent = 1.0\n",
      "step 128000: average return = -6495.0 cullsteps = 50.0 cull_percent = 1.0\n",
      "step 130000: average return = -5217.1 cullsteps = 45.4 cull_percent = 1.0\n",
      "step 132000: average return = -3295.8 cullsteps = 36.9 cull_percent = 1.0\n",
      "step 134000: average return = -5331.9 cullsteps = 45.7 cull_percent = 1.0\n",
      "step 136000: average return = -4736.2 cullsteps = 49.4 cull_percent = 1.0\n",
      "step 138000: average return = -5025.9 cullsteps = 52.4 cull_percent = 1.0\n",
      "step 140000: average return = -4828.1 cullsteps = 57.6 cull_percent = 1.0\n",
      "step 142000: average return = -6224.3 cullsteps = 64.5 cull_percent = 1.0\n",
      "step 144000: average return = -5632.2 cullsteps = 70.4 cull_percent = 1.0\n",
      "step 146000: average return = -4699.2 cullsteps = 75.1 cull_percent = 1.0\n",
      "step 148000: average return = -3347.5 cullsteps = 59.3 cull_percent = 1.0\n",
      "step 150000: average return = -4137.9 cullsteps = 63.9 cull_percent = 1.0\n",
      "step 152000: average return = -5318.9 cullsteps = 75.9 cull_percent = 1.0\n",
      "step 154000: average return = -4698.8 cullsteps = 68.0 cull_percent = 1.0\n",
      "step 156000: average return = -4878.9 cullsteps = 70.0 cull_percent = 1.0\n",
      "step 158000: average return = -6529.8 cullsteps = 84.0 cull_percent = 1.0\n",
      "step 160000: average return = -3609.0 cullsteps = 58.1 cull_percent = 1.0\n",
      "step 162000: average return = -4014.5 cullsteps = 65.5 cull_percent = 1.0\n",
      "step 164000: average return = -3501.7 cullsteps = 55.5 cull_percent = 1.0\n",
      "step 166000: average return = -3178.6 cullsteps = 55.9 cull_percent = 1.0\n",
      "step 168000: average return = -3924.8 cullsteps = 62.2 cull_percent = 1.0\n",
      "step 170000: average return = -5934.3 cullsteps = 76.5 cull_percent = 1.0\n",
      "step 172000: average return = -4486.5 cullsteps = 68.7 cull_percent = 1.0\n",
      "step 174000: average return = -4994.4 cullsteps = 71.3 cull_percent = 1.0\n",
      "step 176000: average return = -4964.7 cullsteps = 72.1 cull_percent = 1.0\n",
      "step 178000: average return = -4180.4 cullsteps = 58.7 cull_percent = 1.0\n",
      "step 180000: average return = -5903.7 cullsteps = 71.9 cull_percent = 1.0\n",
      "step 182000: average return = -4461.5 cullsteps = 63.3 cull_percent = 1.0\n",
      "step 184000: average return = -4923.6 cullsteps = 61.0 cull_percent = 1.0\n",
      "step 186000: average return = -2899.5 cullsteps = 46.3 cull_percent = 1.0\n",
      "step 188000: average return = -3038.8 cullsteps = 46.9 cull_percent = 1.0\n",
      "step 190000: average return = -4714.2 cullsteps = 64.6 cull_percent = 1.0\n",
      "step 192000: average return = -3515.0 cullsteps = 54.5 cull_percent = 1.0\n",
      "step 194000: average return = -4767.6 cullsteps = 58.4 cull_percent = 1.0\n",
      "step 196000: average return = -4782.7 cullsteps = 60.9 cull_percent = 1.0\n",
      "step 198000: average return = -5601.8 cullsteps = 73.5 cull_percent = 1.0\n",
      "step 200000: average return = -5595.5 cullsteps = 76.0 cull_percent = 1.0\n",
      "step 202000: average return = -4261.0 cullsteps = 69.3 cull_percent = 1.0\n",
      "step 204000: average return = -3538.7 cullsteps = 58.0 cull_percent = 1.0\n",
      "step 206000: average return = -3725.0 cullsteps = 51.5 cull_percent = 1.0\n",
      "step 208000: average return = -3837.9 cullsteps = 56.1 cull_percent = 1.0\n",
      "step 210000: average return = -4746.6 cullsteps = 55.1 cull_percent = 1.0\n",
      "step 212000: average return = -3384.1 cullsteps = 43.9 cull_percent = 1.0\n",
      "step 214000: average return = -4130.1 cullsteps = 53.5 cull_percent = 1.0\n",
      "step 216000: average return = -6019.7 cullsteps = 60.1 cull_percent = 1.0\n",
      "step 218000: average return = -3945.1 cullsteps = 60.6 cull_percent = 1.0\n",
      "step 220000: average return = -5520.6 cullsteps = 62.8 cull_percent = 1.0\n",
      "step 222000: average return = -3689.7 cullsteps = 58.5 cull_percent = 1.0\n",
      "step 224000: average return = -4384.1 cullsteps = 58.1 cull_percent = 1.0\n",
      "step 226000: average return = -3538.4 cullsteps = 43.6 cull_percent = 1.0\n",
      "step 228000: average return = -4295.9 cullsteps = 54.9 cull_percent = 1.0\n",
      "step 230000: average return = -4812.5 cullsteps = 62.6 cull_percent = 1.0\n",
      "step 232000: average return = -4935.7 cullsteps = 57.9 cull_percent = 1.0\n",
      "step 234000: average return = -5756.7 cullsteps = 64.8 cull_percent = 1.0\n",
      "step 236000: average return = -3496.9 cullsteps = 53.5 cull_percent = 1.0\n",
      "step 238000: average return = -4687.8 cullsteps = 61.6 cull_percent = 1.0\n",
      "step 240000: average return = -4445.9 cullsteps = 57.0 cull_percent = 1.0\n",
      "step 242000: average return = -4552.4 cullsteps = 56.1 cull_percent = 1.0\n",
      "step 244000: average return = -3048.2 cullsteps = 51.9 cull_percent = 1.0\n",
      "step 246000: average return = -3430.9 cullsteps = 53.8 cull_percent = 1.0\n",
      "step 248000: average return = -3199.5 cullsteps = 54.0 cull_percent = 1.0\n",
      "step 250000: average return = -3607.3 cullsteps = 60.5 cull_percent = 1.0\n",
      "step 252000: average return = -2739.1 cullsteps = 52.2 cull_percent = 1.0\n",
      "step 254000: average return = -3837.1 cullsteps = 57.6 cull_percent = 1.0\n",
      "step 256000: average return = -4530.4 cullsteps = 57.4 cull_percent = 1.0\n",
      "step 258000: average return = -3864.8 cullsteps = 51.2 cull_percent = 1.0\n",
      "step 260000: average return = -3016.5 cullsteps = 48.1 cull_percent = 1.0\n",
      "step 262000: average return = -3868.8 cullsteps = 55.4 cull_percent = 1.0\n",
      "step 264000: average return = -5318.4 cullsteps = 63.2 cull_percent = 1.0\n",
      "step 266000: average return = -4366.5 cullsteps = 52.7 cull_percent = 1.0\n",
      "step 268000: average return = -3398.5 cullsteps = 49.8 cull_percent = 1.0\n",
      "step 270000: average return = -4618.3 cullsteps = 59.0 cull_percent = 1.0\n",
      "step 272000: average return = -5738.8 cullsteps = 67.2 cull_percent = 1.0\n",
      "step 274000: average return = -4980.9 cullsteps = 66.8 cull_percent = 1.0\n",
      "step 276000: average return = -3361.6 cullsteps = 53.9 cull_percent = 1.0\n",
      "step 278000: average return = -4406.7 cullsteps = 61.1 cull_percent = 1.0\n",
      "step 280000: average return = -2900.6 cullsteps = 54.5 cull_percent = 1.0\n",
      "step 282000: average return = -4780.5 cullsteps = 61.0 cull_percent = 1.0\n",
      "step 284000: average return = -3730.4 cullsteps = 68.0 cull_percent = 1.0\n",
      "step 286000: average return = -3841.7 cullsteps = 70.9 cull_percent = 1.0\n",
      "step 288000: average return = -4444.3 cullsteps = 72.2 cull_percent = 1.0\n",
      "step 290000: average return = -3527.0 cullsteps = 66.5 cull_percent = 1.0\n",
      "step 292000: average return = -2868.4 cullsteps = 63.4 cull_percent = 1.0\n",
      "step 294000: average return = -3285.8 cullsteps = 70.9 cull_percent = 1.0\n",
      "step 296000: average return = -3725.8 cullsteps = 83.1 cull_percent = 1.0\n",
      "step 298000: average return = -3741.2 cullsteps = 88.3 cull_percent = 1.0\n",
      "step 300000: average return = -3140.1 cullsteps = 94.0 cull_percent = 1.0\n",
      "step 302000: average return = -3336.7 cullsteps = 82.9 cull_percent = 1.0\n",
      "step 304000: average return = -3318.0 cullsteps = 80.8 cull_percent = 1.0\n",
      "step 306000: average return = -3157.6 cullsteps = 75.2 cull_percent = 1.0\n",
      "step 308000: average return = -8113.3 cullsteps = 39.2 cull_percent = 1.0\n",
      "step 310000: average return = -4042.4 cullsteps = 74.4 cull_percent = 1.0\n",
      "step 312000: average return = -4281.9 cullsteps = 69.2 cull_percent = 1.0\n",
      "step 314000: average return = -3524.1 cullsteps = 71.8 cull_percent = 1.0\n",
      "step 316000: average return = -3484.9 cullsteps = 69.8 cull_percent = 1.0\n",
      "step 318000: average return = -3516.8 cullsteps = 76.6 cull_percent = 1.0\n",
      "step 320000: average return = -2899.4 cullsteps = 66.6 cull_percent = 1.0\n",
      "step 322000: average return = -5295.7 cullsteps = 80.3 cull_percent = 1.0\n",
      "step 324000: average return = -5664.4 cullsteps = 98.2 cull_percent = 1.0\n",
      "step 326000: average return = -4554.8 cullsteps = 73.2 cull_percent = 1.0\n",
      "step 328000: average return = -6239.3 cullsteps = 90.1 cull_percent = 1.0\n",
      "step 330000: average return = -4946.1 cullsteps = 79.2 cull_percent = 1.0\n",
      "step 334000: average return = -3984.8 cullsteps = 83.7 cull_percent = 1.0\n",
      "step 336000: average return = -4264.5 cullsteps = 89.6 cull_percent = 1.0\n",
      "step 338000: average return = -3434.6 cullsteps = 80.8 cull_percent = 1.0\n",
      "step 340000: average return = -4937.9 cullsteps = 99.2 cull_percent = 1.0\n",
      "step 342000: average return = -6757.7 cullsteps = 107.7 cull_percent = 1.0\n",
      "step 344000: average return = -3979.9 cullsteps = 83.8 cull_percent = 1.0\n",
      "step 346000: average return = -3235.6 cullsteps = 70.5 cull_percent = 1.0\n",
      "step 348000: average return = -3203.9 cullsteps = 77.1 cull_percent = 1.0\n",
      "step 350000: average return = -3877.1 cullsteps = 79.6 cull_percent = 1.0\n",
      "step 352000: average return = -4799.9 cullsteps = 81.8 cull_percent = 1.0\n",
      "step 354000: average return = -4086.1 cullsteps = 80.5 cull_percent = 1.0\n",
      "step 356000: average return = -3664.3 cullsteps = 78.4 cull_percent = 1.0\n",
      "step 358000: average return = -5895.8 cullsteps = 102.2 cull_percent = 1.0\n",
      "step 360000: average return = -3090.7 cullsteps = 74.9 cull_percent = 1.0\n",
      "step 362000: average return = -5963.0 cullsteps = 106.5 cull_percent = 1.0\n",
      "step 364000: average return = -4082.0 cullsteps = 87.2 cull_percent = 1.0\n",
      "step 366000: average return = -4956.7 cullsteps = 95.2 cull_percent = 1.0\n",
      "step 368000: average return = -3477.9 cullsteps = 72.8 cull_percent = 1.0\n",
      "step 370000: average return = -4014.7 cullsteps = 74.4 cull_percent = 1.0\n"
     ]
    }
   ],
   "source": [
    "train_eval(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
