{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the environment `HerdEnv` of `herd_env.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "from tf_agents.trajectories import TimeStep\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RNN_Env import FREnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "henv_val = FREnv(herd_sizes = [64,64], rand_recovery_prob = 0.1, rand_infection_prob = 0.05)\n",
    "utils.validate_py_environment(henv_val, episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Herd Environment instance to be trained for\n",
    "max_episode_length=1000\n",
    "num_herds=2\n",
    "henv = FREnv(herd_sizes = [32,32], expected_episode_length=100, max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.02, rand_infection_prob = 0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': 1.0,\n",
      " 'observation': array([0., 0., 0.], dtype=float32),\n",
      " 'reward': 0.0,\n",
      " 'step_type': array(0, dtype=int32)})\n",
      "state:  [1 2 1 2 1 1] observation:  [0.001 0.001 0.001] \treward:  -0.075\n",
      "state:  [1 4 2 6 2 2] observation:  [0.002 0.002 0.002] \treward:  -0.2\n",
      "state:  [ 6  6  8 12  3  3] observation:  [0.003 0.003 0.003] \treward:  -0.5\n",
      "state:  [ 6  7 14 19  4  4] observation:  [0.004 0.004 0.004] \treward:  -0.825\n",
      "state:  [ 8 10 22 29  5  5] observation:  [0.005 0.005 0.005] \treward:  -1.275\n",
      "state:  [ 9 11 31 40  6  6] observation:  [0.006 0.006 0.006] \treward:  -1.775\n",
      "state:  [11 12 42 52  7  7] observation:  [0.007 0.007 0.007] \treward:  -2.35\n",
      "state:  [12 13 54 65  8  8] observation:  [0.008 0.008 0.008] \treward:  -2.975\n",
      "state:  [13 14 67 79  9  9] observation:  [0.009 0.009 0.009] \treward:  -3.65\n",
      "state:  [14 14 81 93 10 10] observation:  [0.01 0.01 0.01] \treward:  -4.35\n",
      "state:  [ 15  16  96 109  11  11] observation:  [0.011 0.011 0.011] \treward:  -5.125\n",
      "state:  [ 18  17 114 126  12  12] observation:  [0.012 0.012 0.012] \treward:  -6.0\n",
      "state:  [ 18  19 132 145  13  13] observation:  [0.013 0.013 0.013] \treward:  -6.925\n",
      "state:  [ 19  20 151 165  14  14] observation:  [0.014 0.014 0.014] \treward:  -7.9\n",
      "state:  [ 20  22 171 187  15  15] observation:  [0.015 0.015 0.015] \treward:  -8.95\n",
      "state:  [ 16  25 187 212  16  16] observation:  [0.016 0.016 0.016] \treward:  -9.975\n",
      "state:  [ 17  24 204 236  17  17] observation:  [0.017 0.017 0.017] \treward:  -11.0\n",
      "state:  [ 18  25 222 261  18  18] observation:  [0.018 0.018 0.018] \treward:  -12.075\n",
      "state:  [ 19  25 241 286  19  19] observation:  [0.019 0.019 0.019] \treward:  -13.175\n",
      "state:  [ 19  25 260 311  20  20] observation:  [0.02 0.02 0.02] \treward:  -14.275\n",
      "state:  [ 20  25 280 336  21  21] observation:  [0.021 0.021 0.021] \treward:  -15.4\n",
      "state:  [ 21  26 301 362  22  22] observation:  [0.022 0.022 0.022] \treward:  -16.575\n",
      "state:  [ 25  27 326 389  23  23] observation:  [0.023 0.023 0.023] \treward:  -17.875\n",
      "state:  [ 25  28 351 417  24  24] observation:  [0.024 0.024 0.024] \treward:  -19.2\n",
      "state:  [ 25  29 376 446  25  25] observation:  [0.025 0.025 0.025] \treward:  -20.55\n",
      "state:  [ 24  29 400 475  26  26] observation:  [0.026 0.026 0.026] \treward:  -21.875\n",
      "state:  [ 24  29 424 504  27  27] observation:  [0.027 0.027 0.027] \treward:  -23.2\n",
      "state:  [ 23  29 447 533  28  28] observation:  [0.028 0.028 0.028] \treward:  -24.5\n",
      "state:  [ 23  30 470 563  29  29] observation:  [0.029 0.029 0.029] \treward:  -25.825\n",
      "state:  [ 22  30 492 593  30  30] observation:  [0.03 0.03 0.03] \treward:  -27.125\n",
      "state:  [ 25  28 517 621  31  31] observation:  [0.031 0.031 0.031] \treward:  -28.45\n",
      "state:  [ 27  28 544 649  32  32] observation:  [0.032 0.032 0.032] \treward:  -29.825\n",
      "state:  [ 27  28 571 677  33  33] observation:  [0.033 0.033 0.033] \treward:  -31.2\n",
      "state:  [ 28  28 599 705  34  34] observation:  [0.034 0.034 0.034] \treward:  -32.6\n",
      "state:  [ 28  27 627 732  35  35] observation:  [0.035 0.035 0.035] \treward:  -33.975\n",
      "state:  [ 28  26 655 758  36  36] observation:  [0.036 0.036 0.036] \treward:  -35.325\n",
      "state:  [ 29  26 684 784  37  37] observation:  [0.037 0.037 0.037] \treward:  -36.7\n",
      "state:  [ 29  26 713 810  38  38] observation:  [0.038 0.038 0.038] \treward:  -38.075\n",
      "state:  [ 29  25 742 835  39  39] observation:  [0.039 0.039 0.039] \treward:  -39.425\n",
      "state:  [ 30  23 772 858  40  40] observation:  [0.04 0.04 0.04] \treward:  -40.75\n",
      "state:  [ 31  25 803 883  41  41] observation:  [0.041 0.041 0.041] \treward:  -42.15\n",
      "state:  [ 30  25 833 908  42  42] observation:  [0.042 0.042 0.042] \treward:  -43.525\n",
      "state:  [ 30  25 863 933  43  43] observation:  [0.043 0.043 0.043] \treward:  -44.9\n",
      "state:  [ 30  27 893 960  44  44] observation:  [0.044 0.044 0.044] \treward:  -46.325\n",
      "state:  [ 30  26 923 986  45  45] observation:  [0.045 0.045 0.045] \treward:  -47.725\n",
      "state:  [  30   26  953 1012   46   46] observation:  [0.046 0.046 0.046] \treward:  -49.125\n",
      "state:  [  29   27  982 1039   47   47] observation:  [0.047 0.047 0.047] \treward:  -50.525\n",
      "state:  [  30   27 1012 1066   48   48] observation:  [0.048 0.048 0.048] \treward:  -51.95\n",
      "state:  [  28   28 1040 1094   49   49] observation:  [0.049 0.049 0.049] \treward:  -53.35\n",
      "state:  [  28   29 1068 1123   50   50] observation:  [0.05 0.05 0.05] \treward:  -54.775\n",
      "state:  [  28   28 1096 1151   51   51] observation:  [0.051 0.051 0.051] \treward:  -56.175\n",
      "state:  [  27   28 1123 1179   52   52] observation:  [0.052 0.052 0.052] \treward:  -57.55\n",
      "state:  [  27   29 1150 1208   53   53] observation:  [0.053 0.053 0.053] \treward:  -58.95\n",
      "state:  [  28   28 1178 1236   54   54] observation:  [0.054 0.054 0.054] \treward:  -60.35\n",
      "state:  [  28   29 1206 1265   55   55] observation:  [0.055 0.055 0.055] \treward:  -61.775\n",
      "state:  [  29   28 1235 1293   56   56] observation:  [0.056 0.056 0.056] \treward:  -63.2\n",
      "state:  [  29   29 1264 1322   57   57] observation:  [0.057 0.057 0.057] \treward:  -64.65\n",
      "state:  [  29   29 1293 1351   58   58] observation:  [0.058 0.058 0.058] \treward:  -66.1\n",
      "state:  [  29   28 1322 1379   59   59] observation:  [0.059 0.059 0.059] \treward:  -67.525\n",
      "state:  [  29   30 1351 1409   60   60] observation:  [0.06 0.06 0.06] \treward:  -69.0\n",
      "state:  [  29   31 1380 1440   61   61] observation:  [0.061 0.061 0.061] \treward:  -70.5\n",
      "state:  [  29   31 1409 1471   62   62] observation:  [0.062 0.062 0.062] \treward:  -72.0\n",
      "state:  [  28   30 1437 1501   63   63] observation:  [0.063 0.063 0.063] \treward:  -73.45\n",
      "state:  [  28   29 1465 1530   64   64] observation:  [0.064 0.064 0.064] \treward:  -74.875\n",
      "state:  [  28   27 1493 1557   65   65] observation:  [0.065 0.065 0.065] \treward:  -76.25\n",
      "state:  [  28   28 1521 1585   66   66] observation:  [0.066 0.066 0.066] \treward:  -77.65\n",
      "state:  [  26   29 1547 1614   67   67] observation:  [0.067 0.067 0.067] \treward:  -79.025\n",
      "state:  [  27   29 1574 1643   68   68] observation:  [0.068 0.068 0.068] \treward:  -80.425\n",
      "state:  [  28   27 1602 1670   69   69] observation:  [0.069 0.069 0.069] \treward:  -81.8\n",
      "state:  [  25   27 1627 1697   70   70] observation:  [0.07 0.07 0.07] \treward:  -83.1\n",
      "state:  [  25   28 1652 1725   71   71] observation:  [0.071 0.071 0.071] \treward:  -84.425\n",
      "state:  [  26   31 1678 1756   72   72] observation:  [0.072 0.072 0.072] \treward:  -85.85\n",
      "state:  [  25   31 1703 1787   73   73] observation:  [0.073 0.073 0.073] \treward:  -87.25\n",
      "state:  [  24   31 1727 1818   74   74] observation:  [0.074 0.074 0.074] \treward:  -88.625\n",
      "state:  [  24   30 1751 1848   75   75] observation:  [0.075 0.075 0.075] \treward:  -89.975\n",
      "state:  [  25   29 1776 1877   76   76] observation:  [0.076 0.076 0.076] \treward:  -91.325\n",
      "state:  [  25   28 1801 1905   77   77] observation:  [0.077 0.077 0.077] \treward:  -92.65\n",
      "state:  [  24   29 1825 1934   78   78] observation:  [0.078 0.078 0.078] \treward:  -93.975\n",
      "state:  [  25   30 1850 1964   79   79] observation:  [0.079 0.079 0.079] \treward:  -95.35\n",
      "state:  [  27   29 1877 1993   80   80] observation:  [0.08 0.08 0.08] \treward:  -96.75\n",
      "state:  [  27   29 1904 2022   81   81] observation:  [0.081 0.081 0.081] \treward:  -98.15\n",
      "state:  [  28   30 1932 2052   82   82] observation:  [0.082 0.082 0.082] \treward:  -99.6\n",
      "state:  [  28   30 1960 2082   83   83] observation:  [0.083 0.083 0.083] \treward:  -101.05\n",
      "state:  [  28   30 1988 2112   84   84] observation:  [0.084 0.084 0.084] \treward:  -102.5\n",
      "state:  [  28   30 2016 2142   85   85] observation:  [0.085 0.085 0.085] \treward:  -103.95\n",
      "state:  [  28   29 2044 2171   86   86] observation:  [0.086 0.086 0.086] \treward:  -105.375\n",
      "state:  [  27   30 2071 2201   87   87] observation:  [0.087 0.087 0.087] \treward:  -106.8\n",
      "state:  [  29   30 2100 2231   88   88] observation:  [0.088 0.088 0.088] \treward:  -108.275\n",
      "state:  [  28   30 2128 2261   89   89] observation:  [0.089 0.089 0.089] \treward:  -109.725\n",
      "state:  [  28   30 2156 2291   90   90] observation:  [0.09 0.09 0.09] \treward:  -111.175\n",
      "state:  [  27   29 2183 2320   91   91] observation:  [0.091 0.091 0.091] \treward:  -112.575\n",
      "state:  [  28   27 2211 2347   92   92] observation:  [0.092 0.092 0.092] \treward:  -113.95\n",
      "Final Reward =  -4729.8257\n"
     ]
    }
   ],
   "source": [
    "# show interor values of environment\n",
    "time_step = henv.reset()\n",
    "print(time_step)\n",
    "cumulative_reward = time_step.reward\n",
    "finished = False\n",
    "\n",
    "while not finished:\n",
    "  time_step = henv.step([0.,0.]) # do nothing\n",
    "  s = henv.get_state()\n",
    "  print(\"state: \", s, \"observation: \", time_step.observation, \"\\treward: \", time_step.reward)\n",
    "  cumulative_reward += time_step.reward\n",
    "  if time_step.step_type == StepType.LAST:\n",
    "    finished = True\n",
    "\n",
    "print('Final Reward = ', cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action spec:\n",
      " BoundedArraySpec(shape=(2,), dtype=dtype('float32'), name='action', minimum=0.0, maximum=1.0) \n",
      "\n",
      "time step spec:\n",
      " TimeStep(\n",
      "{'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
      " 'observation': BoundedArraySpec(shape=(3,), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=1.0),\n",
      " 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n",
      " 'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "action_spec = henv.action_spec()\n",
    "ts_spec = henv.time_step_spec()\n",
    "print(\"action spec:\\n\", action_spec, \"\\n\\ntime step spec:\\n\", ts_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scripted policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do nothing policy: cull never\n",
    "action_script0 = [(max_episode_length, [0,0])]\n",
    "\n",
    "# cull first herd every 20th step and second herd every 20th step\n",
    "action_script1 = [(19, [0,0]), \n",
    "                 (1, [1,1]),\n",
    "                 (19, [0,0]), \n",
    "                 (1, [1,1])] * int(1+max_episode_length/40)\n",
    "\n",
    "manual_scripted_policy0 = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=ts_spec,\n",
    "    action_spec=action_spec,\n",
    "    action_script=action_script0)\n",
    "\n",
    "manual_scripted_policy1 = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=ts_spec,\n",
    "    action_spec=action_spec,\n",
    "    action_script=action_script1)\n",
    "\n",
    "init_policy_state = manual_scripted_policy0.get_initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action= [0. 0.] \tpolicy_state [0, 1]\n",
      "action= [0. 0.] \tpolicy_state [0, 2]\n",
      "action= [0. 0.] \tpolicy_state [0, 3]\n",
      "action= [0. 0.] \tpolicy_state [0, 4]\n",
      "action= [0. 0.] \tpolicy_state [0, 5]\n",
      "action= [0. 0.] \tpolicy_state [0, 6]\n",
      "action= [0. 0.] \tpolicy_state [0, 7]\n",
      "action= [0. 0.] \tpolicy_state [0, 8]\n",
      "action= [0. 0.] \tpolicy_state [0, 9]\n",
      "action= [0. 0.] \tpolicy_state [0, 10]\n",
      "action= [0. 0.] \tpolicy_state [0, 11]\n",
      "action= [0. 0.] \tpolicy_state [0, 12]\n",
      "action= [0. 0.] \tpolicy_state [0, 13]\n",
      "action= [0. 0.] \tpolicy_state [0, 14]\n",
      "action= [0. 0.] \tpolicy_state [0, 15]\n",
      "action= [0. 0.] \tpolicy_state [0, 16]\n",
      "action= [0. 0.] \tpolicy_state [0, 17]\n",
      "action= [0. 0.] \tpolicy_state [0, 18]\n",
      "action= [0. 0.] \tpolicy_state [0, 19]\n",
      "action= [1. 1.] \tpolicy_state [1, 1]\n",
      "action= [0. 0.] \tpolicy_state [2, 1]\n"
     ]
    }
   ],
   "source": [
    "policy_state =  init_policy_state\n",
    "ts0 = henv.reset()\n",
    "for _ in range(21):\n",
    "    action_step = manual_scripted_policy1.action(ts0, policy_state)\n",
    "    policy_state = action_step.state\n",
    "    print(\"action=\", action_step.action, \"\\tpolicy_state\", policy_state)\n",
    "policy_state = manual_scripted_policy1.get_initial_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_policy = random_py_policy.RandomPyPolicy(time_step_spec=ts_spec, action_spec=action_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drive a rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=50, verbose=False):\n",
    "  total_return = 0.0\n",
    "  cullsteps = 0\n",
    "  for e in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "        policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "    else:\n",
    "        policy_state = None # other policies without memory\n",
    "    episode_return = 0.0\n",
    "    i=0\n",
    "    while not time_step.is_last():\n",
    "        i+=1\n",
    "        action_step = policy.action(time_step, policy_state)\n",
    "        if action_step.action[0] > 0 or action_step.action[1] > 0:\n",
    "            cullsteps += 1\n",
    "        policy_state = action_step.state\n",
    "        time_step = environment.step(action_step.action)\n",
    "        if isinstance(environment, FREnv):\n",
    "            state = environment.get_state()\n",
    "        else:\n",
    "            state = None # TF environment from wrapper does not have get_state()\n",
    "        episode_return += time_step.reward\n",
    "        if verbose:\n",
    "            print (f\"episode {e:>2} step{i:>4} action: \", action_step.action, \"state=\", state, \"obs=\", time_step.observation, \"reward=\", time_step.reward)\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  cullsteps /= num_episodes\n",
    "  return avg_return, cullsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average return of random policy: -4403.846 avg steps with culls per episode: 106.9\n"
     ]
    }
   ],
   "source": [
    "random_reward, cullsteps = compute_avg_return(henv, random_policy)\n",
    "print (f\"average return of random policy: {random_reward:.3f} avg steps with culls per episode: {cullsteps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 step   1 action:  [0. 0.] state= [3 1 3 1 1 1] obs= [0.001 0.001 0.001] reward= -0.1\n",
      "episode  0 step   2 action:  [0. 0.] state= [5 2 8 3 2 2] obs= [0.002 0.002 0.002] reward= -0.275\n",
      "episode  0 step   3 action:  [0. 0.] state= [ 5  3 13  6  3  3] obs= [0.003 0.003 0.003] reward= -0.475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.8500000014901161, 0.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show states for one rollout of second scripted policy\n",
    "compute_avg_return(henv, manual_scripted_policy1, num_episodes=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average return of do-nothing-policy: -9756.599 avg culls 0.0\n",
      "average return of manual policy: -7680.644 avg culls 4.725\n"
     ]
    }
   ],
   "source": [
    "manual_reward0, cullsteps = compute_avg_return(henv, manual_scripted_policy0, num_episodes=200)\n",
    "print (f\"average return of do-nothing-policy: {manual_reward0:.3f} avg culls {cullsteps}\")\n",
    "manual_reward1, cullsteps = compute_avg_return(henv, manual_scripted_policy1, num_episodes=200)\n",
    "print (f\"average return of manual policy: {manual_reward1:.3f} avg culls {cullsteps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=50, verbose=False):\n",
    "  total_return = 0.0\n",
    "  cullsteps = 0\n",
    "  for e in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "        policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "    else:\n",
    "        #print(policy.get_initial_state(batch_size=train_env.batch_size()))\n",
    "        policy_state = policy.get_initial_state(batch_size=1) # other policies without memory\n",
    "    episode_return = 0.0\n",
    "    i=0\n",
    "    while not time_step.is_last():\n",
    "        i+=1\n",
    "        action_step = policy.action(time_step, policy_state)\n",
    "        if action_step.action[0][0] > 0 or action_step.action[0][1] > 0:\n",
    "            cullsteps += 1\n",
    "        policy_state = action_step.state\n",
    "        time_step = environment.step(action_step.action)\n",
    "        if isinstance(environment, FREnv):\n",
    "            state = environment.get_state()\n",
    "        else:\n",
    "            state = None # TF environment from wrapper does not have get_state()\n",
    "        episode_return += time_step.reward\n",
    "        if verbose:\n",
    "            print (f\"episode {e:>2} step{i:>4} action: \", action_step.action, \"state=\", state, \"obs=\", time_step.observation, \"reward=\", time_step.reward)\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  cullsteps /= num_episodes\n",
    "  return avg_return, cullsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Made it work to here, implement ddpg agent next.'''\n",
    "\n",
    "import functools\n",
    "from tf_agents.networks import nest_map\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.keras_layers import inner_reshape\n",
    "from tf_agents.networks.sequential import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.agents.ddpg import actor_rnn_network\n",
    "from tf_agents.agents.ddpg import critic_rnn_network\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 30000\n",
    "replay_buffer_max_length = 10000\n",
    "batch_size = 64\n",
    "num_eval_episodes = 100\n",
    "initial_collect_steps = 1000\n",
    "collect_steps_per_iteration = 5\n",
    "log_interval = 500\n",
    "eval_interval = 500\n",
    "target_update_period = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(FREnv(herd_sizes = [32,32], expected_episode_length=100, max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.02, rand_infection_prob = 0.06))\n",
    "eval_env = tf_py_environment.TFPyEnvironment(FREnv(herd_sizes = [32,32], expected_episode_length=100, max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.02, rand_infection_prob = 0.06))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(2,), dtype=tf.float32, name='action', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03)\n",
    "\n",
    "#Make critic and actor net for ddpg\n",
    "\n",
    "actor_fc_layers=(200, 150)\n",
    "critic_obs_fc_layers=(200,)\n",
    "critic_action_fc_layers=None\n",
    "critic_joint_fc_layers=(150,)\n",
    "lstm_sz = 1\n",
    "\n",
    "def lstm_keras_fn(lstm_size):\n",
    "    return tf.keras.layers.LSTM(lstm_size, return_state=True,\n",
    "                              return_sequences=True)\n",
    "\n",
    "actor_net = actor_rnn_network.ActorRnnNetwork(\n",
    "               input_tensor_spec = train_env.observation_spec(),\n",
    "               output_tensor_spec = train_env.action_spec(),\n",
    "               #conv_layer_params=None,  #None\n",
    "               #input_fc_layer_params=(200, 100),\n",
    "               #stm_size=None\n",
    "              #output_fc_layer_params=(200, 100),\n",
    "               activation_fn=tf.keras.activations.relu,\n",
    "               name='ActorRnnNetwork')\n",
    "\n",
    "input_spec = (train_env.observation_spec(), train_env.action_spec())\n",
    "\n",
    "critic_net = critic_rnn_network.CriticRnnNetwork(\n",
    "               input_tensor_spec = input_spec,\n",
    "               #observation_conv_layer_params=None,\n",
    "               #observation_fc_layer_params=(200,),\n",
    "               #action_fc_layer_params=(100,),\n",
    "              # joint_fc_layer_params=(150,),\n",
    "               lstm_size=None,\n",
    "               #output_fc_layer_params=(200,),\n",
    "               activation_fn=tf.keras.activations.relu,\n",
    "               #kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "                            #minval=-0.003, maxval=0.003),\n",
    "              # last_kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "                            #minval=-0.003, maxval=0.003),\n",
    "               rnn_construction_fn=lstm_keras_fn,\n",
    "               rnn_construction_kwargs={'lstm_size': 1},\n",
    "               name='CriticRnnNetwork')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_counter = tf.Variable(0)\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "agent = ddpg_agent.DdpgAgent(\n",
    "        train_env.time_step_spec(),\n",
    "        train_env.action_spec(),\n",
    "        actor_network=actor_net,\n",
    "        critic_network=critic_net,\n",
    "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=1e-4),\n",
    "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=1e-3),\n",
    "        ou_stddev=0.2,\n",
    "        ou_damping=0.15,\n",
    "        target_update_tau=0.05,\n",
    "        target_update_period=5,\n",
    "        #dqda_clipping=dqda_clipping,\n",
    "        td_errors_loss_fn=tf.compat.v1.losses.huber_loss,\n",
    "        gamma=0.99,\n",
    "        #reward_scale_factor=reward_scale_factor,\n",
    "        #gradient_clipping=gradient_clipping,\n",
    "        #debug_summaries=debug_summaries,\n",
    "        #summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "        train_step_counter=global_step)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually initialize a reasonably good policy: kill both herds if the sum of observations is large\n",
    "#W = np.array([[0, 3 ,0, 2],[0, 0, 3, 2,]])\n",
    "#b = np.array([1, 0, 0, 0])\n",
    "#q_net.layers[0].set_weights([W,b])\n",
    "#agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(\n",
       "{action: (64, 2, 2),\n",
       " discount: (64, 2),\n",
       " next_step_type: (64, 2),\n",
       " observation: (64, 2, 3),\n",
       " policy_info: (),\n",
       " reward: (64, 2),\n",
       " step_type: (64, 2)}), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(\n",
       "{action: tf.float32,\n",
       " discount: tf.float32,\n",
       " next_step_type: tf.int32,\n",
       " observation: tf.float32,\n",
       " policy_info: (),\n",
       " reward: tf.float32,\n",
       " step_type: tf.int32}), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Invalid reduction dimension (1 for input with 0 dimension(s) [Op:Sum]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-922e0ab99a44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;31m# Sample a batch of data from the buffer and update the agent's network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tf_agents/agents/tf_agent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, experience, weights, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_functions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m       loss_info = self._train_fn(\n\u001b[0m\u001b[1;32m    332\u001b[0m           experience=experience, weights=weights, **kwargs)\n\u001b[1;32m    333\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tf_agents/agents/ddpg/ddpg_agent.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, experience, weights)\u001b[0m\n\u001b[1;32m    246\u001b[0m                                           'optimize.')\n\u001b[1;32m    247\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_critic_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m       critic_loss = self.critic_loss(time_steps, actions, next_time_steps,\n\u001b[0m\u001b[1;32m    249\u001b[0m                                      weights=weights, training=True)\n\u001b[1;32m    250\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_numerics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Critic loss is inf or nan.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tf_agents/agents/ddpg/ddpg_agent.py\u001b[0m in \u001b[0;36mcritic_loss\u001b[0;34m(self, time_steps, actions, next_time_steps, weights, training)\u001b[0m\n\u001b[1;32m    343\u001b[0m           time_steps, self.time_step_spec, num_outer_dims=2):\n\u001b[1;32m    344\u001b[0m         \u001b[0;31m# Do a sum over the time dimension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mcritic_loss\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2251\u001b[0m   \"\"\"\n\u001b[1;32m   2252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2253\u001b[0;31m   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[0m\u001b[1;32m   2254\u001b[0m                               _ReductionDims(input_tensor, axis))\n\u001b[1;32m   2255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum_with_dims\u001b[0;34m(input_tensor, axis, keepdims, name, dims)\u001b[0m\n\u001b[1;32m   2263\u001b[0m   return _may_reduce_to_scalar(\n\u001b[1;32m   2264\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2265\u001b[0;31m       gen_math_ops._sum(input_tensor, dims, keepdims, name=name))\n\u001b[0m\u001b[1;32m   2266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m  10708\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10709\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10710\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10711\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10712\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6939\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6940\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6941\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6942\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Invalid reduction dimension (1 for input with 0 dimension(s) [Op:Sum]"
     ]
    }
   ],
   "source": [
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step <= 1 or step % log_interval == 0:\n",
    "    print('step = {0:4>}: loss = {1:.4f}'.format(step, train_loss), end=\"\\t\")\n",
    "\n",
    "  if step <= 1 or (step <= 100 and step % 50 == 0) or step % eval_interval == 0:\n",
    "    avg_return, cullsteps = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step {0}: average return = {1:.1f} cullsteps = {2:.1f}'.format(step, avg_return.numpy().item(), cullsteps))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_reward, culleps = compute_avg_return(eval_env, agent.policy, num_episodes=500)\n",
    "print (\"reward of learned policy: \", learned_reward.numpy(), \"cullsteps=\", culleps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ts = eval_env.reset()\n",
    "\n",
    "def get_action(obs):\n",
    "    \"\"\" execute the learned policy network \n",
    "       obs:  one float for global time, one float for each herd - the time since last culling\n",
    "    \"\"\"\n",
    "    _ts = TimeStep(tf.constant([0.]),\n",
    "                   tf.constant([0.]),\n",
    "                   tf.constant([1]),\n",
    "                   tf.constant([obs]))\n",
    "    # a = agent.collect_policy.action(_ts) # just to see how much is explored versus exploited\n",
    "    a = agent.policy.action(_ts)\n",
    "    return a.action.numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what the learned policy does on a grid of observations (5 steps per row&col)\n",
    "'''A = [[get_action([.0, x,y])\n",
    " for y in np.arange(0.,1.,.05,np.float32)]\n",
    " for x in np.arange(0.,1.,.05,np.float32)]\n",
    "A'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with parameters of manually designed q_network policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W, b = agent.policy.trainable_variables\n",
    "# W = W.numpy()\n",
    "# b = b.numpy()\n",
    "# print (\"weights\\n\", W, \"\\nbias\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nn(obs):\n",
    "#    y = np.dot(obs, W)+b\n",
    "#    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn([0.5,.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
