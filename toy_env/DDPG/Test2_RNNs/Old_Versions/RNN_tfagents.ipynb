{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67488708-a8a7-4fd3-aa8b-b90b3d9e461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The TF-Agents Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Lint as: python2, python3\n",
    "r\"\"\"Train and Eval DDPG.\n",
    "\n",
    "To run:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir $HOME/tmp/ddpg_rnn/dm/CartPole-Balance/ --port 2223 &\n",
    "\n",
    "python tf_agents/agents/ddpg/examples/v2/train_eval_rnn.py \\\n",
    "  --root_dir=$HOME/tmp/ddpg_rnn/dm/CartPole-Balance/ \\\n",
    "  --num_iterations=100000 \\\n",
    "  --alsologtostderr\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "root_dir = '~/Masterarbeit/rnn'\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "\n",
    "from absl import app\n",
    "from absl import logging\n",
    "\n",
    "import gin\n",
    "from six.moves import range\n",
    "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
    "\n",
    "from tf_agents.agents.ddpg import actor_rnn_network\n",
    "from tf_agents.agents.ddpg import critic_rnn_network\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.environments import suite_dm_control\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import numpy \n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "from tf_agents.trajectories import TimeStep\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import sequential\n",
    "\n",
    "from RNN_Env import FREnv\n",
    "max_episode_length=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c799e852-cb44-467a-8163-36ba7e236a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=50, verbose=False):\n",
    "  total_return = 0.0\n",
    "  cullsteps = 0 \n",
    "  total_culls = 0\n",
    "  for e in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "        policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "    else:\n",
    "        #print(policy.get_initial_state(batch_size=train_env.batch_size()))\n",
    "        policy_state = policy.get_initial_state(batch_size=1) # other policies without memory\n",
    "    episode_return = 0.0\n",
    "    i=0\n",
    "    while not time_step.is_last():\n",
    "        i+=1\n",
    "        action_step = policy.action(time_step, policy_state)\n",
    "        if action_step.action[0][0] > 0 or action_step.action[0][1] > 0:\n",
    "            cullsteps += 1\n",
    "            total_culls += 1\n",
    "        elif action_step.action[0][0] > 0 and action_step.action[0][1] > 0:\n",
    "            total_culls += 2\n",
    "        policy_state = action_step.state\n",
    "        time_step = environment.step(action_step.action)\n",
    "        if isinstance(environment, FREnv):\n",
    "            state = environment.get_state()\n",
    "        else:\n",
    "            state = None # TF environment from wrapper does not have get_state()\n",
    "        episode_return += time_step.reward\n",
    "        if verbose:\n",
    "            print (f\"episode {e:>2} step{i:>4} action: \", action_step.action, \"state=\", state, \"obs=\", time_step.observation, \"reward=\", time_step.reward)\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  cullsteps /= num_episodes\n",
    "  total_culls /= num_episodes\n",
    "  cull_percent = cullsteps / total_culls\n",
    "  return avg_return, cullsteps, cull_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce0c5642-0e58-4bc2-873c-2c26ecff20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "@gin.configurable\n",
    "def train_eval(\n",
    "    root_dir,\n",
    "    env_name='cartpole',\n",
    "    task_name='balance',\n",
    "    observations_allowlist='position',\n",
    "    num_iterations=100000,\n",
    "    actor_fc_layers=(400, 300),\n",
    "    actor_output_fc_layers=(100,),\n",
    "    actor_lstm_size=(40,),\n",
    "    critic_obs_fc_layers=(400,),\n",
    "    critic_action_fc_layers=None,\n",
    "    critic_joint_fc_layers=(300,),\n",
    "    critic_output_fc_layers=(100,),\n",
    "    critic_lstm_size=(40,),\n",
    "    # Params for collect\n",
    "    initial_collect_episodes=1000,    #1\n",
    "    collect_episodes_per_iteration=5,    #1\n",
    "    replay_buffer_capacity=100000,\n",
    "    ou_stddev=0.2,\n",
    "    ou_damping=0.15,\n",
    "    # Params for target update\n",
    "    target_update_tau=0.05,\n",
    "    target_update_period=5,\n",
    "    # Params for train\n",
    "    # Params for train\n",
    "    train_steps_per_iteration=200,    #200\n",
    "    batch_size=64,\n",
    "    train_sequence_length=100,    #10\n",
    "    actor_learning_rate=1e-4,\n",
    "    critic_learning_rate=1e-3,\n",
    "    dqda_clipping=None,\n",
    "    td_errors_loss_fn=None,\n",
    "    gamma=0.99,    #.995\n",
    "    reward_scale_factor=1.0,\n",
    "    gradient_clipping=None,\n",
    "    use_tf_functions=True,\n",
    "    # Params for eval\n",
    "    num_eval_episodes=50,    #10\n",
    "    eval_interval=2000,    #1000\n",
    "    # Params for checkpoints, summaries, and logging\n",
    "    log_interval=1000,\n",
    "    summary_interval=1000,\n",
    "    summaries_flush_secs=10,\n",
    "    debug_summaries=True,\n",
    "    summarize_grads_and_vars=True,\n",
    "    eval_metrics_callback=None):\n",
    "\n",
    "  \"\"\"A simple train and eval for DDPG.\"\"\"\n",
    "  root_dir = os.path.expanduser(root_dir)\n",
    "  train_dir = os.path.join(root_dir, 'train')\n",
    "  eval_dir = os.path.join(root_dir, 'eval')\n",
    "\n",
    "  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "      train_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "  train_summary_writer.set_as_default()\n",
    "\n",
    "  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "      eval_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "  eval_metrics = [\n",
    "      tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n",
    "      tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n",
    "  ]\n",
    "\n",
    "  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "  with tf.compat.v2.summary.record_if(\n",
    "      lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
    "    if observations_allowlist is not None:\n",
    "      env_wrappers = [\n",
    "          functools.partial(\n",
    "              wrappers.FlattenObservationsWrapper,\n",
    "              observations_allowlist=[observations_allowlist])\n",
    "      ]\n",
    "    else:\n",
    "      env_wrappers = []\n",
    "\n",
    "    tf_env = tf_py_environment.TFPyEnvironment(\n",
    "        FREnv(herd_sizes = [32,32], expected_episode_length=100, max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.02, rand_infection_prob = 0.06))\n",
    "    eval_tf_env = tf_py_environment.TFPyEnvironment(\n",
    "        FREnv(herd_sizes = [32,32], expected_episode_length=100, max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.02, rand_infection_prob = 0.06))\n",
    "\n",
    "    actor_net = actor_rnn_network.ActorRnnNetwork(\n",
    "        tf_env.time_step_spec().observation,\n",
    "        tf_env.action_spec(),\n",
    "        input_fc_layer_params=actor_fc_layers,\n",
    "        lstm_size=actor_lstm_size,\n",
    "        output_fc_layer_params=actor_output_fc_layers)\n",
    "\n",
    "    critic_net_input_specs = (tf_env.time_step_spec().observation,\n",
    "                              tf_env.action_spec())\n",
    "\n",
    "    critic_net = critic_rnn_network.CriticRnnNetwork(\n",
    "        critic_net_input_specs,\n",
    "        observation_fc_layer_params=critic_obs_fc_layers,\n",
    "        action_fc_layer_params=critic_action_fc_layers,\n",
    "        joint_fc_layer_params=critic_joint_fc_layers,\n",
    "        lstm_size=critic_lstm_size,\n",
    "        output_fc_layer_params=critic_output_fc_layers,\n",
    "    )\n",
    "\n",
    "    tf_agent = ddpg_agent.DdpgAgent(\n",
    "        tf_env.time_step_spec(),\n",
    "        tf_env.action_spec(),\n",
    "        actor_network=actor_net,\n",
    "        critic_network=critic_net,\n",
    "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=actor_learning_rate),\n",
    "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=critic_learning_rate),\n",
    "        ou_stddev=ou_stddev,\n",
    "        ou_damping=ou_damping,\n",
    "        target_update_tau=target_update_tau,\n",
    "        target_update_period=target_update_period,\n",
    "        dqda_clipping=dqda_clipping,\n",
    "        td_errors_loss_fn=td_errors_loss_fn,\n",
    "        gamma=gamma,\n",
    "        reward_scale_factor=reward_scale_factor,\n",
    "        gradient_clipping=gradient_clipping,\n",
    "        debug_summaries=debug_summaries,\n",
    "        summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "        train_step_counter=global_step)\n",
    "    tf_agent.initialize()\n",
    "\n",
    "    train_metrics = [\n",
    "        tf_metrics.NumberOfEpisodes(),\n",
    "        tf_metrics.EnvironmentSteps(),\n",
    "        tf_metrics.AverageReturnMetric(),\n",
    "        tf_metrics.AverageEpisodeLengthMetric(),\n",
    "    ]\n",
    "\n",
    "    eval_policy = tf_agent.policy\n",
    "    collect_policy = tf_agent.collect_policy\n",
    "\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        tf_agent.collect_data_spec,\n",
    "        batch_size=tf_env.batch_size,\n",
    "        max_length=replay_buffer_capacity)\n",
    "\n",
    "    initial_collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "        tf_env,\n",
    "        collect_policy,\n",
    "        observers=[replay_buffer.add_batch] + train_metrics,\n",
    "        num_episodes=initial_collect_episodes)\n",
    "\n",
    "    collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "        tf_env,\n",
    "        collect_policy,\n",
    "        observers=[replay_buffer.add_batch] + train_metrics,\n",
    "        num_episodes=collect_episodes_per_iteration)\n",
    "\n",
    "    if use_tf_functions:\n",
    "      initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "      collect_driver.run = common.function(collect_driver.run)\n",
    "      tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "    # Collect initial replay data.\n",
    "    logging.info(\n",
    "        'Initializing replay buffer by collecting experience for %d episodes '\n",
    "        'with a random policy.', initial_collect_episodes)\n",
    "    initial_collect_driver.run()\n",
    "\n",
    "    results = metric_utils.eager_compute(\n",
    "        eval_metrics,\n",
    "        eval_tf_env,\n",
    "        eval_policy,\n",
    "        num_episodes=num_eval_episodes,\n",
    "        train_step=global_step,\n",
    "        summary_writer=eval_summary_writer,\n",
    "        summary_prefix='Metrics',\n",
    "    )\n",
    "    if eval_metrics_callback is not None:\n",
    "      eval_metrics_callback(results, global_step.numpy())\n",
    "    metric_utils.log_metrics(eval_metrics)\n",
    "\n",
    "    time_step = None\n",
    "    policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n",
    "\n",
    "    timed_at_step = global_step.numpy()\n",
    "    time_acc = 0\n",
    "\n",
    "    # Dataset generates trajectories with shape [BxTx...]\n",
    "    dataset = replay_buffer.as_dataset(\n",
    "        num_parallel_calls=3,\n",
    "        sample_batch_size=batch_size,\n",
    "        num_steps=train_sequence_length + 1).prefetch(3)\n",
    "    iterator = iter(dataset)\n",
    "\n",
    "    def train_step():\n",
    "      experience, _ = next(iterator)\n",
    "      return tf_agent.train(experience)\n",
    "\n",
    "    if use_tf_functions:\n",
    "      train_step = common.function(train_step)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "      start_time = time.time()\n",
    "      time_step, policy_state = collect_driver.run(\n",
    "          time_step=time_step,\n",
    "          policy_state=policy_state,\n",
    "      )\n",
    "      for _ in range(train_steps_per_iteration):\n",
    "        train_loss = train_step()\n",
    "      time_acc += time.time() - start_time\n",
    "\n",
    "      if global_step.numpy() % log_interval == 0:\n",
    "        logging.info('step = %d, loss = %f', global_step.numpy(),\n",
    "                     train_loss.loss)\n",
    "        steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n",
    "        logging.info('%.3f steps/sec', steps_per_sec)\n",
    "        tf.compat.v2.summary.scalar(\n",
    "            name='global_steps_per_sec', data=steps_per_sec, step=global_step)\n",
    "        timed_at_step = global_step.numpy()\n",
    "        time_acc = 0\n",
    "\n",
    "      for train_metric in train_metrics:\n",
    "        train_metric.tf_summaries(\n",
    "            train_step=global_step, step_metrics=train_metrics[:2])\n",
    "\n",
    "      if global_step.numpy() % eval_interval == 0:\n",
    "        results = metric_utils.eager_compute(\n",
    "            eval_metrics,\n",
    "            eval_tf_env,\n",
    "            eval_policy,\n",
    "            num_episodes=num_eval_episodes,\n",
    "            train_step=global_step,\n",
    "            summary_writer=eval_summary_writer,\n",
    "            summary_prefix='Metrics',\n",
    "        )\n",
    "        if eval_metrics_callback is not None:\n",
    "          eval_metrics_callback(results, global_step.numpy())\n",
    "        metric_utils.log_metrics(eval_metrics)\n",
    "        avg_return, cullsteps, cull_percent = compute_avg_return(eval_tf_env, eval_policy, num_episodes=50, verbose=False)\n",
    "        print('step {0}: average return = {1:.1f} cullsteps = {2:.1f} cull_percent = {3:.1f}'.format(global_step.numpy(), avg_return.numpy().item(), cullsteps, cull_percent))\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c098712-8656-4607-9014-a1a1d4710bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 8 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x7eff48442550>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 8 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x7eff48442550>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: average return = -7574.0 cullsteps = 86.1\n",
      "step 2000: average return = -12260.4 cullsteps = 83.3\n",
      "step 3000: average return = -14480.0 cullsteps = 101.6\n",
      "step 4000: average return = -15942.9 cullsteps = 123.9\n",
      "step 5000: average return = -7728.7 cullsteps = 90.2\n",
      "step 6000: average return = -5202.9 cullsteps = 78.1\n",
      "step 7000: average return = -9280.4 cullsteps = 90.0\n",
      "step 8000: average return = -7811.9 cullsteps = 101.3\n",
      "step 9000: average return = -7811.9 cullsteps = 75.5\n",
      "step 10000: average return = -7622.1 cullsteps = 101.8\n",
      "step 11000: average return = -5164.6 cullsteps = 77.2\n",
      "step 12000: average return = -6565.5 cullsteps = 77.6\n",
      "step 13000: average return = -7513.6 cullsteps = 95.8\n",
      "step 14000: average return = -7526.2 cullsteps = 89.0\n",
      "step 15000: average return = -5015.9 cullsteps = 80.3\n",
      "step 16000: average return = -4526.5 cullsteps = 78.2\n",
      "step 17000: average return = -7287.3 cullsteps = 101.2\n",
      "step 18000: average return = -5802.4 cullsteps = 80.1\n",
      "step 19000: average return = -7024.7 cullsteps = 88.8\n",
      "step 20000: average return = -9630.5 cullsteps = 122.4\n",
      "step 21000: average return = -6994.0 cullsteps = 89.8\n",
      "step 22000: average return = -9660.6 cullsteps = 118.7\n",
      "step 23000: average return = -9192.4 cullsteps = 110.1\n",
      "step 24000: average return = -5677.1 cullsteps = 90.4\n",
      "step 25000: average return = -3496.8 cullsteps = 72.1\n",
      "step 26000: average return = -14057.7 cullsteps = 117.1\n",
      "step 27000: average return = -9711.4 cullsteps = 105.9\n",
      "step 28000: average return = -5867.4 cullsteps = 96.4\n",
      "step 29000: average return = -6255.0 cullsteps = 86.4\n",
      "step 30000: average return = -9056.9 cullsteps = 98.5\n",
      "step 31000: average return = -10861.2 cullsteps = 118.4\n",
      "step 32000: average return = -7775.1 cullsteps = 97.2\n",
      "step 33000: average return = -6764.5 cullsteps = 74.5\n",
      "step 34000: average return = -11428.2 cullsteps = 111.5\n",
      "step 35000: average return = -6203.1 cullsteps = 80.7\n",
      "step 36000: average return = -6513.6 cullsteps = 88.3\n",
      "step 37000: average return = -10911.3 cullsteps = 104.5\n",
      "step 38000: average return = -6609.1 cullsteps = 77.7\n",
      "step 39000: average return = -11632.8 cullsteps = 106.8\n",
      "step 40000: average return = -5678.7 cullsteps = 84.5\n",
      "step 41000: average return = -8537.5 cullsteps = 96.1\n",
      "step 42000: average return = -9935.5 cullsteps = 102.5\n",
      "step 43000: average return = -11435.8 cullsteps = 97.0\n",
      "step 44000: average return = -8753.7 cullsteps = 92.0\n",
      "step 45000: average return = -10531.3 cullsteps = 97.0\n",
      "step 46000: average return = -9785.2 cullsteps = 84.2\n",
      "step 47000: average return = -8969.3 cullsteps = 108.7\n",
      "step 48000: average return = -9708.1 cullsteps = 104.0\n",
      "step 49000: average return = -7475.5 cullsteps = 97.2\n",
      "step 50000: average return = -6448.8 cullsteps = 72.7\n",
      "step 51000: average return = -7097.9 cullsteps = 96.8\n",
      "step 52000: average return = -9034.4 cullsteps = 113.9\n",
      "step 53000: average return = -7304.5 cullsteps = 88.8\n",
      "step 54000: average return = -6607.8 cullsteps = 91.0\n",
      "step 55000: average return = -8543.2 cullsteps = 113.3\n",
      "step 56000: average return = -6448.9 cullsteps = 97.6\n",
      "step 57000: average return = -7647.6 cullsteps = 86.7\n",
      "step 58000: average return = -8014.7 cullsteps = 103.7\n",
      "step 59000: average return = -12532.0 cullsteps = 108.5\n",
      "step 60000: average return = -5547.9 cullsteps = 90.5\n",
      "step 61000: average return = -9191.1 cullsteps = 117.2\n",
      "step 62000: average return = -5809.2 cullsteps = 95.9\n",
      "step 63000: average return = -6350.2 cullsteps = 91.5\n",
      "step 64000: average return = -8345.5 cullsteps = 117.1\n",
      "step 65000: average return = -3630.3 cullsteps = 72.6\n",
      "step 66000: average return = -5986.4 cullsteps = 88.1\n",
      "step 67000: average return = -9417.3 cullsteps = 116.9\n",
      "step 68000: average return = -5644.5 cullsteps = 84.4\n",
      "step 69000: average return = -6913.8 cullsteps = 88.0\n",
      "step 70000: average return = -5998.6 cullsteps = 97.2\n",
      "step 71000: average return = -8945.3 cullsteps = 95.9\n",
      "step 72000: average return = -5144.7 cullsteps = 86.1\n",
      "step 73000: average return = -5683.7 cullsteps = 99.1\n",
      "step 74000: average return = -7895.8 cullsteps = 100.9\n",
      "step 75000: average return = -5585.2 cullsteps = 95.5\n",
      "step 76000: average return = -6040.1 cullsteps = 103.0\n",
      "step 77000: average return = -4938.9 cullsteps = 87.1\n",
      "step 78000: average return = -4956.0 cullsteps = 85.8\n",
      "step 79000: average return = -7377.3 cullsteps = 86.0\n",
      "step 80000: average return = -9235.1 cullsteps = 115.9\n",
      "step 81000: average return = -10091.7 cullsteps = 112.9\n",
      "step 82000: average return = -9721.2 cullsteps = 111.6\n",
      "step 83000: average return = -9674.5 cullsteps = 102.1\n",
      "step 84000: average return = -5861.4 cullsteps = 88.2\n",
      "step 85000: average return = -12278.9 cullsteps = 141.0\n",
      "step 86000: average return = -5347.9 cullsteps = 92.2\n",
      "step 87000: average return = -6392.6 cullsteps = 99.7\n",
      "step 88000: average return = -4239.8 cullsteps = 80.1\n",
      "step 89000: average return = -7201.7 cullsteps = 115.7\n",
      "step 90000: average return = -11635.7 cullsteps = 122.9\n",
      "step 91000: average return = -6344.4 cullsteps = 92.8\n",
      "step 92000: average return = -9733.3 cullsteps = 110.4\n",
      "step 93000: average return = -3734.0 cullsteps = 70.2\n",
      "step 94000: average return = -7025.4 cullsteps = 87.0\n",
      "step 95000: average return = -4218.9 cullsteps = 80.6\n",
      "step 96000: average return = -4676.9 cullsteps = 83.3\n",
      "step 97000: average return = -11315.4 cullsteps = 124.3\n",
      "step 98000: average return = -12680.3 cullsteps = 121.7\n",
      "step 99000: average return = -4351.6 cullsteps = 85.6\n",
      "step 100000: average return = -3728.2 cullsteps = 81.4\n",
      "step 101000: average return = -7726.0 cullsteps = 94.9\n",
      "step 102000: average return = -7996.2 cullsteps = 115.2\n",
      "step 103000: average return = -6275.4 cullsteps = 97.1\n",
      "step 104000: average return = -4574.6 cullsteps = 85.9\n",
      "step 105000: average return = -3451.4 cullsteps = 74.7\n",
      "step 106000: average return = -2732.1 cullsteps = 58.4\n",
      "step 107000: average return = -12310.5 cullsteps = 118.3\n",
      "step 108000: average return = -7486.0 cullsteps = 94.4\n",
      "step 109000: average return = -10092.0 cullsteps = 102.7\n",
      "step 110000: average return = -5178.8 cullsteps = 105.5\n",
      "step 111000: average return = -2034.3 cullsteps = 58.1\n",
      "step 112000: average return = -6528.3 cullsteps = 95.1\n",
      "step 113000: average return = -7031.4 cullsteps = 106.2\n",
      "step 114000: average return = -5395.2 cullsteps = 101.1\n",
      "step 115000: average return = -4227.7 cullsteps = 76.1\n",
      "step 116000: average return = -8109.4 cullsteps = 107.1\n",
      "step 117000: average return = -6901.0 cullsteps = 120.2\n",
      "step 118000: average return = -3937.5 cullsteps = 77.2\n",
      "step 119000: average return = -4902.0 cullsteps = 79.3\n",
      "step 120000: average return = -5969.7 cullsteps = 88.3\n",
      "step 121000: average return = -4506.9 cullsteps = 99.8\n",
      "step 122000: average return = -5991.6 cullsteps = 94.6\n",
      "step 123000: average return = -10182.1 cullsteps = 125.0\n",
      "step 124000: average return = -4620.5 cullsteps = 85.0\n",
      "step 125000: average return = -3998.6 cullsteps = 84.6\n",
      "step 126000: average return = -3985.9 cullsteps = 85.3\n",
      "step 127000: average return = -4290.9 cullsteps = 83.9\n",
      "step 128000: average return = -4438.0 cullsteps = 84.4\n",
      "step 129000: average return = -7796.4 cullsteps = 99.9\n",
      "step 130000: average return = -8206.2 cullsteps = 105.0\n",
      "step 131000: average return = -7298.3 cullsteps = 110.5\n",
      "step 132000: average return = -6148.6 cullsteps = 93.6\n",
      "step 133000: average return = -5124.4 cullsteps = 76.0\n",
      "step 134000: average return = -3662.0 cullsteps = 71.7\n",
      "step 135000: average return = -4426.6 cullsteps = 80.2\n",
      "step 136000: average return = -3524.3 cullsteps = 71.3\n",
      "step 137000: average return = -7774.0 cullsteps = 109.0\n",
      "step 138000: average return = -5456.4 cullsteps = 78.9\n",
      "step 139000: average return = -5553.0 cullsteps = 81.5\n",
      "step 140000: average return = -4162.1 cullsteps = 83.1\n",
      "step 141000: average return = -3702.7 cullsteps = 71.8\n",
      "step 142000: average return = -6083.7 cullsteps = 86.9\n",
      "step 143000: average return = -5221.4 cullsteps = 81.5\n",
      "step 144000: average return = -5349.5 cullsteps = 91.2\n",
      "step 145000: average return = -6434.2 cullsteps = 88.0\n",
      "step 146000: average return = -4655.2 cullsteps = 82.2\n",
      "step 147000: average return = -3800.2 cullsteps = 71.5\n",
      "step 148000: average return = -6123.0 cullsteps = 90.7\n",
      "step 149000: average return = -5221.6 cullsteps = 81.8\n",
      "step 150000: average return = -4272.1 cullsteps = 76.2\n",
      "step 151000: average return = -7427.5 cullsteps = 86.9\n",
      "step 152000: average return = -4624.7 cullsteps = 71.0\n",
      "step 153000: average return = -6423.7 cullsteps = 84.0\n",
      "step 154000: average return = -7370.6 cullsteps = 90.4\n",
      "step 155000: average return = -6110.0 cullsteps = 75.5\n",
      "step 156000: average return = -4100.8 cullsteps = 74.8\n",
      "step 157000: average return = -4768.3 cullsteps = 69.1\n",
      "step 158000: average return = -5200.7 cullsteps = 77.2\n",
      "step 159000: average return = -5024.0 cullsteps = 69.1\n",
      "step 160000: average return = -3893.8 cullsteps = 70.6\n",
      "step 161000: average return = -4692.5 cullsteps = 73.5\n",
      "step 162000: average return = -4380.9 cullsteps = 84.2\n",
      "step 163000: average return = -7669.8 cullsteps = 105.7\n",
      "step 164000: average return = -5294.7 cullsteps = 89.6\n",
      "step 165000: average return = -7755.9 cullsteps = 114.9\n",
      "step 166000: average return = -5042.2 cullsteps = 87.6\n",
      "step 167000: average return = -4913.5 cullsteps = 100.3\n",
      "step 168000: average return = -3934.9 cullsteps = 88.1\n",
      "step 169000: average return = -7221.4 cullsteps = 100.4\n",
      "step 170000: average return = -9262.4 cullsteps = 79.0\n",
      "step 171000: average return = -5170.3 cullsteps = 90.8\n",
      "step 172000: average return = -4840.1 cullsteps = 96.4\n",
      "step 173000: average return = -7202.5 cullsteps = 86.8\n",
      "step 174000: average return = -4661.8 cullsteps = 62.7\n",
      "step 175000: average return = -7715.2 cullsteps = 81.2\n",
      "step 176000: average return = -3996.5 cullsteps = 64.0\n",
      "step 177000: average return = -6409.4 cullsteps = 83.8\n",
      "step 178000: average return = -5511.7 cullsteps = 75.1\n",
      "step 179000: average return = -3974.6 cullsteps = 60.1\n",
      "step 180000: average return = -6467.9 cullsteps = 81.4\n",
      "step 181000: average return = -6158.8 cullsteps = 87.2\n",
      "step 182000: average return = -4200.9 cullsteps = 66.2\n",
      "step 183000: average return = -5237.9 cullsteps = 68.2\n",
      "step 184000: average return = -4721.3 cullsteps = 72.9\n",
      "step 185000: average return = -7419.4 cullsteps = 103.8\n",
      "step 186000: average return = -8517.9 cullsteps = 103.1\n",
      "step 187000: average return = -5230.4 cullsteps = 82.4\n",
      "step 188000: average return = -5908.9 cullsteps = 86.6\n",
      "step 189000: average return = -5596.3 cullsteps = 76.5\n",
      "step 190000: average return = -5425.6 cullsteps = 83.2\n",
      "step 191000: average return = -6432.7 cullsteps = 97.2\n",
      "step 192000: average return = -5068.8 cullsteps = 68.6\n",
      "step 193000: average return = -3699.3 cullsteps = 63.6\n",
      "step 194000: average return = -6381.2 cullsteps = 76.3\n",
      "step 195000: average return = -7236.6 cullsteps = 78.2\n",
      "step 196000: average return = -11658.4 cullsteps = 109.5\n",
      "step 197000: average return = -3390.8 cullsteps = 57.3\n",
      "step 198000: average return = -6136.4 cullsteps = 79.6\n",
      "step 199000: average return = -5761.9 cullsteps = 70.5\n",
      "step 200000: average return = -4758.3 cullsteps = 75.6\n",
      "step 201000: average return = -4055.8 cullsteps = 57.5\n",
      "step 202000: average return = -5422.1 cullsteps = 67.4\n",
      "step 203000: average return = -3603.1 cullsteps = 53.0\n",
      "step 204000: average return = -4972.1 cullsteps = 66.1\n",
      "step 205000: average return = -3640.7 cullsteps = 64.4\n",
      "step 206000: average return = -3246.9 cullsteps = 60.5\n",
      "step 207000: average return = -5063.1 cullsteps = 75.4\n",
      "step 208000: average return = -3671.9 cullsteps = 63.0\n",
      "step 209000: average return = -4927.0 cullsteps = 77.8\n",
      "step 210000: average return = -6437.0 cullsteps = 85.9\n",
      "step 211000: average return = -4067.1 cullsteps = 68.2\n",
      "step 212000: average return = -7719.6 cullsteps = 82.8\n",
      "step 213000: average return = -8672.3 cullsteps = 108.4\n",
      "step 214000: average return = -5755.3 cullsteps = 83.9\n",
      "step 215000: average return = -6329.1 cullsteps = 90.8\n",
      "step 216000: average return = -3172.7 cullsteps = 72.1\n",
      "step 217000: average return = -4955.6 cullsteps = 84.4\n",
      "step 218000: average return = -3537.3 cullsteps = 78.2\n",
      "step 219000: average return = -4807.3 cullsteps = 91.2\n",
      "step 220000: average return = -4576.6 cullsteps = 79.5\n",
      "step 221000: average return = -3724.2 cullsteps = 69.3\n",
      "step 222000: average return = -6669.8 cullsteps = 92.3\n",
      "step 223000: average return = -4663.1 cullsteps = 78.1\n",
      "step 224000: average return = -3656.6 cullsteps = 63.9\n",
      "step 225000: average return = -3718.1 cullsteps = 72.9\n",
      "step 226000: average return = -7015.7 cullsteps = 86.9\n",
      "step 227000: average return = -12226.8 cullsteps = 124.8\n",
      "step 228000: average return = -4839.5 cullsteps = 83.9\n",
      "step 229000: average return = -3443.4 cullsteps = 61.5\n",
      "step 230000: average return = -3760.9 cullsteps = 60.5\n",
      "step 231000: average return = -5848.1 cullsteps = 100.4\n",
      "step 232000: average return = -4549.6 cullsteps = 74.5\n",
      "step 233000: average return = -3043.7 cullsteps = 65.2\n",
      "step 234000: average return = -5920.8 cullsteps = 85.8\n",
      "step 235000: average return = -4578.6 cullsteps = 90.7\n",
      "step 236000: average return = -3477.7 cullsteps = 74.3\n",
      "step 237000: average return = -4669.0 cullsteps = 76.8\n",
      "step 238000: average return = -5776.7 cullsteps = 82.7\n",
      "step 239000: average return = -5990.5 cullsteps = 98.7\n",
      "step 240000: average return = -5114.2 cullsteps = 90.2\n",
      "step 241000: average return = -4845.9 cullsteps = 82.1\n",
      "step 242000: average return = -6280.9 cullsteps = 96.8\n",
      "step 243000: average return = -8678.1 cullsteps = 103.1\n",
      "step 244000: average return = -5570.2 cullsteps = 92.0\n",
      "step 245000: average return = -5847.1 cullsteps = 92.0\n",
      "step 246000: average return = -3937.0 cullsteps = 73.6\n",
      "step 247000: average return = -2512.4 cullsteps = 54.5\n",
      "step 248000: average return = -5555.0 cullsteps = 87.6\n",
      "step 249000: average return = -5961.8 cullsteps = 102.6\n",
      "step 250000: average return = -4371.8 cullsteps = 76.1\n",
      "step 251000: average return = -6806.8 cullsteps = 99.7\n",
      "step 252000: average return = -5785.4 cullsteps = 86.0\n",
      "step 253000: average return = -4392.6 cullsteps = 76.8\n",
      "step 254000: average return = -4579.0 cullsteps = 72.1\n",
      "step 255000: average return = -7251.3 cullsteps = 96.9\n",
      "step 256000: average return = -6005.4 cullsteps = 92.7\n",
      "step 257000: average return = -4790.1 cullsteps = 80.1\n",
      "step 258000: average return = -3035.7 cullsteps = 65.9\n",
      "step 259000: average return = -5387.1 cullsteps = 101.9\n",
      "step 260000: average return = -3235.7 cullsteps = 62.4\n",
      "step 261000: average return = -4972.1 cullsteps = 77.4\n",
      "step 262000: average return = -4816.1 cullsteps = 90.2\n",
      "step 263000: average return = -5981.9 cullsteps = 79.0\n",
      "step 264000: average return = -8356.4 cullsteps = 118.4\n",
      "step 265000: average return = -6080.6 cullsteps = 98.0\n",
      "step 266000: average return = -5030.3 cullsteps = 89.3\n",
      "step 267000: average return = -2980.0 cullsteps = 60.2\n",
      "step 268000: average return = -5138.3 cullsteps = 88.7\n",
      "step 269000: average return = -3613.4 cullsteps = 75.4\n",
      "step 270000: average return = -5081.2 cullsteps = 80.7\n",
      "step 271000: average return = -3863.1 cullsteps = 72.1\n",
      "step 272000: average return = -4276.1 cullsteps = 80.1\n",
      "step 273000: average return = -5016.5 cullsteps = 87.0\n",
      "step 274000: average return = -3565.6 cullsteps = 72.7\n",
      "step 275000: average return = -3885.4 cullsteps = 73.0\n",
      "step 276000: average return = -4310.4 cullsteps = 75.7\n",
      "step 277000: average return = -4663.3 cullsteps = 82.7\n",
      "step 278000: average return = -9077.5 cullsteps = 117.8\n",
      "step 279000: average return = -5053.4 cullsteps = 80.3\n",
      "step 280000: average return = -5022.6 cullsteps = 84.8\n",
      "step 281000: average return = -4785.0 cullsteps = 78.6\n",
      "step 282000: average return = -3553.8 cullsteps = 68.0\n",
      "step 283000: average return = -4904.3 cullsteps = 91.5\n",
      "step 284000: average return = -5158.9 cullsteps = 72.5\n",
      "step 285000: average return = -6184.6 cullsteps = 94.0\n",
      "step 286000: average return = -4865.2 cullsteps = 79.6\n",
      "step 287000: average return = -4242.2 cullsteps = 77.2\n",
      "step 288000: average return = -5077.6 cullsteps = 84.9\n",
      "step 289000: average return = -5866.9 cullsteps = 84.6\n",
      "step 290000: average return = -4147.0 cullsteps = 74.6\n",
      "step 291000: average return = -5142.5 cullsteps = 82.1\n",
      "step 292000: average return = -3714.1 cullsteps = 71.4\n",
      "step 293000: average return = -5999.9 cullsteps = 103.7\n",
      "step 294000: average return = -4549.7 cullsteps = 85.4\n",
      "step 295000: average return = -3259.3 cullsteps = 74.2\n",
      "step 296000: average return = -5938.7 cullsteps = 99.7\n",
      "step 297000: average return = -5203.2 cullsteps = 100.8\n",
      "step 298000: average return = -4169.9 cullsteps = 79.8\n",
      "step 299000: average return = -4459.1 cullsteps = 78.5\n",
      "step 300000: average return = -4868.2 cullsteps = 86.2\n",
      "step 301000: average return = -4703.4 cullsteps = 76.9\n",
      "step 302000: average return = -4270.1 cullsteps = 75.2\n",
      "step 303000: average return = -5827.8 cullsteps = 95.6\n",
      "step 304000: average return = -6523.6 cullsteps = 102.2\n",
      "step 305000: average return = -7644.2 cullsteps = 106.8\n",
      "step 306000: average return = -4935.4 cullsteps = 72.3\n",
      "step 307000: average return = -4124.7 cullsteps = 76.4\n",
      "step 308000: average return = -3731.4 cullsteps = 74.9\n",
      "step 309000: average return = -9409.8 cullsteps = 108.5\n",
      "step 310000: average return = -7625.8 cullsteps = 96.6\n",
      "step 311000: average return = -5214.3 cullsteps = 75.8\n",
      "step 312000: average return = -5326.1 cullsteps = 80.9\n",
      "step 313000: average return = -6113.3 cullsteps = 86.2\n",
      "step 314000: average return = -8324.7 cullsteps = 103.3\n",
      "step 315000: average return = -5721.6 cullsteps = 83.1\n",
      "step 316000: average return = -5387.7 cullsteps = 83.6\n",
      "step 317000: average return = -7851.4 cullsteps = 98.5\n",
      "step 318000: average return = -7261.6 cullsteps = 102.6\n",
      "step 319000: average return = -4300.4 cullsteps = 67.9\n",
      "step 320000: average return = -3488.6 cullsteps = 69.1\n",
      "step 321000: average return = -10933.7 cullsteps = 108.9\n",
      "step 322000: average return = -4592.0 cullsteps = 73.9\n",
      "step 323000: average return = -4429.8 cullsteps = 79.3\n",
      "step 324000: average return = -6326.7 cullsteps = 97.8\n",
      "step 325000: average return = -3343.3 cullsteps = 67.7\n",
      "step 326000: average return = -7734.3 cullsteps = 102.9\n",
      "step 327000: average return = -4962.6 cullsteps = 88.2\n",
      "step 328000: average return = -8967.0 cullsteps = 118.4\n",
      "step 329000: average return = -5970.0 cullsteps = 101.6\n",
      "step 330000: average return = -5012.8 cullsteps = 80.3\n",
      "step 331000: average return = -4410.5 cullsteps = 66.7\n",
      "step 332000: average return = -5332.3 cullsteps = 96.0\n",
      "step 333000: average return = -6129.8 cullsteps = 99.5\n",
      "step 334000: average return = -6460.9 cullsteps = 106.0\n",
      "step 335000: average return = -5986.6 cullsteps = 95.3\n",
      "step 336000: average return = -3765.6 cullsteps = 84.3\n",
      "step 337000: average return = -3749.0 cullsteps = 83.6\n",
      "step 338000: average return = -5324.5 cullsteps = 80.5\n",
      "step 339000: average return = -7078.4 cullsteps = 105.2\n",
      "step 340000: average return = -4458.3 cullsteps = 91.3\n",
      "step 341000: average return = -4662.5 cullsteps = 73.7\n",
      "step 342000: average return = -5463.5 cullsteps = 76.4\n",
      "step 343000: average return = -7077.4 cullsteps = 83.0\n",
      "step 344000: average return = -3112.2 cullsteps = 57.9\n",
      "step 345000: average return = -6347.8 cullsteps = 82.0\n",
      "step 346000: average return = -10475.2 cullsteps = 118.3\n",
      "step 347000: average return = -6120.8 cullsteps = 101.1\n",
      "step 348000: average return = -5416.0 cullsteps = 92.3\n",
      "step 349000: average return = -7064.2 cullsteps = 121.6\n",
      "step 350000: average return = -10508.7 cullsteps = 115.2\n",
      "step 351000: average return = -5545.2 cullsteps = 77.9\n",
      "step 352000: average return = -5993.0 cullsteps = 84.5\n",
      "step 353000: average return = -4940.9 cullsteps = 80.3\n",
      "step 354000: average return = -4823.9 cullsteps = 74.8\n",
      "step 355000: average return = -9048.6 cullsteps = 108.1\n",
      "step 356000: average return = -6714.4 cullsteps = 87.9\n",
      "step 357000: average return = -5080.3 cullsteps = 66.6\n",
      "step 358000: average return = -5294.5 cullsteps = 75.4\n",
      "step 359000: average return = -6695.9 cullsteps = 86.7\n",
      "step 360000: average return = -6715.0 cullsteps = 80.1\n",
      "step 361000: average return = -8115.7 cullsteps = 103.9\n",
      "step 362000: average return = -3978.7 cullsteps = 66.3\n",
      "step 363000: average return = -6075.8 cullsteps = 103.0\n",
      "step 364000: average return = -8307.2 cullsteps = 104.7\n",
      "step 365000: average return = -4790.8 cullsteps = 83.8\n",
      "step 366000: average return = -4806.4 cullsteps = 77.7\n",
      "step 367000: average return = -6552.3 cullsteps = 73.8\n",
      "step 368000: average return = -6074.6 cullsteps = 97.0\n",
      "step 369000: average return = -5470.8 cullsteps = 80.9\n",
      "step 370000: average return = -5805.0 cullsteps = 87.9\n",
      "step 371000: average return = -6458.7 cullsteps = 101.5\n",
      "step 372000: average return = -5602.6 cullsteps = 85.3\n",
      "step 373000: average return = -5990.1 cullsteps = 87.8\n",
      "step 374000: average return = -4356.0 cullsteps = 69.9\n",
      "step 375000: average return = -3783.9 cullsteps = 57.1\n",
      "step 376000: average return = -16087.2 cullsteps = 123.2\n",
      "step 377000: average return = -5991.2 cullsteps = 83.5\n",
      "step 378000: average return = -6357.2 cullsteps = 87.4\n",
      "step 379000: average return = -6539.6 cullsteps = 86.7\n",
      "step 380000: average return = -2942.7 cullsteps = 58.4\n",
      "step 381000: average return = -9343.5 cullsteps = 101.7\n",
      "step 382000: average return = -5847.2 cullsteps = 91.9\n",
      "step 383000: average return = -5466.0 cullsteps = 77.2\n",
      "step 384000: average return = -6865.2 cullsteps = 89.2\n",
      "step 385000: average return = -7227.4 cullsteps = 76.2\n",
      "step 386000: average return = -4938.0 cullsteps = 72.6\n",
      "step 387000: average return = -3323.0 cullsteps = 64.2\n",
      "step 388000: average return = -6474.2 cullsteps = 82.4\n",
      "step 389000: average return = -9164.0 cullsteps = 106.1\n",
      "step 390000: average return = -6257.8 cullsteps = 98.9\n",
      "step 391000: average return = -5789.0 cullsteps = 83.1\n",
      "step 392000: average return = -5715.2 cullsteps = 92.3\n",
      "step 393000: average return = -7469.5 cullsteps = 106.8\n",
      "step 394000: average return = -5942.1 cullsteps = 98.9\n",
      "step 395000: average return = -6111.2 cullsteps = 91.4\n",
      "step 396000: average return = -6725.4 cullsteps = 105.6\n",
      "step 397000: average return = -5448.8 cullsteps = 99.5\n",
      "step 398000: average return = -6942.4 cullsteps = 100.9\n",
      "step 399000: average return = -4926.8 cullsteps = 86.4\n",
      "step 400000: average return = -6744.0 cullsteps = 100.1\n",
      "step 401000: average return = -4797.9 cullsteps = 85.9\n",
      "step 402000: average return = -11076.9 cullsteps = 112.5\n",
      "step 403000: average return = -5175.6 cullsteps = 90.4\n",
      "step 404000: average return = -6199.8 cullsteps = 95.6\n",
      "step 405000: average return = -5555.8 cullsteps = 92.9\n",
      "step 406000: average return = -3665.9 cullsteps = 79.2\n",
      "step 407000: average return = -2776.2 cullsteps = 68.4\n",
      "step 408000: average return = -5792.2 cullsteps = 112.5\n",
      "step 409000: average return = -6452.4 cullsteps = 111.5\n",
      "step 410000: average return = -4685.5 cullsteps = 93.7\n",
      "step 411000: average return = -4910.7 cullsteps = 89.4\n",
      "step 412000: average return = -4200.0 cullsteps = 81.9\n",
      "step 413000: average return = -3887.1 cullsteps = 72.2\n",
      "step 414000: average return = -6388.6 cullsteps = 107.1\n",
      "step 415000: average return = -5513.5 cullsteps = 100.7\n",
      "step 416000: average return = -3583.7 cullsteps = 81.6\n",
      "step 417000: average return = -6167.9 cullsteps = 100.4\n",
      "step 418000: average return = -2918.1 cullsteps = 69.1\n",
      "step 419000: average return = -5276.4 cullsteps = 93.4\n",
      "step 420000: average return = -5851.4 cullsteps = 105.2\n",
      "step 421000: average return = -5534.8 cullsteps = 97.3\n",
      "step 422000: average return = -6699.8 cullsteps = 113.6\n",
      "step 423000: average return = -4724.4 cullsteps = 99.7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ea82a0703565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-9e8a8a92790a>\u001b[0m in \u001b[0;36mtrain_eval\u001b[0;34m(root_dir, env_name, task_name, observations_allowlist, num_iterations, actor_fc_layers, actor_output_fc_layers, actor_lstm_size, critic_obs_fc_layers, critic_action_fc_layers, critic_joint_fc_layers, critic_output_fc_layers, critic_lstm_size, initial_collect_episodes, collect_episodes_per_iteration, replay_buffer_capacity, ou_stddev, ou_damping, target_update_tau, target_update_period, train_steps_per_iteration, batch_size, train_sequence_length, actor_learning_rate, critic_learning_rate, dqda_clipping, td_errors_loss_fn, gamma, reward_scale_factor, gradient_clipping, use_tf_functions, num_eval_episodes, eval_interval, log_interval, summary_interval, summaries_flush_secs, debug_summaries, summarize_grads_and_vars, eval_metrics_callback)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         results = metric_utils.eager_compute(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0meval_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0meval_tf_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tf_agents/eval/metric_utils.py\u001b[0m in \u001b[0;36meager_compute\u001b[0;34m(metrics, environment, policy, num_episodes, train_step, summary_writer, summary_prefix, use_function)\u001b[0m\n\u001b[1;32m    161\u001b[0m       num_episodes=num_episodes)\n\u001b[1;32m    162\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0muse_function\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    954\u001b[0m               *args, **kwds)\n\u001b[1;32m    955\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m       return self._concrete_stateful_fn._call_flat(\n\u001b[0m\u001b[1;32m    957\u001b[0m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_eval(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
