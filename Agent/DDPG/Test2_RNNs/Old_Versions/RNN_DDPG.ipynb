{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the environment `HerdEnv` of `herd_env.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "from tf_agents.trajectories import TimeStep\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RNN_Env import FREnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "henv_val = FREnv(herd_sizes = [64,64], rand_recovery_prob = 0.1, rand_infection_prob = 0.05)\n",
    "utils.validate_py_environment(henv_val, episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Herd Environment instance to be trained for\n",
    "max_episode_length=1000\n",
    "num_herds=2\n",
    "henv = FREnv(herd_sizes = [32,32], expected_episode_length=100, max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.02, rand_infection_prob = 0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': 1.0,\n",
      " 'observation': array([0., 0., 0.], dtype=float32),\n",
      " 'reward': 0.0,\n",
      " 'step_type': array(0, dtype=int32)})\n",
      "state:  [2 0 2 0 1 1] observation:  [0.001 0.001 0.001] \treward:  -0.05\n",
      "state:  [3 2 5 2 2 2] observation:  [0.002 0.002 0.002] \treward:  -0.175\n",
      "state:  [3 3 8 5 3 3] observation:  [0.003 0.003 0.003] \treward:  -0.325\n",
      "state:  [ 5  6 13 11  4  4] observation:  [0.004 0.004 0.004] \treward:  -0.6\n",
      "state:  [ 9  8 22 19  5  5] observation:  [0.005 0.005 0.005] \treward:  -1.025\n",
      "state:  [ 9  8 31 27  6  6] observation:  [0.006 0.006 0.006] \treward:  -1.45\n",
      "state:  [12  7 43 34  7  7] observation:  [0.007 0.007 0.007] \treward:  -1.925\n",
      "state:  [13  8 56 42  8  8] observation:  [0.008 0.008 0.008] \treward:  -2.45\n",
      "state:  [14  9 70 51  9  9] observation:  [0.009 0.009 0.009] \treward:  -3.025\n",
      "state:  [16  9 86 60 10 10] observation:  [0.01 0.01 0.01] \treward:  -3.65\n",
      "state:  [ 17  10 103  70  11  11] observation:  [0.011 0.011 0.011] \treward:  -4.325\n",
      "state:  [ 18  10 121  80  12  12] observation:  [0.012 0.012 0.012] \treward:  -5.025\n",
      "state:  [ 18  11 139  91  13  13] observation:  [0.013 0.013 0.013] \treward:  -5.75\n",
      "state:  [ 20  11 159 102  14  14] observation:  [0.014 0.014 0.014] \treward:  -6.525\n",
      "state:  [ 20  13 179 115  15  15] observation:  [0.015 0.015 0.015] \treward:  -7.35\n",
      "state:  [ 20  16 199 131  16  16] observation:  [0.016 0.016 0.016] \treward:  -8.25\n",
      "state:  [ 20  18 219 149  17  17] observation:  [0.017 0.017 0.017] \treward:  -9.2\n",
      "state:  [ 21  18 240 167  18  18] observation:  [0.018 0.018 0.018] \treward:  -10.175\n",
      "state:  [ 21  17 261 184  19  19] observation:  [0.019 0.019 0.019] \treward:  -11.125\n",
      "state:  [ 22  18 283 202  20  20] observation:  [0.02 0.02 0.02] \treward:  -12.125\n",
      "state:  [ 22  16 305 218  21  21] observation:  [0.021 0.021 0.021] \treward:  -13.075\n",
      "state:  [ 22  16 327 234  22  22] observation:  [0.022 0.022 0.022] \treward:  -14.025\n",
      "state:  [ 22  16 349 250  23  23] observation:  [0.023 0.023 0.023] \treward:  -14.975\n",
      "state:  [ 22  17 371 267  24  24] observation:  [0.024 0.024 0.024] \treward:  -15.95\n",
      "state:  [ 22  17 393 284  25  25] observation:  [0.025 0.025 0.025] \treward:  -16.925\n",
      "state:  [ 21  16 414 300  26  26] observation:  [0.026 0.026 0.026] \treward:  -17.85\n",
      "state:  [ 22  16 436 316  27  27] observation:  [0.027 0.027 0.027] \treward:  -18.8\n",
      "state:  [ 23  16 459 332  28  28] observation:  [0.028 0.028 0.028] \treward:  -19.775\n",
      "state:  [ 24  16 483 348  29  29] observation:  [0.029 0.029 0.029] \treward:  -20.775\n",
      "state:  [ 24  16 507 364  30  30] observation:  [0.03 0.03 0.03] \treward:  -21.775\n",
      "state:  [ 23  15 530 379  31  31] observation:  [0.031 0.031 0.031] \treward:  -22.725\n",
      "state:  [ 24  19 554 398  32  32] observation:  [0.032 0.032 0.032] \treward:  -23.8\n",
      "state:  [ 24  20 578 418  33  33] observation:  [0.033 0.033 0.033] \treward:  -24.9\n",
      "state:  [ 24  22 602 440  34  34] observation:  [0.034 0.034 0.034] \treward:  -26.05\n",
      "state:  [ 24  24 626 464  35  35] observation:  [0.035 0.035 0.035] \treward:  -27.25\n",
      "state:  [ 23  25 649 489  36  36] observation:  [0.036 0.036 0.036] \treward:  -28.45\n",
      "state:  [ 23  26 672 515  37  37] observation:  [0.037 0.037 0.037] \treward:  -29.675\n",
      "state:  [ 23  25 695 540  38  38] observation:  [0.038 0.038 0.038] \treward:  -30.875\n",
      "state:  [ 23  24 718 564  39  39] observation:  [0.039 0.039 0.039] \treward:  -32.05\n",
      "state:  [ 23  24 741 588  40  40] observation:  [0.04 0.04 0.04] \treward:  -33.225\n",
      "state:  [ 24  24 765 612  41  41] observation:  [0.041 0.041 0.041] \treward:  -34.425\n",
      "state:  [ 26  24 791 636  42  42] observation:  [0.042 0.042 0.042] \treward:  -35.675\n",
      "state:  [ 24  24 815 660  43  43] observation:  [0.043 0.043 0.043] \treward:  -36.875\n",
      "state:  [ 24  25 839 685  44  44] observation:  [0.044 0.044 0.044] \treward:  -38.1\n",
      "state:  [ 25  26 864 711  45  45] observation:  [0.045 0.045 0.045] \treward:  -39.375\n",
      "state:  [ 25  27 889 738  46  46] observation:  [0.046 0.046 0.046] \treward:  -40.675\n",
      "state:  [ 26  27 915 765  47  47] observation:  [0.047 0.047 0.047] \treward:  -42.0\n",
      "state:  [ 26  30 941 795  48  48] observation:  [0.048 0.048 0.048] \treward:  -43.4\n",
      "state:  [ 28  30 969 825  49  49] observation:  [0.049 0.049 0.049] \treward:  -44.85\n",
      "state:  [ 27  30 996 855  50  50] observation:  [0.05 0.05 0.05] \treward:  -46.275\n",
      "state:  [  27   30 1023  885   51   51] observation:  [0.051 0.051 0.051] \treward:  -47.7\n",
      "state:  [  27   30 1050  915   52   52] observation:  [0.052 0.052 0.052] \treward:  -49.125\n",
      "state:  [  31   30 1081  945   53   53] observation:  [0.053 0.053 0.053] \treward:  -50.65\n",
      "state:  [  31   30 1112  975   54   54] observation:  [0.054 0.054 0.054] \treward:  -52.175\n",
      "state:  [  31   30 1143 1005   55   55] observation:  [0.055 0.055 0.055] \treward:  -53.7\n",
      "state:  [  30   30 1173 1035   56   56] observation:  [0.056 0.056 0.056] \treward:  -55.2\n",
      "state:  [  29   30 1202 1065   57   57] observation:  [0.057 0.057 0.057] \treward:  -56.675\n",
      "state:  [  29   30 1231 1095   58   58] observation:  [0.058 0.058 0.058] \treward:  -58.15\n",
      "state:  [  29   28 1260 1123   59   59] observation:  [0.059 0.059 0.059] \treward:  -59.575\n",
      "state:  [  28   29 1288 1152   60   60] observation:  [0.06 0.06 0.06] \treward:  -61.0\n",
      "state:  [  28   30 1316 1182   61   61] observation:  [0.061 0.061 0.061] \treward:  -62.45\n",
      "state:  [  27   29 1343 1211   62   62] observation:  [0.062 0.062 0.062] \treward:  -63.85\n",
      "state:  [  29   29 1372 1240   63   63] observation:  [0.063 0.063 0.063] \treward:  -65.3\n",
      "state:  [  30   29 1402 1269   64   64] observation:  [0.064 0.064 0.064] \treward:  -66.775\n",
      "state:  [  32   28 1434 1297   65   65] observation:  [0.065 0.065 0.065] \treward:  -68.275\n",
      "state:  [  31   28 1465 1325   66   66] observation:  [0.066 0.066 0.066] \treward:  -69.75\n",
      "state:  [  31   28 1496 1353   67   67] observation:  [0.067 0.067 0.067] \treward:  -71.225\n",
      "state:  [  30   28 1526 1381   68   68] observation:  [0.068 0.068 0.068] \treward:  -72.675\n",
      "state:  [  30   28 1556 1409   69   69] observation:  [0.069 0.069 0.069] \treward:  -74.125\n",
      "state:  [  31   29 1587 1438   70   70] observation:  [0.07 0.07 0.07] \treward:  -75.625\n",
      "state:  [  31   30 1618 1468   71   71] observation:  [0.071 0.071 0.071] \treward:  -77.15\n",
      "state:  [  31   30 1649 1498   72   72] observation:  [0.072 0.072 0.072] \treward:  -78.675\n",
      "state:  [  29   31 1678 1529   73   73] observation:  [0.073 0.073 0.073] \treward:  -80.175\n",
      "state:  [  30   31 1708 1560   74   74] observation:  [0.074 0.074 0.074] \treward:  -81.7\n",
      "state:  [  29   30 1737 1590   75   75] observation:  [0.075 0.075 0.075] \treward:  -83.175\n",
      "state:  [  29   29 1766 1619   76   76] observation:  [0.076 0.076 0.076] \treward:  -84.625\n",
      "state:  [  29   28 1795 1647   77   77] observation:  [0.077 0.077 0.077] \treward:  -86.05\n",
      "state:  [  28   29 1823 1676   78   78] observation:  [0.078 0.078 0.078] \treward:  -87.475\n",
      "state:  [  29   29 1852 1705   79   79] observation:  [0.079 0.079 0.079] \treward:  -88.925\n",
      "state:  [  29   29 1881 1734   80   80] observation:  [0.08 0.08 0.08] \treward:  -90.375\n",
      "state:  [  27   30 1908 1764   81   81] observation:  [0.081 0.081 0.081] \treward:  -91.8\n",
      "state:  [  28   30 1936 1794   82   82] observation:  [0.082 0.082 0.082] \treward:  -93.25\n",
      "state:  [  27   28 1963 1822   83   83] observation:  [0.083 0.083 0.083] \treward:  -94.625\n",
      "state:  [  28   28 1991 1850   84   84] observation:  [0.084 0.084 0.084] \treward:  -96.025\n",
      "state:  [  29   29 2020 1879   85   85] observation:  [0.085 0.085 0.085] \treward:  -97.475\n",
      "state:  [  29   29 2049 1908   86   86] observation:  [0.086 0.086 0.086] \treward:  -98.925\n",
      "state:  [  28   27 2077 1935   87   87] observation:  [0.087 0.087 0.087] \treward:  -100.3\n",
      "state:  [  27   27 2104 1962   88   88] observation:  [0.088 0.088 0.088] \treward:  -101.65\n",
      "state:  [  26   28 2130 1990   89   89] observation:  [0.089 0.089 0.089] \treward:  -103.0\n",
      "state:  [  28   28 2158 2018   90   90] observation:  [0.09 0.09 0.09] \treward:  -104.4\n",
      "state:  [  28   28 2186 2046   91   91] observation:  [0.091 0.091 0.091] \treward:  -105.8\n",
      "state:  [  29   28 2215 2074   92   92] observation:  [0.092 0.092 0.092] \treward:  -107.225\n",
      "state:  [  28   26 2243 2100   93   93] observation:  [0.093 0.093 0.093] \treward:  -108.575\n",
      "state:  [  28   25 2271 2125   94   94] observation:  [0.094 0.094 0.094] \treward:  -109.9\n",
      "state:  [  29   25 2300 2150   95   95] observation:  [0.095 0.095 0.095] \treward:  -111.25\n",
      "state:  [  28   25 2328 2175   96   96] observation:  [0.096 0.096 0.096] \treward:  -112.575\n",
      "state:  [  28   24 2356 2199   97   97] observation:  [0.097 0.097 0.097] \treward:  -113.875\n",
      "state:  [  26   24 2382 2223   98   98] observation:  [0.098 0.098 0.098] \treward:  -115.125\n",
      "state:  [  24   26 2406 2249   99   99] observation:  [0.099 0.099 0.099] \treward:  -116.375\n",
      "state:  [  23   26 2429 2275  100  100] observation:  [0.1 0.1 0.1] \treward:  -117.6\n",
      "state:  [  23   25 2452 2300  101  101] observation:  [0.101 0.101 0.101] \treward:  -118.8\n",
      "state:  [  24   25 2476 2325  102  102] observation:  [0.102 0.102 0.102] \treward:  -120.025\n",
      "state:  [  22   23 2498 2348  103  103] observation:  [0.103 0.103 0.103] \treward:  -121.15\n",
      "state:  [  22   23 2520 2371  104  104] observation:  [0.104 0.104 0.104] \treward:  -122.275\n",
      "state:  [  22   24 2542 2395  105  105] observation:  [0.105 0.105 0.105] \treward:  -123.425\n",
      "state:  [  22   23 2564 2418  106  106] observation:  [0.106 0.106 0.106] \treward:  -124.55\n",
      "state:  [  21   23 2585 2441  107  107] observation:  [0.107 0.107 0.107] \treward:  -125.65\n",
      "state:  [  22   23 2607 2464  108  108] observation:  [0.108 0.108 0.108] \treward:  -126.775\n",
      "state:  [  22   23 2629 2487  109  109] observation:  [0.109 0.109 0.109] \treward:  -127.9\n",
      "state:  [  25   22 2654 2509  110  110] observation:  [0.11 0.11 0.11] \treward:  -129.075\n",
      "state:  [  26   22 2680 2531  111  111] observation:  [0.111 0.111 0.111] \treward:  -130.275\n",
      "state:  [  26   23 2706 2554  112  112] observation:  [0.112 0.112 0.112] \treward:  -131.5\n",
      "state:  [  25   25 2731 2579  113  113] observation:  [0.113 0.113 0.113] \treward:  -132.75\n",
      "state:  [  26   23 2757 2602  114  114] observation:  [0.114 0.114 0.114] \treward:  -133.975\n",
      "state:  [  28   24 2785 2626  115  115] observation:  [0.115 0.115 0.115] \treward:  -135.275\n",
      "state:  [  28   25 2813 2651  116  116] observation:  [0.116 0.116 0.116] \treward:  -136.6\n",
      "state:  [  28   25 2841 2676  117  117] observation:  [0.117 0.117 0.117] \treward:  -137.925\n",
      "state:  [  28   25 2869 2701  118  118] observation:  [0.118 0.118 0.118] \treward:  -139.25\n",
      "state:  [  30   24 2899 2725  119  119] observation:  [0.119 0.119 0.119] \treward:  -140.6\n",
      "state:  [  32   25 2931 2750  120  120] observation:  [0.12 0.12 0.12] \treward:  -142.025\n",
      "state:  [  31   28 2962 2778  121  121] observation:  [0.121 0.121 0.121] \treward:  -143.5\n",
      "state:  [  28   27 2990 2805  122  122] observation:  [0.122 0.122 0.122] \treward:  -144.875\n",
      "state:  [  27   29 3017 2834  123  123] observation:  [0.123 0.123 0.123] \treward:  -146.275\n",
      "state:  [  26   26 3043 2860  124  124] observation:  [0.124 0.124 0.124] \treward:  -147.575\n",
      "state:  [  26   26 3069 2886  125  125] observation:  [0.125 0.125 0.125] \treward:  -148.875\n",
      "state:  [  25   26 3094 2912  126  126] observation:  [0.126 0.126 0.126] \treward:  -150.15\n",
      "state:  [  27   25 3121 2937  127  127] observation:  [0.127 0.127 0.127] \treward:  -151.45\n",
      "state:  [  29   24 3150 2961  128  128] observation:  [0.128 0.128 0.128] \treward:  -152.775\n",
      "state:  [  29   25 3179 2986  129  129] observation:  [0.129 0.129 0.129] \treward:  -154.125\n",
      "state:  [  30   25 3209 3011  130  130] observation:  [0.13 0.13 0.13] \treward:  -155.5\n",
      "state:  [  29   26 3238 3037  131  131] observation:  [0.131 0.131 0.131] \treward:  -156.875\n",
      "state:  [  28   26 3266 3063  132  132] observation:  [0.132 0.132 0.132] \treward:  -158.225\n",
      "state:  [  27   26 3293 3089  133  133] observation:  [0.133 0.133 0.133] \treward:  -159.55\n",
      "state:  [  26   27 3319 3116  134  134] observation:  [0.134 0.134 0.134] \treward:  -160.875\n",
      "state:  [  26   26 3345 3142  135  135] observation:  [0.135 0.135 0.135] \treward:  -162.175\n",
      "state:  [  25   25 3370 3167  136  136] observation:  [0.136 0.136 0.136] \treward:  -163.425\n",
      "state:  [  24   27 3394 3194  137  137] observation:  [0.137 0.137 0.137] \treward:  -164.7\n",
      "state:  [  24   28 3418 3222  138  138] observation:  [0.138 0.138 0.138] \treward:  -166.0\n",
      "state:  [  24   28 3442 3250  139  139] observation:  [0.139 0.139 0.139] \treward:  -167.3\n",
      "state:  [  22   28 3464 3278  140  140] observation:  [0.14 0.14 0.14] \treward:  -168.55\n",
      "state:  [  24   29 3488 3307  141  141] observation:  [0.141 0.141 0.141] \treward:  -169.875\n",
      "state:  [  23   29 3511 3336  142  142] observation:  [0.142 0.142 0.142] \treward:  -171.175\n",
      "state:  [  22   28 3533 3364  143  143] observation:  [0.143 0.143 0.143] \treward:  -172.425\n",
      "state:  [  24   28 3557 3392  144  144] observation:  [0.144 0.144 0.144] \treward:  -173.725\n",
      "state:  [  24   27 3581 3419  145  145] observation:  [0.145 0.145 0.145] \treward:  -175.0\n",
      "state:  [  24   26 3605 3445  146  146] observation:  [0.146 0.146 0.146] \treward:  -176.25\n",
      "state:  [  24   28 3629 3473  147  147] observation:  [0.147 0.147 0.147] \treward:  -177.55\n",
      "state:  [  24   27 3653 3500  148  148] observation:  [0.148 0.148 0.148] \treward:  -178.825\n",
      "state:  [  23   28 3676 3528  149  149] observation:  [0.149 0.149 0.149] \treward:  -180.1\n",
      "state:  [  23   28 3699 3556  150  150] observation:  [0.15 0.15 0.15] \treward:  -181.375\n",
      "state:  [  22   28 3721 3584  151  151] observation:  [0.151 0.151 0.151] \treward:  -182.625\n",
      "state:  [  21   28 3742 3612  152  152] observation:  [0.152 0.152 0.152] \treward:  -183.85\n",
      "state:  [  20   26 3762 3638  153  153] observation:  [0.153 0.153 0.153] \treward:  -185.0\n",
      "state:  [  19   24 3781 3662  154  154] observation:  [0.154 0.154 0.154] \treward:  -186.075\n",
      "state:  [  19   24 3800 3686  155  155] observation:  [0.155 0.155 0.155] \treward:  -187.15\n",
      "state:  [  19   25 3819 3711  156  156] observation:  [0.156 0.156 0.156] \treward:  -188.25\n",
      "state:  [  19   24 3838 3735  157  157] observation:  [0.157 0.157 0.157] \treward:  -189.325\n",
      "state:  [  20   24 3858 3759  158  158] observation:  [0.158 0.158 0.158] \treward:  -190.425\n",
      "state:  [  18   24 3876 3783  159  159] observation:  [0.159 0.159 0.159] \treward:  -191.475\n",
      "state:  [  20   24 3896 3807  160  160] observation:  [0.16 0.16 0.16] \treward:  -192.575\n",
      "state:  [  21   24 3917 3831  161  161] observation:  [0.161 0.161 0.161] \treward:  -193.7\n",
      "state:  [  23   26 3940 3857  162  162] observation:  [0.162 0.162 0.162] \treward:  -194.925\n",
      "state:  [  23   27 3963 3884  163  163] observation:  [0.163 0.163 0.163] \treward:  -196.175\n",
      "state:  [  25   26 3988 3910  164  164] observation:  [0.164 0.164 0.164] \treward:  -197.45\n",
      "state:  [  25   26 4013 3936  165  165] observation:  [0.165 0.165 0.165] \treward:  -198.725\n",
      "state:  [  25   28 4038 3964  166  166] observation:  [0.166 0.166 0.166] \treward:  -200.05\n",
      "state:  [  25   29 4063 3993  167  167] observation:  [0.167 0.167 0.167] \treward:  -201.4\n",
      "state:  [  26   28 4089 4021  168  168] observation:  [0.168 0.168 0.168] \treward:  -202.75\n",
      "state:  [  27   27 4116 4048  169  169] observation:  [0.169 0.169 0.169] \treward:  -204.1\n",
      "state:  [  28   29 4144 4077  170  170] observation:  [0.17 0.17 0.17] \treward:  -205.525\n",
      "state:  [  28   28 4172 4105  171  171] observation:  [0.171 0.171 0.171] \treward:  -206.925\n",
      "state:  [  28   28 4200 4133  172  172] observation:  [0.172 0.172 0.172] \treward:  -208.325\n",
      "state:  [  28   27 4228 4160  173  173] observation:  [0.173 0.173 0.173] \treward:  -209.7\n",
      "state:  [  29   26 4257 4186  174  174] observation:  [0.174 0.174 0.174] \treward:  -211.075\n",
      "state:  [  29   25 4286 4211  175  175] observation:  [0.175 0.175 0.175] \treward:  -212.425\n",
      "state:  [  29   26 4315 4237  176  176] observation:  [0.176 0.176 0.176] \treward:  -213.8\n",
      "state:  [  29   26 4344 4263  177  177] observation:  [0.177 0.177 0.177] \treward:  -215.175\n",
      "state:  [  30   26 4374 4289  178  178] observation:  [0.178 0.178 0.178] \treward:  -216.575\n",
      "state:  [  30   26 4404 4315  179  179] observation:  [0.179 0.179 0.179] \treward:  -217.975\n",
      "state:  [  30   27 4434 4342  180  180] observation:  [0.18 0.18 0.18] \treward:  -219.4\n",
      "state:  [  30   28 4464 4370  181  181] observation:  [0.181 0.181 0.181] \treward:  -220.85\n",
      "state:  [  30   27 4494 4397  182  182] observation:  [0.182 0.182 0.182] \treward:  -222.275\n",
      "state:  [  30   28 4524 4425  183  183] observation:  [0.183 0.183 0.183] \treward:  -223.725\n",
      "state:  [  31   28 4555 4453  184  184] observation:  [0.184 0.184 0.184] \treward:  -225.2\n",
      "state:  [  30   29 4585 4482  185  185] observation:  [0.185 0.185 0.185] \treward:  -226.675\n",
      "state:  [  30   29 4615 4511  186  186] observation:  [0.186 0.186 0.186] \treward:  -228.15\n",
      "state:  [  31   29 4646 4540  187  187] observation:  [0.187 0.187 0.187] \treward:  -229.65\n",
      "state:  [  31   28 4677 4568  188  188] observation:  [0.188 0.188 0.188] \treward:  -231.125\n",
      "state:  [  30   31 4707 4599  189  189] observation:  [0.189 0.189 0.189] \treward:  -232.65\n",
      "state:  [  30   29 4737 4628  190  190] observation:  [0.19 0.19 0.19] \treward:  -234.125\n",
      "state:  [  29   28 4766 4656  191  191] observation:  [0.191 0.191 0.191] \treward:  -235.55\n",
      "state:  [  29   30 4795 4686  192  192] observation:  [0.192 0.192 0.192] \treward:  -237.025\n",
      "state:  [  28   28 4823 4714  193  193] observation:  [0.193 0.193 0.193] \treward:  -238.425\n",
      "Final Reward =  -21588.4\n"
     ]
    }
   ],
   "source": [
    "# show interor values of environment\n",
    "time_step = henv.reset()\n",
    "print(time_step)\n",
    "cumulative_reward = time_step.reward\n",
    "finished = False\n",
    "\n",
    "while not finished:\n",
    "  time_step = henv.step([0.,0.]) # do nothing\n",
    "  s = henv.get_state()\n",
    "  print(\"state: \", s, \"observation: \", time_step.observation, \"\\treward: \", time_step.reward)\n",
    "  cumulative_reward += time_step.reward\n",
    "  if time_step.step_type == StepType.LAST:\n",
    "    finished = True\n",
    "\n",
    "print('Final Reward = ', cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action spec:\n",
      " BoundedArraySpec(shape=(2,), dtype=dtype('float32'), name='action', minimum=0.0, maximum=1.0) \n",
      "\n",
      "time step spec:\n",
      " TimeStep(\n",
      "{'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
      " 'observation': BoundedArraySpec(shape=(3,), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=1.0),\n",
      " 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n",
      " 'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "action_spec = henv.action_spec()\n",
    "ts_spec = henv.time_step_spec()\n",
    "print(\"action spec:\\n\", action_spec, \"\\n\\ntime step spec:\\n\", ts_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scripted policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do nothing policy: cull never\n",
    "action_script0 = [(max_episode_length, [0,0])]\n",
    "\n",
    "# cull first herd every 20th step and second herd every 20th step\n",
    "action_script1 = [(9, [0,0]), \n",
    "                 (1, [1,1]),\n",
    "                 (9, [0,0]), \n",
    "                 (1, [1,1])] * int(1+max_episode_length/20)\n",
    "\n",
    "manual_scripted_policy0 = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=ts_spec,\n",
    "    action_spec=action_spec,\n",
    "    action_script=action_script0)\n",
    "\n",
    "manual_scripted_policy1 = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=ts_spec,\n",
    "    action_spec=action_spec,\n",
    "    action_script=action_script1)\n",
    "\n",
    "init_policy_state = manual_scripted_policy0.get_initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action= [0. 0.] \tpolicy_state [0, 1]\n",
      "action= [0. 0.] \tpolicy_state [0, 2]\n",
      "action= [0. 0.] \tpolicy_state [0, 3]\n",
      "action= [0. 0.] \tpolicy_state [0, 4]\n",
      "action= [0. 0.] \tpolicy_state [0, 5]\n",
      "action= [0. 0.] \tpolicy_state [0, 6]\n",
      "action= [0. 0.] \tpolicy_state [0, 7]\n",
      "action= [0. 0.] \tpolicy_state [0, 8]\n",
      "action= [0. 0.] \tpolicy_state [0, 9]\n",
      "action= [1. 1.] \tpolicy_state [1, 1]\n",
      "action= [0. 0.] \tpolicy_state [2, 1]\n",
      "action= [0. 0.] \tpolicy_state [2, 2]\n",
      "action= [0. 0.] \tpolicy_state [2, 3]\n",
      "action= [0. 0.] \tpolicy_state [2, 4]\n",
      "action= [0. 0.] \tpolicy_state [2, 5]\n",
      "action= [0. 0.] \tpolicy_state [2, 6]\n",
      "action= [0. 0.] \tpolicy_state [2, 7]\n",
      "action= [0. 0.] \tpolicy_state [2, 8]\n",
      "action= [0. 0.] \tpolicy_state [2, 9]\n",
      "action= [1. 1.] \tpolicy_state [3, 1]\n",
      "action= [0. 0.] \tpolicy_state [4, 1]\n"
     ]
    }
   ],
   "source": [
    "policy_state =  init_policy_state\n",
    "ts0 = henv.reset()\n",
    "for _ in range(21):\n",
    "    action_step = manual_scripted_policy1.action(ts0, policy_state)\n",
    "    policy_state = action_step.state\n",
    "    print(\"action=\", action_step.action, \"\\tpolicy_state\", policy_state)\n",
    "policy_state = manual_scripted_policy1.get_initial_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_policy = random_py_policy.RandomPyPolicy(time_step_spec=ts_spec, action_spec=action_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drive a rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=50, verbose=False):\n",
    "  total_return = 0.0\n",
    "  cullsteps = 0\n",
    "  for e in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "        policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "    else:\n",
    "        policy_state = None # other policies without memory\n",
    "    episode_return = 0.0\n",
    "    i=0\n",
    "    while not time_step.is_last():\n",
    "        i+=1\n",
    "        action_step = policy.action(time_step, policy_state)\n",
    "        if action_step.action[0] > 0 or action_step.action[1] > 0:\n",
    "            cullsteps += 1\n",
    "        policy_state = action_step.state\n",
    "        time_step = environment.step(action_step.action)\n",
    "        if isinstance(environment, FREnv):\n",
    "            state = environment.get_state()\n",
    "        else:\n",
    "            state = None # TF environment from wrapper does not have get_state()\n",
    "        episode_return += time_step.reward\n",
    "        if verbose:\n",
    "            print (f\"episode {e:>2} step{i:>4} action: \", action_step.action, \"state=\", state, \"obs=\", time_step.observation, \"reward=\", time_step.reward)\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  cullsteps /= num_episodes\n",
    "  return avg_return, cullsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average return of random policy: -2986.306 avg steps with culls per episode: 75.8\n"
     ]
    }
   ],
   "source": [
    "random_reward, cullsteps = compute_avg_return(henv, random_policy)\n",
    "print (f\"average return of random policy: {random_reward:.3f} avg steps with culls per episode: {cullsteps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 step   1 action:  [0. 0.] state= [2 1 2 1 1 1] obs= [0.001 0.001 0.001] reward= -0.075\n",
      "episode  0 step   2 action:  [0. 0.] state= [2 3 4 4 2 2] obs= [0.002 0.002 0.002] reward= -0.2\n",
      "episode  0 step   3 action:  [0. 0.] state= [ 6  6 10 10  3  3] obs= [0.003 0.003 0.003] reward= -0.5\n",
      "episode  0 step   4 action:  [0. 0.] state= [ 8  6 18 16  4  4] obs= [0.004 0.004 0.004] reward= -0.85\n",
      "episode  0 step   5 action:  [0. 0.] state= [10  9 28 25  5  5] obs= [0.005 0.005 0.005] reward= -1.325\n",
      "episode  0 step   6 action:  [0. 0.] state= [10 10 38 35  6  6] obs= [0.006 0.006 0.006] reward= -1.825\n",
      "episode  0 step   7 action:  [0. 0.] state= [12 14 50 49  7  7] obs= [0.007 0.007 0.007] reward= -2.475\n",
      "episode  0 step   8 action:  [0. 0.] state= [12 15 62 64  8  8] obs= [0.008 0.008 0.008] reward= -3.15\n",
      "episode  0 step   9 action:  [0. 0.] state= [15 16 77 80  9  9] obs= [0.009 0.009 0.009] reward= -3.925\n",
      "episode  0 step  10 action:  [1. 1.] state= [ 0  0 77 80  0  0] obs= [0.01 0.   0.  ] reward= -67.925\n",
      "episode  0 step  11 action:  [0. 0.] state= [ 3  4 80 84  1  1] obs= [0.011 0.001 0.001] reward= -4.1\n",
      "episode  0 step  12 action:  [0. 0.] state= [ 3  4 83 88  2  2] obs= [0.012 0.002 0.002] reward= -4.275\n",
      "episode  0 step  13 action:  [0. 0.] state= [ 5  5 88 93  3  3] obs= [0.013 0.003 0.003] reward= -4.525\n",
      "episode  0 step  14 action:  [0. 0.] state= [  6   9  94 102   4   4] obs= [0.014 0.004 0.004] reward= -4.9\n",
      "episode  0 step  15 action:  [0. 0.] state= [  7  10 101 112   5   5] obs= [0.015 0.005 0.005] reward= -5.325\n",
      "episode  0 step  16 action:  [0. 0.] state= [  9  10 110 122   6   6] obs= [0.016 0.006 0.006] reward= -5.8\n",
      "episode  0 step  17 action:  [0. 0.] state= [ 15  12 125 134   7   7] obs= [0.017 0.007 0.007] reward= -6.475\n",
      "episode  0 step  18 action:  [0. 0.] state= [ 15  12 140 146   8   8] obs= [0.018 0.008 0.008] reward= -7.15\n",
      "episode  0 step  19 action:  [0. 0.] state= [ 15  14 155 160   9   9] obs= [0.019 0.009 0.009] reward= -7.875\n",
      "episode  0 step  20 action:  [1. 1.] state= [  0   0 155 160   0   0] obs= [0.02 0.   0.  ] reward= -71.875\n",
      "episode  0 step  21 action:  [0. 0.] state= [  3   2 158 162   1   1] obs= [0.021 0.001 0.001] reward= -8.0\n",
      "episode  0 step  22 action:  [0. 0.] state= [  4   4 162 166   2   2] obs= [0.022 0.002 0.002] reward= -8.2\n",
      "episode  0 step  23 action:  [0. 0.] state= [  6   4 168 170   3   3] obs= [0.023 0.003 0.003] reward= -8.45\n",
      "episode  0 step  24 action:  [0. 0.] state= [  8   8 176 178   4   4] obs= [0.024 0.004 0.004] reward= -8.85\n",
      "episode  0 step  25 action:  [0. 0.] state= [  9   9 185 187   5   5] obs= [0.025 0.005 0.005] reward= -9.3\n",
      "episode  0 step  26 action:  [0. 0.] state= [ 12  10 197 197   6   6] obs= [0.026 0.006 0.006] reward= -9.85\n",
      "episode  0 step  27 action:  [0. 0.] state= [ 12  11 209 208   7   7] obs= [0.027 0.007 0.007] reward= -10.425\n",
      "episode  0 step  28 action:  [0. 0.] state= [ 13  11 222 219   8   8] obs= [0.028 0.008 0.008] reward= -11.025\n",
      "episode  0 step  29 action:  [0. 0.] state= [ 15  12 237 231   9   9] obs= [0.029 0.009 0.009] reward= -11.7\n",
      "episode  0 step  30 action:  [1. 1.] state= [  0   0 237 231   0   0] obs= [0.03 0.   0.  ] reward= -75.7\n",
      "episode  0 step  31 action:  [0. 0.] state= [  1   2 238 233   1   1] obs= [0.031 0.001 0.001] reward= -11.775\n",
      "episode  0 step  32 action:  [0. 0.] state= [  3   5 241 238   2   2] obs= [0.032 0.002 0.002] reward= -11.975\n",
      "episode  0 step  33 action:  [0. 0.] state= [  4   8 245 246   3   3] obs= [0.033 0.003 0.003] reward= -12.275\n",
      "episode  0 step  34 action:  [0. 0.] state= [  8  10 253 256   4   4] obs= [0.034 0.004 0.004] reward= -12.725\n",
      "episode  0 step  35 action:  [0. 0.] state= [ 10  13 263 269   5   5] obs= [0.035 0.005 0.005] reward= -13.3\n",
      "episode  0 step  36 action:  [0. 0.] state= [ 10  16 273 285   6   6] obs= [0.036 0.006 0.006] reward= -13.95\n",
      "episode  0 step  37 action:  [0. 0.] state= [ 11  17 284 302   7   7] obs= [0.037 0.007 0.007] reward= -14.65\n",
      "episode  0 step  38 action:  [0. 0.] state= [ 12  17 296 319   8   8] obs= [0.038 0.008 0.008] reward= -15.375\n",
      "episode  0 step  39 action:  [0. 0.] state= [ 14  16 310 335   9   9] obs= [0.039 0.009 0.009] reward= -16.125\n",
      "episode  0 step  40 action:  [1. 1.] state= [  0   0 310 335   0   0] obs= [0.04 0.   0.  ] reward= -80.125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-568.325000077486, 4.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show states for one rollout of second scripted policy\n",
    "compute_avg_return(henv, manual_scripted_policy1, num_episodes=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average return of do-nothing-policy: -16469.791 avg culls 0.0\n",
      "average return of manual policy: -4017.298 avg culls 9.365\n"
     ]
    }
   ],
   "source": [
    "manual_reward0, cullsteps = compute_avg_return(henv, manual_scripted_policy0, num_episodes=200)\n",
    "print (f\"average return of do-nothing-policy: {manual_reward0:.3f} avg culls {cullsteps}\")\n",
    "manual_reward1, cullsteps = compute_avg_return(henv, manual_scripted_policy1, num_episodes=200)\n",
    "print (f\"average return of manual policy: {manual_reward1:.3f} avg culls {cullsteps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train a DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=50, verbose=False):\n",
    "  total_return = 0.0\n",
    "  cullsteps = 0\n",
    "  for e in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "        policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "    else:\n",
    "        #print(policy.get_initial_state(batch_size=train_env.batch_size()))\n",
    "        policy_state = policy.get_initial_state(batch_size=1) # other policies without memory\n",
    "    episode_return = 0.0\n",
    "    i=0\n",
    "    while not time_step.is_last():\n",
    "        i+=1\n",
    "        action_step = policy.action(time_step, policy_state)\n",
    "        if action_step.action[0][0] > 0 or action_step.action[0][1] > 0:\n",
    "            cullsteps += 1\n",
    "        policy_state = action_step.state\n",
    "        time_step = environment.step(action_step.action)\n",
    "        if isinstance(environment, FREnv):\n",
    "            state = environment.get_state()\n",
    "        else:\n",
    "            state = None # TF environment from wrapper does not have get_state()\n",
    "        episode_return += time_step.reward\n",
    "        if verbose:\n",
    "            print (f\"episode {e:>2} step{i:>4} action: \", action_step.action, \"state=\", state, \"obs=\", time_step.observation, \"reward=\", time_step.reward)\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  cullsteps /= num_episodes\n",
    "  return avg_return, cullsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Made it work to here, implement ddpg agent next.'''\n",
    "\n",
    "import functools\n",
    "from tf_agents.networks import nest_map\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.keras_layers import inner_reshape\n",
    "from tf_agents.networks.sequential import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.agents.ddpg import actor_rnn_network\n",
    "from tf_agents.agents.ddpg import critic_rnn_network\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 30000\n",
    "replay_buffer_max_length = 10000\n",
    "batch_size = 64\n",
    "num_eval_episodes = 100\n",
    "initial_collect_steps = 1000\n",
    "collect_steps_per_iteration = 5\n",
    "log_interval = 500\n",
    "eval_interval = 500\n",
    "target_update_period = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(FREnv(herd_sizes = [32,32], expected_episode_length=100, max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.02, rand_infection_prob = 0.06))\n",
    "eval_env = tf_py_environment.TFPyEnvironment(FREnv(herd_sizes = [32,32], expected_episode_length=100, max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.02, rand_infection_prob = 0.06))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03)\n",
    "\n",
    "#Make critic and actor net for ddpg\n",
    "\n",
    "actor_fc_layers=(400, 300)\n",
    "actor_output_fc_layers=(100,)\n",
    "actor_lstm_size=(40,)\n",
    "critic_obs_fc_layers=(400,)\n",
    "critic_action_fc_layers=None\n",
    "critic_joint_fc_layers=(300,)\n",
    "critic_output_fc_layers=(100,)\n",
    "critic_lstm_size=(40,)\n",
    "\n",
    "\n",
    "actor_net = actor_rnn_network.ActorRnnNetwork(\n",
    "        train_env.time_step_spec().observation,\n",
    "        train_env.action_spec(),\n",
    "        input_fc_layer_params=actor_fc_layers,\n",
    "        lstm_size=actor_lstm_size,\n",
    "        output_fc_layer_params=actor_output_fc_layers)\n",
    "\n",
    "critic_net_input_specs = (train_env.time_step_spec().observation,\n",
    "                              train_env.action_spec())\n",
    "\n",
    "critic_net = critic_rnn_network.CriticRnnNetwork(\n",
    "        critic_net_input_specs,\n",
    "        observation_fc_layer_params=critic_obs_fc_layers,\n",
    "        action_fc_layer_params=critic_action_fc_layers,\n",
    "        joint_fc_layer_params=critic_joint_fc_layers,\n",
    "        lstm_size=critic_lstm_size,\n",
    "        output_fc_layer_params=critic_output_fc_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_counter = tf.Variable(0)\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "agent = ddpg_agent.DdpgAgent(\n",
    "        train_env.time_step_spec(),\n",
    "        train_env.action_spec(),\n",
    "        actor_network=actor_net,\n",
    "        critic_network=critic_net,\n",
    "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=1e-4),\n",
    "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=1e-3),\n",
    "        ou_stddev=0.2,\n",
    "        ou_damping=0.15,\n",
    "        target_update_tau=0.05,\n",
    "        target_update_period=5,\n",
    "        #dqda_clipping=dqda_clipping,\n",
    "        td_errors_loss_fn=tf.compat.v1.losses.huber_loss,\n",
    "        gamma=0.99,\n",
    "        #reward_scale_factor=reward_scale_factor,\n",
    "        #gradient_clipping=gradient_clipping,\n",
    "        #debug_summaries=debug_summaries,\n",
    "        #summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "        train_step_counter=global_step)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually initialize a reasonably good policy: kill both herds if the sum of observations is large\n",
    "#W = np.array([[0, 3 ,0, 2],[0, 0, 3, 2,]])\n",
    "#b = np.array([1, 0, 0, 0])\n",
    "#q_net.layers[0].set_weights([W,b])\n",
    "#agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(\n",
       "{action: (64, 2, 2),\n",
       " discount: (64, 2),\n",
       " next_step_type: (64, 2),\n",
       " observation: (64, 2, 3),\n",
       " policy_info: (),\n",
       " reward: (64, 2),\n",
       " step_type: (64, 2)}), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(\n",
       "{action: tf.float32,\n",
       " discount: tf.float32,\n",
       " next_step_type: tf.int32,\n",
       " observation: tf.float32,\n",
       " policy_info: (),\n",
       " reward: tf.float32,\n",
       " step_type: tf.int32}), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(64, 1), dtype=float32, numpy=\n",
      "array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(64, 1, 3), dtype=float32, numpy=\n",
      "array([[[0.104, 0.006, 0.002]],\n",
      "\n",
      "       [[0.131, 0.001, 0.   ]],\n",
      "\n",
      "       [[0.099, 0.   , 0.001]],\n",
      "\n",
      "       [[0.011, 0.   , 0.008]],\n",
      "\n",
      "       [[0.03 , 0.003, 0.   ]],\n",
      "\n",
      "       [[0.069, 0.001, 0.001]],\n",
      "\n",
      "       [[0.003, 0.   , 0.   ]],\n",
      "\n",
      "       [[0.135, 0.   , 0.001]],\n",
      "\n",
      "       [[0.062, 0.003, 0.002]],\n",
      "\n",
      "       [[0.127, 0.   , 0.   ]],\n",
      "\n",
      "       [[0.051, 0.   , 0.001]],\n",
      "\n",
      "       [[0.088, 0.   , 0.002]],\n",
      "\n",
      "       [[0.027, 0.001, 0.   ]],\n",
      "\n",
      "       [[0.031, 0.002, 0.   ]],\n",
      "\n",
      "       [[0.008, 0.   , 0.   ]],\n",
      "\n",
      "       [[0.006, 0.001, 0.001]],\n",
      "\n",
      "       [[0.058, 0.   , 0.   ]],\n",
      "\n",
      "       [[0.073, 0.   , 0.   ]],\n",
      "\n",
      "       [[0.043, 0.001, 0.   ]],\n",
      "\n",
      "       [[0.043, 0.   , 0.002]],\n",
      "\n",
      "       [[0.113, 0.001, 0.   ]],\n",
      "\n",
      "       [[0.05 , 0.   , 0.001]],\n",
      "\n",
      "       [[0.004, 0.   , 0.001]],\n",
      "\n",
      "       [[0.128, 0.   , 0.002]],\n",
      "\n",
      "       [[0.151, 0.002, 0.001]],\n",
      "\n",
      "       [[0.038, 0.   , 0.   ]],\n",
      "\n",
      "       [[0.03 , 0.004, 0.   ]],\n",
      "\n",
      "       [[0.005, 0.   , 0.002]],\n",
      "\n",
      "       [[0.174, 0.   , 0.002]],\n",
      "\n",
      "       [[0.131, 0.   , 0.002]],\n",
      "\n",
      "       [[0.077, 0.001, 0.   ]],\n",
      "\n",
      "       [[0.007, 0.002, 0.   ]],\n",
      "\n",
      "       [[0.032, 0.001, 0.   ]],\n",
      "\n",
      "       [[0.039, 0.001, 0.   ]],\n",
      "\n",
      "       [[0.144, 0.   , 0.   ]],\n",
      "\n",
      "       [[0.008, 0.   , 0.001]],\n",
      "\n",
      "       [[0.166, 0.   , 0.   ]],\n",
      "\n",
      "       [[0.072, 0.001, 0.002]],\n",
      "\n",
      "       [[0.064, 0.   , 0.   ]],\n",
      "\n",
      "       [[0.012, 0.001, 0.   ]],\n",
      "\n",
      "       [[0.061, 0.   , 0.   ]],\n",
      "\n",
      "       [[0.099, 0.   , 0.002]],\n",
      "\n",
      "       [[0.085, 0.   , 0.   ]],\n",
      "\n",
      "       [[0.043, 0.001, 0.001]],\n",
      "\n",
      "       [[0.15 , 0.001, 0.004]],\n",
      "\n",
      "       [[0.029, 0.002, 0.003]],\n",
      "\n",
      "       [[0.023, 0.   , 0.001]],\n",
      "\n",
      "       [[0.02 , 0.   , 0.   ]],\n",
      "\n",
      "       [[0.018, 0.   , 0.002]],\n",
      "\n",
      "       [[0.138, 0.003, 0.   ]],\n",
      "\n",
      "       [[0.121, 0.008, 0.003]],\n",
      "\n",
      "       [[0.012, 0.   , 0.   ]],\n",
      "\n",
      "       [[0.025, 0.   , 0.   ]],\n",
      "\n",
      "       [[0.003, 0.   , 0.002]],\n",
      "\n",
      "       [[0.019, 0.   , 0.003]],\n",
      "\n",
      "       [[0.123, 0.   , 0.003]],\n",
      "\n",
      "       [[0.054, 0.   , 0.   ]],\n",
      "\n",
      "       [[0.009, 0.   , 0.001]],\n",
      "\n",
      "       [[0.045, 0.   , 0.001]],\n",
      "\n",
      "       [[0.159, 0.   , 0.   ]],\n",
      "\n",
      "       [[0.051, 0.   , 0.001]],\n",
      "\n",
      "       [[0.068, 0.   , 0.   ]],\n",
      "\n",
      "       [[0.013, 0.001, 0.   ]],\n",
      "\n",
      "       [[0.057, 0.   , 0.   ]]], dtype=float32)>,\n",
      " 'reward': <tf.Tensor: shape=(64, 1), dtype=float32, numpy=\n",
      "array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(64, 1), dtype=int32, numpy=\n",
      "array([[1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1]], dtype=int32)>})\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Invalid reduction dimension (1 for input with 0 dimension(s) [Op:Sum]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-1f490787fca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tf_agents/agents/tf_agent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, experience, weights, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_functions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m       loss_info = self._train_fn(\n\u001b[0m\u001b[1;32m    332\u001b[0m           experience=experience, weights=weights, **kwargs)\n\u001b[1;32m    333\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tf_agents/agents/ddpg/ddpg_agent.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, experience, weights)\u001b[0m\n\u001b[1;32m    246\u001b[0m                                           'optimize.')\n\u001b[1;32m    247\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_critic_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m       critic_loss = self.critic_loss(time_steps, actions, next_time_steps,\n\u001b[0m\u001b[1;32m    249\u001b[0m                                      weights=weights, training=True)\n\u001b[1;32m    250\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_numerics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Critic loss is inf or nan.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tf_agents/agents/ddpg/ddpg_agent.py\u001b[0m in \u001b[0;36mcritic_loss\u001b[0;34m(self, time_steps, actions, next_time_steps, weights, training)\u001b[0m\n\u001b[1;32m    343\u001b[0m           time_steps, self.time_step_spec, num_outer_dims=2):\n\u001b[1;32m    344\u001b[0m         \u001b[0;31m# Do a sum over the time dimension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mcritic_loss\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2251\u001b[0m   \"\"\"\n\u001b[1;32m   2252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2253\u001b[0;31m   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[0m\u001b[1;32m   2254\u001b[0m                               _ReductionDims(input_tensor, axis))\n\u001b[1;32m   2255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum_with_dims\u001b[0;34m(input_tensor, axis, keepdims, name, dims)\u001b[0m\n\u001b[1;32m   2263\u001b[0m   return _may_reduce_to_scalar(\n\u001b[1;32m   2264\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2265\u001b[0;31m       gen_math_ops._sum(input_tensor, dims, keepdims, name=name))\n\u001b[0m\u001b[1;32m   2266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m  10708\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10709\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10710\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10711\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10712\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6939\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6940\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6941\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6942\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Invalid reduction dimension (1 for input with 0 dimension(s) [Op:Sum]"
     ]
    }
   ],
   "source": [
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  tra = agent._as_transition(experience)\n",
    "  ts, b, c = tra\n",
    "  print(ts)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step <= 1 or step % log_interval == 0:\n",
    "    print('step = {0:4>}: loss = {1:.4f}'.format(step, train_loss), end=\"\\t\")\n",
    "\n",
    "  if step <= 1 or (step <= 100 and step % 50 == 0) or step % eval_interval == 0:\n",
    "    avg_return, cullsteps = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step {0}: average return = {1:.1f} cullsteps = {2:.1f}'.format(step, avg_return.numpy().item(), cullsteps))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_reward, culleps = compute_avg_return(eval_env, agent.policy, num_episodes=500)\n",
    "print (\"reward of learned policy: \", learned_reward.numpy(), \"cullsteps=\", culleps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ts = eval_env.reset()\n",
    "\n",
    "def get_action(obs):\n",
    "    \"\"\" execute the learned policy network \n",
    "       obs:  one float for global time, one float for each herd - the time since last culling\n",
    "    \"\"\"\n",
    "    _ts = TimeStep(tf.constant([0.]),\n",
    "                   tf.constant([0.]),\n",
    "                   tf.constant([1]),\n",
    "                   tf.constant([obs]))\n",
    "    # a = agent.collect_policy.action(_ts) # just to see how much is explored versus exploited\n",
    "    a = agent.policy.action(_ts)\n",
    "    return a.action.numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what the learned policy does on a grid of observations (5 steps per row&col)\n",
    "'''A = [[get_action([.0, x,y])\n",
    " for y in np.arange(0.,1.,.05,np.float32)]\n",
    " for x in np.arange(0.,1.,.05,np.float32)]\n",
    "A'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with parameters of manually designed q_network policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W, b = agent.policy.trainable_variables\n",
    "# W = W.numpy()\n",
    "# b = b.numpy()\n",
    "# print (\"weights\\n\", W, \"\\nbias\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nn(obs):\n",
    "#    y = np.dot(obs, W)+b\n",
    "#    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn([0.5,.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
