{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the environment `HerdEnv` of `herd_env.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "from tf_agents.trajectories import TimeStep\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.specs import tensor_spec\n",
    "\n",
    "from tf_agents.networks import sequential\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '/home/jovyan/Masterarbeit/reinforce-one/Environment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Env import Env\n",
    "root_dir = '~/Masterarbeit/DDPG'\n",
    "num_herds=2\n",
    "total_population=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=50, verbose=False):\n",
    "  total_return = 0.0\n",
    "  cullsteps = 0 \n",
    "  for e in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "        policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "    else:\n",
    "        #print(policy.get_initial_state(batch_size=train_env.batch_size()))\n",
    "        policy_state = policy.get_initial_state(batch_size=1) # other policies without memory\n",
    "    episode_return = 0.0\n",
    "    i=0\n",
    "    while not time_step.is_last():\n",
    "        i+=1\n",
    "        action_step = policy.action(time_step, policy_state)\n",
    "        for i in range (num_herds, num_herds*2):\n",
    "            if action_step.action[0][i] >= 0.5:\n",
    "                cullsteps += 1\n",
    "                break\n",
    "        policy_state = action_step.state\n",
    "        time_step = environment.step(action_step.action)\n",
    "\n",
    "        state = None # TF environment from wrapper does not have get_state()\n",
    "        episode_return += time_step.reward\n",
    "        if verbose:\n",
    "            print (f\"episode {e:>2} step{i:>4} action: \", action_step.action, \n",
    "                   \"state=\", state, \"obs=\", time_step.observation, \"reward=\", time_step.reward)\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  cullsteps /= num_episodes\n",
    "  return avg_return, cullsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import functools\n",
    "from tf_agents.networks import nest_map\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.keras_layers import inner_reshape\n",
    "from tf_agents.networks.sequential import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 50000\n",
    "replay_buffer_max_length = 100000\n",
    "batch_size = 64\n",
    "num_eval_episodes = 100\n",
    "initial_collect_steps = 5000\n",
    "collect_steps_per_iteration = 200\n",
    "log_interval = 1000\n",
    "eval_interval = 500\n",
    "threshhold_reset_interval = 10000\n",
    "target_update_period = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(Env(num_herds = num_herds, total_population = total_population, \n",
    "                                                  fix_episode_length = True, average_episode_length = 200))\n",
    "eval_env = tf_py_environment.TFPyEnvironment(Env(num_herds = num_herds, total_population = total_population, \n",
    "                                                 fix_episode_length = True, average_episode_length = 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03)\n",
    "\n",
    "#Make critic and actor net for ddpg\n",
    "\n",
    "dense = functools.partial(\n",
    "    tf.keras.layers.Dense,\n",
    "    activation=tf.keras.activations.relu,\n",
    "    kernel_initializer=tf.compat.v1.variance_scaling_initializer(\n",
    "        scale=1./ 3.0, mode='fan_in', distribution='uniform'))\n",
    "\n",
    "\n",
    "def create_identity_layer():\n",
    "    return tf.keras.layers.Lambda(lambda x: x)\n",
    "\n",
    "\n",
    "def create_fc_network(layer_units):\n",
    "    return sequential.Sequential([dense(num_units) for num_units in layer_units])\n",
    "\n",
    "\n",
    "def create_actor_network(fc_layer_units, action_spec):\n",
    "    \"\"\"Create an actor network for DDPG.\"\"\"\n",
    "    flat_action_spec = tf.nest.flatten(action_spec)\n",
    "    if len(flat_action_spec) > 1:\n",
    "        raise ValueError('Only a single action tensor is supported by this network')\n",
    "    flat_action_spec = flat_action_spec[0]\n",
    "\n",
    "    fc_layers = [dense(num_units) for num_units in fc_layer_units]\n",
    "\n",
    "    num_actions = flat_action_spec.shape.num_elements()\n",
    "    action_fc_layer = tf.keras.layers.Dense(\n",
    "        num_actions,\n",
    "        activation=tf.keras.activations.tanh,\n",
    "        kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "            minval=-0.003, maxval=0.003))\n",
    "\n",
    "    scaling_layer = tf.keras.layers.Lambda(\n",
    "        lambda x: common.scale_to_spec(x, flat_action_spec))\n",
    "    return sequential.Sequential(fc_layers + [action_fc_layer, scaling_layer])\n",
    "\n",
    "\n",
    "def create_critic_network(obs_fc_layer_units,\n",
    "                          action_fc_layer_units,\n",
    "                          joint_fc_layer_units):\n",
    "    \"\"\"Create a critic network for DDPG.\"\"\"\n",
    "\n",
    "    def split_inputs(inputs):\n",
    "        return {'observation': inputs[0], 'action': inputs[1]}\n",
    "\n",
    "    obs_network = create_fc_network(\n",
    "        obs_fc_layer_units) if obs_fc_layer_units else create_identity_layer()\n",
    "    action_network = create_fc_network(\n",
    "        action_fc_layer_units\n",
    "        ) if action_fc_layer_units else create_identity_layer()\n",
    "    joint_network = create_fc_network(\n",
    "        joint_fc_layer_units) if joint_fc_layer_units else create_identity_layer()\n",
    "    \n",
    "    value_fc_layer = tf.keras.layers.Dense(\n",
    "        1,\n",
    "        activation=None,\n",
    "        kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "            minval=-0.003, maxval=0.003))\n",
    "\n",
    "    return sequential.Sequential([\n",
    "        tf.keras.layers.Lambda(split_inputs),\n",
    "        nest_map.NestMap({\n",
    "            'observation': obs_network,\n",
    "            'action': action_network\n",
    "        }),\n",
    "        nest_map.NestFlatten(),\n",
    "        tf.keras.layers.Concatenate(),\n",
    "        joint_network,\n",
    "        value_fc_layer,\n",
    "        inner_reshape.InnerReshape([1], [])\n",
    "    ])\n",
    "\n",
    "actor_fc_layers=(200, 150)\n",
    "critic_obs_fc_layers=(200,)\n",
    "critic_action_fc_layers=None\n",
    "critic_joint_fc_layers=(150,)\n",
    "\n",
    "actor_net = create_actor_network(actor_fc_layers, train_env.action_spec())\n",
    "critic_net = create_critic_network(critic_obs_fc_layers,\n",
    "                                       critic_action_fc_layers,\n",
    "                                       critic_joint_fc_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_counter = tf.Variable(0)\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "agent = ddpg_agent.DdpgAgent(\n",
    "        train_env.time_step_spec(),\n",
    "        train_env.action_spec(),\n",
    "        actor_network=actor_net,\n",
    "        critic_network=critic_net,\n",
    "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=1e-4),    #1e-4\n",
    "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=1e-3),    #1e-3\n",
    "        ou_stddev=0.2,\n",
    "        ou_damping=0.15,\n",
    "        target_update_tau=0.05,\n",
    "        target_update_period=5,\n",
    "        #dqda_clipping=dqda_clipping,\n",
    "        td_errors_loss_fn=tf.compat.v1.losses.huber_loss,\n",
    "        gamma=0.99,\n",
    "        #reward_scale_factor=reward_scale_factor,\n",
    "        #gradient_clipping=gradient_clipping,\n",
    "        #debug_summaries=debug_summaries,\n",
    "        #summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "        train_step_counter=global_step)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually initialize a reasonably good policy: kill both herds if the sum of observations is large\n",
    "#W = np.array([[0, 3 ,0, 2],[0, 0, 3, 2,]])\n",
    "#b = np.array([1, 0, 0, 0])\n",
    "#q_net.layers[0].set_weights([W,b])\n",
    "#agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(\n",
    "    action_spec=train_env.action_spec(), time_step_spec=train_env.time_step_spec())\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(\n",
       "{action: (64, 2, 4),\n",
       " discount: (64, 2),\n",
       " next_step_type: (64, 2),\n",
       " observation: (64, 2, 7),\n",
       " policy_info: (),\n",
       " reward: (64, 2),\n",
       " step_type: (64, 2)}), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(\n",
       "{action: tf.float32,\n",
       " discount: tf.float32,\n",
       " next_step_type: tf.int32,\n",
       " observation: tf.float32,\n",
       " policy_info: (),\n",
       " reward: tf.float32,\n",
       " step_type: tf.int32}), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: average return = -586.1 cullsteps = 0.0\n",
      "New best return:  tf.Tensor([-586.05316], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/500/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/500/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1000: loss = 1.0825\tstep 1000: average return = -587.3 cullsteps = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1500: average return = -584.7 cullsteps = 0.0\n",
      "New best return:  tf.Tensor([-584.6961], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/1500/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/1500/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2000: loss = 2.0383\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000: average return = -584.0 cullsteps = 0.0\n",
      "New best return:  tf.Tensor([-583.9886], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/2000/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/2000/assets\n",
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2500: average return = -289.4 cullsteps = 161.8\n",
      "New best return:  tf.Tensor([-289.4167], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/2500/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/2500/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3000: loss = 0.9718\tstep 3000: average return = -311.5 cullsteps = 174.5\n",
      "step 3500: average return = -442.9 cullsteps = 82.8\n",
      "step = 4000: loss = 1.9933\tstep 4000: average return = -476.8 cullsteps = 44.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4500: average return = -216.1 cullsteps = 140.5\n",
      "New best return:  tf.Tensor([-216.14601], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/4500/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/4500/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 5000: loss = 1.6512\tstep 5000: average return = -270.1 cullsteps = 168.0\n",
      "step 5500: average return = -502.0 cullsteps = 13.6\n",
      "step = 6000: loss = 0.9712\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6000: average return = -69.2 cullsteps = 52.8\n",
      "New best return:  tf.Tensor([-69.178444], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/6000/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/6000/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6500: average return = -69.3 cullsteps = 45.2\n",
      "step = 7000: loss = 1.0256\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7000: average return = -45.1 cullsteps = 23.9\n",
      "New best return:  tf.Tensor([-45.105785], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/7000/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/7000/assets\n",
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7500: average return = -33.7 cullsteps = 15.8\n",
      "New best return:  tf.Tensor([-33.659115], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/7500/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/7500/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 8000: loss = 0.4521\tstep 8000: average return = -34.2 cullsteps = 15.0\n",
      "step 8500: average return = -37.7 cullsteps = 14.0\n",
      "step = 9000: loss = 0.4773\tstep 9000: average return = -38.2 cullsteps = 12.9\n",
      "step 9500: average return = -36.1 cullsteps = 13.1\n",
      "step = 10000: loss = 0.1384\tstep 10000: average return = -38.1 cullsteps = 13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10500: average return = -40.6 cullsteps = 12.7\n",
      "New best return:  tf.Tensor([-40.590405], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/10500/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/10500/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 11000: loss = 0.0898\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 11000: average return = -39.4 cullsteps = 12.7\n",
      "New best return:  tf.Tensor([-39.400803], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/11000/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/11000/assets\n",
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 11500: average return = -36.7 cullsteps = 14.2\n",
      "New best return:  tf.Tensor([-36.72893], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/11500/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/11500/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 12000: loss = 0.2219\tstep 12000: average return = -47.7 cullsteps = 12.8\n",
      "step 12500: average return = -41.3 cullsteps = 13.2\n",
      "step = 13000: loss = 0.0595\tstep 13000: average return = -40.9 cullsteps = 13.4\n",
      "step 13500: average return = -51.4 cullsteps = 14.6\n",
      "step = 14000: loss = 0.1596\tstep 14000: average return = -226.8 cullsteps = 92.0\n",
      "step 14500: average return = -242.1 cullsteps = 122.5\n",
      "step = 15000: loss = 0.1315\tstep 15000: average return = -183.5 cullsteps = 52.1\n",
      "step 15500: average return = -180.8 cullsteps = 26.9\n",
      "step = 16000: loss = 0.0746\tstep 16000: average return = -189.2 cullsteps = 31.3\n",
      "step 16500: average return = -352.0 cullsteps = 117.6\n",
      "step = 17000: loss = 0.2729\tstep 17000: average return = -130.4 cullsteps = 16.8\n",
      "step 17500: average return = -47.6 cullsteps = 13.6\n",
      "step = 18000: loss = 0.1635\tstep 18000: average return = -49.8 cullsteps = 12.7\n",
      "step 18500: average return = -46.9 cullsteps = 13.3\n",
      "step = 19000: loss = 0.0873\tstep 19000: average return = -42.3 cullsteps = 12.6\n",
      "step 19500: average return = -216.9 cullsteps = 39.0\n",
      "step = 20000: loss = 0.1310\tstep 20000: average return = -94.6 cullsteps = 18.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20500: average return = -237.2 cullsteps = 53.1\n",
      "New best return:  tf.Tensor([-237.24811], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/20500/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/20500/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 21000: loss = 0.2637\tstep 21000: average return = -585.2 cullsteps = 0.1\n",
      "step 21500: average return = -417.4 cullsteps = 32.9\n",
      "step = 22000: loss = 0.1604\tstep 22000: average return = -272.5 cullsteps = 57.1\n",
      "step 22500: average return = -240.2 cullsteps = 150.1\n",
      "step = 23000: loss = 0.2990\tstep 23000: average return = -423.1 cullsteps = 8.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 23500: average return = -202.5 cullsteps = 45.2\n",
      "New best return:  tf.Tensor([-202.54333], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/23500/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/23500/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 24000: loss = 0.1892\tstep 24000: average return = -319.7 cullsteps = 30.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 24500: average return = -160.1 cullsteps = 15.6\n",
      "New best return:  tf.Tensor([-160.09172], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/24500/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/24500/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 25000: loss = 0.3236\tstep 25000: average return = -297.7 cullsteps = 151.3\n",
      "step 25500: average return = -251.3 cullsteps = 19.7\n",
      "step = 26000: loss = 0.4156\tstep 26000: average return = -219.1 cullsteps = 21.8\n",
      "step = 27000: loss = 0.1495\tstep 27000: average return = -249.4 cullsteps = 17.1\n",
      "step 27500: average return = -490.7 cullsteps = 4.0\n",
      "step = 28000: loss = 0.0976\tstep 28000: average return = -413.1 cullsteps = 7.0\n",
      "step 28500: average return = -565.9 cullsteps = 0.6\n",
      "step = 29000: loss = 0.1104\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 29000: average return = -60.2 cullsteps = 15.5\n",
      "New best return:  tf.Tensor([-60.243286], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/29000/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/29000/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 29500: average return = -75.5 cullsteps = 9.9\n",
      "step = 30000: loss = 0.4207\tstep 30000: average return = -145.6 cullsteps = 26.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 30500: average return = -198.3 cullsteps = 35.4\n",
      "New best return:  tf.Tensor([-198.27051], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/30500/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/30500/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 31000: loss = 0.2544\tstep 31000: average return = -215.2 cullsteps = 20.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 31500: average return = -119.7 cullsteps = 16.4\n",
      "New best return:  tf.Tensor([-119.70339], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/31500/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/31500/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 32000: loss = 0.1672\tstep 32000: average return = -219.5 cullsteps = 24.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 32500: average return = -97.8 cullsteps = 16.0\n",
      "New best return:  tf.Tensor([-97.84627], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/32500/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/32500/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 33000: loss = 0.0645\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 33000: average return = -54.8 cullsteps = 8.3\n",
      "New best return:  tf.Tensor([-54.7697], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/33000/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/33000/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 33500: average return = -111.6 cullsteps = 9.8\n",
      "step = 34000: loss = 0.2982\tstep 34000: average return = -115.9 cullsteps = 43.1\n",
      "step 34500: average return = -210.9 cullsteps = 12.6\n",
      "step = 35000: loss = 0.3199\tstep 35000: average return = -254.7 cullsteps = 17.8\n",
      "step 35500: average return = -88.0 cullsteps = 11.4\n",
      "step = 36000: loss = 0.1141\tstep 36000: average return = -302.8 cullsteps = 11.1\n"
     ]
    }
   ],
   "source": [
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "# Initialize policy saver\n",
    "saver = policy_saver.PolicySaver(agent.policy)\n",
    "best_return = -1000000\n",
    "root_dir = os.path.expanduser(root_dir)\n",
    "policy_dir = os.path.join(root_dir, 'policy')\n",
    "#compute_avg_return = common.function(compute_avg_return)\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0:4>}: loss = {1:.4f}'.format(step, train_loss), end=\"\\t\")\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return, cullsteps = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step {0}: average return = {1:.1f} cullsteps = {2:.1f}'.format(step, avg_return.numpy().item(), cullsteps))\n",
    "    returns.append(avg_return)\n",
    "   # if avg_return <= -100:\n",
    "      #  eval_interval = 500\n",
    "    #elif avg_return > -100 and avg_return < -40:\n",
    "       # eval_interval = 100\n",
    "    #elif avg_return >= -40:\n",
    "      #  eval_interval = 100\n",
    "    if avg_return <= -100:\n",
    "        eval_interval = 500\n",
    "    if avg_return > best_return:\n",
    "            best_return = avg_return\n",
    "            if best_return >= -33:\n",
    "                eval_interval = 5\n",
    "            print('New best return: ', best_return)\n",
    "            saver.save(os.path.join(policy_dir, str(global_step.numpy())))\n",
    "    elif step % threshhold_reset_interval == 0:\n",
    "        best_return = -1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_reward, culleps = compute_avg_return(eval_env, agent.policy, num_episodes=500)\n",
    "print (\"reward of learned policy: \", learned_reward.numpy(), \"cullsteps=\", culleps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ts = eval_env.reset()\n",
    "\n",
    "def get_action(obs):\n",
    "    \"\"\" execute the learned policy network \n",
    "       obs:  one float for global time, one float for each herd - the time since last culling\n",
    "    \"\"\"\n",
    "    _ts = TimeStep(tf.constant([0.]),\n",
    "                   tf.constant([0.]),\n",
    "                   tf.constant([1]),\n",
    "                   tf.constant([obs]))\n",
    "    # a = agent.collect_policy.action(_ts) # just to see how much is explored versus exploited\n",
    "    a = agent.policy.action(_ts)\n",
    "    return a.action.numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what the learned policy does on a grid of observations (5 steps per row&col)\n",
    "'''A = [[get_action([.0, x,y])\n",
    " for y in np.arange(0.,1.,.05,np.float32)]\n",
    " for x in np.arange(0.,1.,.05,np.float32)]\n",
    "A'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with parameters of manually designed q_network policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W, b = agent.policy.trainable_variables\n",
    "# W = W.numpy()\n",
    "# b = b.numpy()\n",
    "# print (\"weights\\n\", W, \"\\nbias\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nn(obs):\n",
    "#    y = np.dot(obs, W)+b\n",
    "#    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn([0.5,.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc-autonumbering": true,
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
