{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the environment `HerdEnv` of `herd_env.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "from tf_agents.trajectories import TimeStep\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.specs import tensor_spec\n",
    "\n",
    "from tf_agents.networks import sequential\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '/home/jovyan/Masterarbeit/reinforce-one/Environment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Env import Env\n",
    "root_dir = '~/Masterarbeit/DDPG'\n",
    "num_herds=2\n",
    "total_population=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=50, verbose=False):\n",
    "  total_return = 0.0\n",
    "  cullsteps = 0 \n",
    "  for e in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "        policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "    else:\n",
    "        #print(policy.get_initial_state(batch_size=train_env.batch_size()))\n",
    "        policy_state = policy.get_initial_state(batch_size=1) # other policies without memory\n",
    "    episode_return = 0.0\n",
    "    i=0\n",
    "    while not time_step.is_last():\n",
    "        i+=1\n",
    "        action_step = policy.action(time_step, policy_state)\n",
    "        for i in range (num_herds, num_herds*2):\n",
    "            if action_step.action[0][i] > 0.1:\n",
    "                cullsteps += 1\n",
    "                break\n",
    "        policy_state = action_step.state\n",
    "        time_step = environment.step(action_step.action)\n",
    "\n",
    "        state = None # TF environment from wrapper does not have get_state()\n",
    "        episode_return += time_step.reward\n",
    "        if verbose:\n",
    "            print (f\"episode {e:>2} step{i:>4} action: \", action_step.action, \n",
    "                   \"state=\", state, \"obs=\", time_step.observation, \"reward=\", time_step.reward)\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  cullsteps /= num_episodes\n",
    "  return avg_return, cullsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import functools\n",
    "from tf_agents.networks import nest_map\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.keras_layers import inner_reshape\n",
    "from tf_agents.networks.sequential import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 300000\n",
    "replay_buffer_max_length = 20000\n",
    "batch_size = 64\n",
    "num_eval_episodes = 200\n",
    "initial_collect_steps = 2000\n",
    "collect_steps_per_iteration = 10\n",
    "log_interval = 500\n",
    "eval_interval = 500\n",
    "target_update_period = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(Env(num_herds = num_herds, total_population = total_population, \n",
    "                                                  fix_episode_length = True, average_episode_length = 100))\n",
    "eval_env = tf_py_environment.TFPyEnvironment(Env(num_herds = num_herds, total_population = total_population, \n",
    "                                                 fix_episode_length = True, average_episode_length = 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03)\n",
    "\n",
    "#Make critic and actor net for ddpg\n",
    "\n",
    "dense = functools.partial(\n",
    "    tf.keras.layers.Dense,\n",
    "    activation=tf.keras.activations.relu,\n",
    "    kernel_initializer=tf.compat.v1.variance_scaling_initializer(\n",
    "        scale=1./ 3.0, mode='fan_in', distribution='uniform'))\n",
    "\n",
    "\n",
    "def create_identity_layer():\n",
    "    return tf.keras.layers.Lambda(lambda x: x)\n",
    "\n",
    "\n",
    "def create_fc_network(layer_units):\n",
    "    return sequential.Sequential([dense(num_units) for num_units in layer_units])\n",
    "\n",
    "\n",
    "def create_actor_network(fc_layer_units, action_spec):\n",
    "    \"\"\"Create an actor network for DDPG.\"\"\"\n",
    "    flat_action_spec = tf.nest.flatten(action_spec)\n",
    "    if len(flat_action_spec) > 1:\n",
    "        raise ValueError('Only a single action tensor is supported by this network')\n",
    "    flat_action_spec = flat_action_spec[0]\n",
    "\n",
    "    fc_layers = [dense(num_units) for num_units in fc_layer_units]\n",
    "\n",
    "    num_actions = flat_action_spec.shape.num_elements()\n",
    "    action_fc_layer = tf.keras.layers.Dense(\n",
    "        num_actions,\n",
    "        activation=tf.keras.activations.tanh,\n",
    "        kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "            minval=-0.003, maxval=0.003))\n",
    "\n",
    "    scaling_layer = tf.keras.layers.Lambda(\n",
    "        lambda x: common.scale_to_spec(x, flat_action_spec))\n",
    "    return sequential.Sequential(fc_layers + [action_fc_layer, scaling_layer])\n",
    "\n",
    "\n",
    "def create_critic_network(obs_fc_layer_units,\n",
    "                          action_fc_layer_units,\n",
    "                          joint_fc_layer_units):\n",
    "    \"\"\"Create a critic network for DDPG.\"\"\"\n",
    "\n",
    "    def split_inputs(inputs):\n",
    "        return {'observation': inputs[0], 'action': inputs[1]}\n",
    "\n",
    "    obs_network = create_fc_network(\n",
    "        obs_fc_layer_units) if obs_fc_layer_units else create_identity_layer()\n",
    "    action_network = create_fc_network(\n",
    "        action_fc_layer_units\n",
    "        ) if action_fc_layer_units else create_identity_layer()\n",
    "    joint_network = create_fc_network(\n",
    "        joint_fc_layer_units) if joint_fc_layer_units else create_identity_layer()\n",
    "    \n",
    "    value_fc_layer = tf.keras.layers.Dense(\n",
    "        1,\n",
    "        activation=None,\n",
    "        kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "            minval=-0.003, maxval=0.003))\n",
    "\n",
    "    return sequential.Sequential([\n",
    "        tf.keras.layers.Lambda(split_inputs),\n",
    "        nest_map.NestMap({\n",
    "            'observation': obs_network,\n",
    "            'action': action_network\n",
    "        }),\n",
    "        nest_map.NestFlatten(),\n",
    "        tf.keras.layers.Concatenate(),\n",
    "        joint_network,\n",
    "        value_fc_layer,\n",
    "        inner_reshape.InnerReshape([1], [])\n",
    "    ])\n",
    "\n",
    "actor_fc_layers=(200, 150)\n",
    "critic_obs_fc_layers=(200,)\n",
    "critic_action_fc_layers=None\n",
    "critic_joint_fc_layers=(150,)\n",
    "\n",
    "actor_net = create_actor_network(actor_fc_layers, train_env.action_spec())\n",
    "critic_net = create_critic_network(critic_obs_fc_layers,\n",
    "                                       critic_action_fc_layers,\n",
    "                                       critic_joint_fc_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_counter = tf.Variable(0)\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "agent = ddpg_agent.DdpgAgent(\n",
    "        train_env.time_step_spec(),\n",
    "        train_env.action_spec(),\n",
    "        actor_network=actor_net,\n",
    "        critic_network=critic_net,\n",
    "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=1e-4),\n",
    "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=1e-3),\n",
    "        ou_stddev=0.2,\n",
    "        ou_damping=0.15,\n",
    "        target_update_tau=0.05,\n",
    "        target_update_period=5,\n",
    "        #dqda_clipping=dqda_clipping,\n",
    "        td_errors_loss_fn=tf.compat.v1.losses.huber_loss,\n",
    "        gamma=0.99,\n",
    "        #reward_scale_factor=reward_scale_factor,\n",
    "        #gradient_clipping=gradient_clipping,\n",
    "        #debug_summaries=debug_summaries,\n",
    "        #summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "        train_step_counter=global_step)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually initialize a reasonably good policy: kill both herds if the sum of observations is large\n",
    "#W = np.array([[0, 3 ,0, 2],[0, 0, 3, 2,]])\n",
    "#b = np.array([1, 0, 0, 0])\n",
    "#q_net.layers[0].set_weights([W,b])\n",
    "#agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(\n",
    "    action_spec=train_env.action_spec(), time_step_spec=train_env.time_step_spec())\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(\n",
       "{action: (64, 2, 4),\n",
       " discount: (64, 2),\n",
       " next_step_type: (64, 2),\n",
       " observation: (64, 2, 7),\n",
       " policy_info: (),\n",
       " reward: (64, 2),\n",
       " step_type: (64, 2)}), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(\n",
       "{action: tf.float32,\n",
       " discount: tf.float32,\n",
       " next_step_type: tf.int32,\n",
       " observation: tf.float32,\n",
       " policy_info: (),\n",
       " reward: tf.float32,\n",
       " step_type: tf.int32}), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1: loss = 155.2799\tstep 1: average return = -14491.5 cullsteps = 100.0\n",
      "step 50: average return = -13029.7 cullsteps = 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as sequential_layer_call_and_return_conditional_losses, sequential_layer_call_fn, sequential_layer_call_fn, sequential_layer_call_and_return_conditional_losses, sequential_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100: average return = -7695.6 cullsteps = 100.0\n",
      "New best return:  tf.Tensor([-7695.5557], shape=(1,), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/100/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:558: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Masterarbeit/DDPG/policy/100/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 500: loss = 146.8252\tstep 500: average return = -44287.2 cullsteps = 0.0\n",
      "step = 1000: loss = 452.5674\tstep 1000: average return = -44797.3 cullsteps = 0.0\n",
      "step = 1500: loss = 668.3804\tstep 1500: average return = -45507.8 cullsteps = 0.0\n",
      "step = 2000: loss = 475.6338\tstep 2000: average return = -44457.6 cullsteps = 0.0\n",
      "step = 2500: loss = 327.9823\tstep 2500: average return = -44899.3 cullsteps = 0.0\n",
      "step = 3000: loss = 393.5921\tstep 3000: average return = -45116.8 cullsteps = 0.0\n",
      "step = 3500: loss = 337.7229\tstep 3500: average return = -44810.9 cullsteps = 0.0\n",
      "step = 4000: loss = 124.0544\tstep 4000: average return = -45075.3 cullsteps = 0.0\n",
      "step = 4500: loss = 220.9010\tstep 4500: average return = -44766.3 cullsteps = 0.0\n",
      "step = 5000: loss = 257.3946\tstep 5000: average return = -45069.6 cullsteps = 0.0\n",
      "step = 5500: loss = 112.0091\tstep 5500: average return = -45268.4 cullsteps = 0.0\n",
      "step = 6000: loss = 144.2850\tstep 6000: average return = -45290.2 cullsteps = 0.0\n",
      "step = 6500: loss = 570.3718\tstep 6500: average return = -44949.0 cullsteps = 0.0\n",
      "step = 7000: loss = 256.2391\tstep 7000: average return = -45315.9 cullsteps = 0.0\n",
      "step = 7500: loss = 771.6492\tstep 7500: average return = -44871.7 cullsteps = 0.0\n",
      "step = 8000: loss = 293.4711\tstep 8000: average return = -45067.3 cullsteps = 0.0\n",
      "step = 8500: loss = 578.6191\tstep 8500: average return = -45082.1 cullsteps = 0.0\n",
      "step = 9000: loss = 632.0035\tstep 9000: average return = -44833.9 cullsteps = 0.0\n",
      "step = 9500: loss = 428.1420\tstep 9500: average return = -44616.8 cullsteps = 0.0\n",
      "step = 10000: loss = 132.5407\tstep 10000: average return = -45380.9 cullsteps = 0.0\n",
      "step = 10500: loss = 378.7708\tstep 10500: average return = -44775.3 cullsteps = 0.0\n",
      "step = 11000: loss = 186.3701\tstep 11000: average return = -44793.6 cullsteps = 0.0\n",
      "step = 11500: loss = 249.7712\tstep 11500: average return = -30621.4 cullsteps = 10.5\n",
      "step = 12000: loss = 236.1634\tstep 12000: average return = -27869.4 cullsteps = 16.0\n",
      "step = 12500: loss = 359.7977\tstep 12500: average return = -44410.9 cullsteps = 0.0\n",
      "step = 13000: loss = 748.9944\tstep 13000: average return = -44902.6 cullsteps = 0.0\n",
      "step = 13500: loss = 995.0919\tstep 13500: average return = -44914.1 cullsteps = 0.0\n",
      "step = 14000: loss = 769.6581\tstep 14000: average return = -44515.9 cullsteps = 0.0\n",
      "step = 14500: loss = 229.6564\tstep 14500: average return = -44791.2 cullsteps = 0.0\n",
      "step = 15000: loss = 636.8618\tstep 15000: average return = -44705.2 cullsteps = 0.0\n",
      "step = 15500: loss = 794.2949\tstep 15500: average return = -44641.2 cullsteps = 0.0\n",
      "step = 16000: loss = 270.3897\tstep 16000: average return = -45358.7 cullsteps = 0.0\n",
      "step = 16500: loss = 526.1903\tstep 16500: average return = -45053.6 cullsteps = 0.0\n",
      "step = 17000: loss = 308.0592\tstep 17000: average return = -45444.3 cullsteps = 0.0\n",
      "step = 17500: loss = 1642.4885\tstep 17500: average return = -45102.1 cullsteps = 0.0\n",
      "step = 18000: loss = 715.7631\tstep 18000: average return = -45178.3 cullsteps = 0.0\n",
      "step = 18500: loss = 862.1692\tstep 18500: average return = -44704.8 cullsteps = 0.0\n",
      "step = 19000: loss = 256.8263\tstep 19000: average return = -45347.2 cullsteps = 0.0\n",
      "step = 19500: loss = 190.4147\tstep 19500: average return = -44473.8 cullsteps = 0.0\n",
      "step = 20000: loss = 456.3004\tstep 20000: average return = -15198.1 cullsteps = 100.0\n",
      "step = 20500: loss = 792.0186\tstep 20500: average return = -44431.3 cullsteps = 0.0\n",
      "step = 21000: loss = 675.9006\tstep 21000: average return = -45271.5 cullsteps = 0.0\n",
      "step = 21500: loss = 2052.6208\tstep 21500: average return = -44924.4 cullsteps = 0.0\n",
      "step = 22000: loss = 1559.3662\tstep 22000: average return = -44942.3 cullsteps = 0.0\n",
      "step = 22500: loss = 2257.2500\tstep 22500: average return = -45122.6 cullsteps = 0.0\n",
      "step = 23000: loss = 802.4963\tstep 23000: average return = -44661.1 cullsteps = 0.0\n",
      "step = 23500: loss = 1141.2877\tstep 23500: average return = -44885.9 cullsteps = 0.0\n",
      "step = 24000: loss = 1189.7517\tstep 24000: average return = -45263.2 cullsteps = 0.0\n",
      "step = 24500: loss = 834.6246\tstep 24500: average return = -44949.5 cullsteps = 0.0\n",
      "step = 25000: loss = 1056.0411\tstep 25000: average return = -44512.1 cullsteps = 0.0\n",
      "step = 25500: loss = 712.3325\tstep 25500: average return = -44465.2 cullsteps = 0.0\n",
      "step = 26000: loss = 468.5451\tstep 26000: average return = -45517.7 cullsteps = 0.0\n",
      "step = 26500: loss = 466.7766\tstep 26500: average return = -44892.7 cullsteps = 0.0\n",
      "step = 27000: loss = 1452.6011\tstep 27000: average return = -15974.7 cullsteps = 100.0\n",
      "step = 27500: loss = 2219.4331\tstep 27500: average return = -41229.2 cullsteps = 0.0\n",
      "step = 28000: loss = 1804.9011\tstep 28000: average return = -44621.2 cullsteps = 0.0\n",
      "step = 28500: loss = 2657.4238\tstep 28500: average return = -44549.4 cullsteps = 0.0\n",
      "step = 29000: loss = 1892.9409\tstep 29000: average return = -45125.6 cullsteps = 0.0\n",
      "step = 29500: loss = 1389.1565\tstep 29500: average return = -44584.3 cullsteps = 0.0\n",
      "step = 30000: loss = 1421.2494\tstep 30000: average return = -44945.8 cullsteps = 0.0\n",
      "step = 30500: loss = 1015.2092\tstep 30500: average return = -45029.8 cullsteps = 0.0\n",
      "step = 31000: loss = 1984.2742\tstep 31000: average return = -44870.9 cullsteps = 0.0\n",
      "step = 31500: loss = 929.8977\tstep 31500: average return = -45171.7 cullsteps = 0.0\n",
      "step = 32000: loss = 1385.9314\tstep 32000: average return = -44544.6 cullsteps = 0.0\n",
      "step = 32500: loss = 2657.9888\tstep 32500: average return = -44647.6 cullsteps = 0.0\n",
      "step = 33000: loss = 755.7778\tstep 33000: average return = -45022.0 cullsteps = 0.0\n",
      "step = 33500: loss = 2997.4062\tstep 33500: average return = -45069.6 cullsteps = 0.0\n",
      "step = 34000: loss = 949.8579\tstep 34000: average return = -45122.1 cullsteps = 0.0\n",
      "step = 34500: loss = 1898.3210\tstep 34500: average return = -25230.0 cullsteps = 100.0\n",
      "step = 35000: loss = 2439.9990\tstep 35000: average return = -45123.4 cullsteps = 0.0\n",
      "step = 35500: loss = 1944.8160\tstep 35500: average return = -44789.2 cullsteps = 0.0\n",
      "step = 36000: loss = 2146.8286\tstep 36000: average return = -44868.9 cullsteps = 0.0\n",
      "step = 36500: loss = 2702.2537\tstep 36500: average return = -45237.7 cullsteps = 0.0\n",
      "step = 37000: loss = 1882.3777\tstep 37000: average return = -44622.4 cullsteps = 0.0\n",
      "step = 37500: loss = 1307.0764\tstep 37500: average return = -45042.6 cullsteps = 0.0\n",
      "step = 38000: loss = 1449.3882\tstep 38000: average return = -45167.0 cullsteps = 0.0\n",
      "step = 38500: loss = 1622.9357\tstep 38500: average return = -44892.1 cullsteps = 0.0\n",
      "step = 39000: loss = 1787.8801\tstep 39000: average return = -45406.0 cullsteps = 0.0\n",
      "step = 39500: loss = 1395.1211\tstep 39500: average return = -44771.8 cullsteps = 0.0\n",
      "step = 40000: loss = 1391.7975\tstep 40000: average return = -45008.8 cullsteps = 0.0\n",
      "step = 40500: loss = 1703.8064\tstep 40500: average return = -45478.2 cullsteps = 0.0\n",
      "step = 41000: loss = 2192.1714\tstep 41000: average return = -45498.9 cullsteps = 0.0\n",
      "step = 41500: loss = 2666.5405\tstep 41500: average return = -45110.0 cullsteps = 0.0\n",
      "step = 42000: loss = 2751.8923\tstep 42000: average return = -44720.9 cullsteps = 0.0\n",
      "step = 42500: loss = 2172.1743\tstep 42500: average return = -15981.9 cullsteps = 100.0\n",
      "step = 43000: loss = 2779.2842\tstep 43000: average return = -28885.1 cullsteps = 11.0\n",
      "step = 43500: loss = 2602.8140\tstep 43500: average return = -44959.2 cullsteps = 0.0\n",
      "step = 44000: loss = 3192.0376\tstep 44000: average return = -44748.7 cullsteps = 0.0\n",
      "step = 44500: loss = 3329.0493\tstep 44500: average return = -44769.8 cullsteps = 0.0\n",
      "step = 45000: loss = 4029.4648\tstep 45000: average return = -44835.8 cullsteps = 0.0\n",
      "step = 45500: loss = 3004.9688\tstep 45500: average return = -44798.5 cullsteps = 0.0\n",
      "step = 46000: loss = 2177.0210\tstep 46000: average return = -45126.8 cullsteps = 0.0\n",
      "step = 46500: loss = 3461.1741\tstep 46500: average return = -45025.3 cullsteps = 0.0\n",
      "step = 47000: loss = 3109.6099\tstep 47000: average return = -44708.5 cullsteps = 0.0\n",
      "step = 47500: loss = 2195.1106\tstep 47500: average return = -44739.5 cullsteps = 0.0\n",
      "step = 48000: loss = 2903.2466\tstep 48000: average return = -44852.3 cullsteps = 0.0\n",
      "step = 48500: loss = 2940.7522\tstep 48500: average return = -44995.1 cullsteps = 0.0\n",
      "step = 49000: loss = 2559.3801\tstep 49000: average return = -16667.7 cullsteps = 100.0\n",
      "step = 49500: loss = 3044.1377\tstep 49500: average return = -16548.3 cullsteps = 100.0\n",
      "step = 50000: loss = 3421.9058\tstep 50000: average return = -44986.0 cullsteps = 0.0\n",
      "step = 50500: loss = 2810.1243\tstep 50500: average return = -44676.0 cullsteps = 0.0\n",
      "step = 51000: loss = 3364.3708\tstep 51000: average return = -44927.2 cullsteps = 0.0\n",
      "step = 51500: loss = 3500.5918\tstep 51500: average return = -44839.6 cullsteps = 0.0\n",
      "step = 52000: loss = 3129.7986\tstep 52000: average return = -45046.4 cullsteps = 0.0\n",
      "step = 52500: loss = 3260.3352\tstep 52500: average return = -44738.1 cullsteps = 0.0\n",
      "step = 53000: loss = 3459.0183\tstep 53000: average return = -45366.7 cullsteps = 0.0\n",
      "step = 53500: loss = 3691.9395\tstep 53500: average return = -14818.5 cullsteps = 100.0\n",
      "step = 54000: loss = 3411.4390\tstep 54000: average return = -45456.3 cullsteps = 0.0\n",
      "step = 54500: loss = 3099.5107\tstep 54500: average return = -44375.8 cullsteps = 0.0\n",
      "step = 55000: loss = 2309.9692\tstep 55000: average return = -45274.6 cullsteps = 0.0\n",
      "step = 55500: loss = 2371.7065\tstep 55500: average return = -44637.9 cullsteps = 0.0\n",
      "step = 56000: loss = 2497.4358\tstep 56000: average return = -44763.3 cullsteps = 0.0\n",
      "step = 56500: loss = 3105.5615\tstep 56500: average return = -16635.4 cullsteps = 100.0\n",
      "step = 57000: loss = 3001.5959\tstep 57000: average return = -45692.4 cullsteps = 0.0\n",
      "step = 57500: loss = 3873.0532\tstep 57500: average return = -44640.6 cullsteps = 0.0\n",
      "step = 58000: loss = 2116.4487\tstep 58000: average return = -44812.8 cullsteps = 0.0\n",
      "step = 58500: loss = 2447.8413\tstep 58500: average return = -44898.1 cullsteps = 0.0\n",
      "step = 59000: loss = 2598.5327\tstep 59000: average return = -45031.9 cullsteps = 0.0\n",
      "step = 59500: loss = 3470.3254\tstep 59500: average return = -45057.6 cullsteps = 0.0\n",
      "step = 60000: loss = 2931.6069\tstep 60000: average return = -45246.1 cullsteps = 0.0\n",
      "step = 60500: loss = 2261.7170\tstep 60500: average return = -45137.0 cullsteps = 0.0\n",
      "step = 61000: loss = 3047.4263\tstep 61000: average return = -45175.9 cullsteps = 0.0\n",
      "step = 61500: loss = 3412.6104\tstep 61500: average return = -45004.2 cullsteps = 0.0\n",
      "step = 62000: loss = 3637.3059\tstep 62000: average return = -44799.2 cullsteps = 0.0\n",
      "step = 62500: loss = 4560.7515\tstep 62500: average return = -25148.8 cullsteps = 100.0\n",
      "step = 63000: loss = 4278.6729\tstep 63000: average return = -44902.4 cullsteps = 0.0\n",
      "step = 63500: loss = 5768.2007\tstep 63500: average return = -44780.0 cullsteps = 0.0\n",
      "step = 64000: loss = 4644.9243\tstep 64000: average return = -44860.6 cullsteps = 0.0\n",
      "step = 64500: loss = 5028.8447\tstep 64500: average return = -45209.7 cullsteps = 0.0\n",
      "step = 65000: loss = 4463.4785\tstep 65000: average return = -44666.1 cullsteps = 0.0\n",
      "step = 65500: loss = 5341.2354\tstep 65500: average return = -44465.1 cullsteps = 0.0\n",
      "step = 66000: loss = 4908.7114\tstep 66000: average return = -44630.4 cullsteps = 0.0\n",
      "step = 66500: loss = 5629.2466\tstep 66500: average return = -45273.0 cullsteps = 0.0\n",
      "step = 67000: loss = 5404.0483\tstep 67000: average return = -44898.2 cullsteps = 0.0\n",
      "step = 67500: loss = 6048.9097\tstep 67500: average return = -44488.1 cullsteps = 0.0\n",
      "step = 68000: loss = 6531.7065\tstep 68000: average return = -13134.6 cullsteps = 100.0\n",
      "step = 68500: loss = 7580.2563\tstep 68500: average return = -25044.8 cullsteps = 100.0\n",
      "step = 69000: loss = 9469.4707\tstep 69000: average return = -44508.2 cullsteps = 0.0\n",
      "step = 69500: loss = 10530.0020\tstep 69500: average return = -44806.2 cullsteps = 0.0\n",
      "step = 70000: loss = 12378.5146\tstep 70000: average return = -45172.3 cullsteps = 0.0\n",
      "step = 70500: loss = 14107.6836\tstep 70500: average return = -45067.6 cullsteps = 0.0\n",
      "step = 71000: loss = 11940.8027\tstep 71000: average return = -45299.9 cullsteps = 0.0\n",
      "step = 71500: loss = 11055.4336\tstep 71500: average return = -44954.7 cullsteps = 0.0\n",
      "step = 72000: loss = 10841.1270\tstep 72000: average return = -44847.5 cullsteps = 0.0\n",
      "step = 72500: loss = 11825.8018\tstep 72500: average return = -45241.2 cullsteps = 0.0\n",
      "step = 73000: loss = 11326.5439\tstep 73000: average return = -44997.0 cullsteps = 0.0\n",
      "step = 73500: loss = 12452.5430\tstep 73500: average return = -45104.5 cullsteps = 0.0\n",
      "step = 74000: loss = 13268.1064\tstep 74000: average return = -15034.1 cullsteps = 100.0\n",
      "step = 74500: loss = 16087.0654\tstep 74500: average return = -24247.1 cullsteps = 100.0\n",
      "step = 75000: loss = 18458.0645\tstep 75000: average return = -44383.3 cullsteps = 0.0\n",
      "step = 75500: loss = 20734.6465\tstep 75500: average return = -44106.9 cullsteps = 0.0\n",
      "step = 76000: loss = 21130.5625\tstep 76000: average return = -44746.9 cullsteps = 0.0\n",
      "step = 76500: loss = 21596.1484\tstep 76500: average return = -45024.0 cullsteps = 0.0\n",
      "step = 77000: loss = 20840.6289\tstep 77000: average return = -44795.9 cullsteps = 0.0\n",
      "step = 77500: loss = 21211.3926\tstep 77500: average return = -44892.3 cullsteps = 0.0\n",
      "step = 78000: loss = 24560.9023\tstep 78000: average return = -44781.2 cullsteps = 0.0\n",
      "step = 78500: loss = 28795.1230\tstep 78500: average return = -44830.6 cullsteps = 0.0\n",
      "step = 79000: loss = 33736.4883\tstep 79000: average return = -44728.1 cullsteps = 0.0\n",
      "step = 79500: loss = 37243.5586\tstep 79500: average return = -45023.5 cullsteps = 0.0\n",
      "step = 80000: loss = 40369.8320\tstep 80000: average return = -45137.8 cullsteps = 0.0\n",
      "step = 80500: loss = 38991.7617\tstep 80500: average return = -44767.0 cullsteps = 0.0\n",
      "step = 81000: loss = 44616.2969\tstep 81000: average return = -45105.3 cullsteps = 0.0\n",
      "step = 81500: loss = 45608.6250\tstep 81500: average return = -44778.6 cullsteps = 0.0\n",
      "step = 82000: loss = 48902.2383\tstep 82000: average return = -23920.6 cullsteps = 100.0\n",
      "step = 82500: loss = 51088.5664\tstep 82500: average return = -45154.1 cullsteps = 0.0\n",
      "step = 83000: loss = 46823.5117\tstep 83000: average return = -44670.0 cullsteps = 0.0\n",
      "step = 83500: loss = 48659.3477\tstep 83500: average return = -44877.6 cullsteps = 0.0\n",
      "step = 84000: loss = 45351.9570\tstep 84000: average return = -45001.2 cullsteps = 0.0\n",
      "step = 84500: loss = 42687.7031\tstep 84500: average return = -44895.9 cullsteps = 0.0\n",
      "step = 85000: loss = 38907.0430\tstep 85000: average return = -44718.6 cullsteps = 0.0\n",
      "step = 85500: loss = 38562.7539\tstep 85500: average return = -44908.3 cullsteps = 0.0\n",
      "step = 86000: loss = 37490.0898\tstep 86000: average return = -45175.8 cullsteps = 0.0\n",
      "step = 86500: loss = 28214.8105\tstep 86500: average return = -44883.7 cullsteps = 0.0\n",
      "step = 87000: loss = 29693.0020\tstep 87000: average return = -44947.1 cullsteps = 0.0\n",
      "step = 87500: loss = 29275.9531\tstep 87500: average return = -44921.1 cullsteps = 0.0\n",
      "step = 88000: loss = 30454.8438\tstep 88000: average return = -45407.1 cullsteps = 0.0\n",
      "step = 88500: loss = 20683.4766\tstep 88500: average return = -44811.5 cullsteps = 0.0\n",
      "step = 89000: loss = 25148.5625\tstep 89000: average return = -44896.9 cullsteps = 0.0\n",
      "step = 89500: loss = 17784.3984\tstep 89500: average return = -44872.5 cullsteps = 0.0\n",
      "step = 90000: loss = 15615.2197\tstep 90000: average return = -45153.9 cullsteps = 0.0\n",
      "step = 90500: loss = 8849.4463\tstep 90500: average return = -44783.3 cullsteps = 0.0\n",
      "step = 91000: loss = 10473.7021\tstep 91000: average return = -45413.3 cullsteps = 0.0\n",
      "step = 91500: loss = 4974.8813\tstep 91500: average return = -44974.0 cullsteps = 0.0\n",
      "step = 92000: loss = 8369.4746\tstep 92000: average return = -44971.6 cullsteps = 0.0\n",
      "step = 92500: loss = 6458.4668\tstep 92500: average return = -44833.4 cullsteps = 0.0\n",
      "step = 93000: loss = 7026.6851\tstep 93000: average return = -45149.5 cullsteps = 0.0\n",
      "step = 93500: loss = 7141.6099\tstep 93500: average return = -45129.9 cullsteps = 0.0\n",
      "step = 94000: loss = 4589.2383\tstep 94000: average return = -45214.2 cullsteps = 0.0\n",
      "step = 94500: loss = 4963.6724\tstep 94500: average return = -45137.3 cullsteps = 0.0\n",
      "step = 95000: loss = 4389.5620\tstep 95000: average return = -45188.2 cullsteps = 0.0\n",
      "step = 95500: loss = 3613.8088\tstep 95500: average return = -45327.1 cullsteps = 0.0\n",
      "step = 96000: loss = 3091.9819\tstep 96000: average return = -44902.4 cullsteps = 0.0\n",
      "step = 96500: loss = 2796.7842\tstep 96500: average return = -44686.2 cullsteps = 0.0\n",
      "step = 97000: loss = 2863.0522\tstep 97000: average return = -45211.4 cullsteps = 0.0\n",
      "step = 97500: loss = 2869.1628\tstep 97500: average return = -44801.9 cullsteps = 0.0\n",
      "step = 98000: loss = 2970.0596\tstep 98000: average return = -10881.7 cullsteps = 70.0\n",
      "step = 98500: loss = 3430.4189\tstep 98500: average return = -44974.5 cullsteps = 0.0\n",
      "step = 99000: loss = 3276.5391\tstep 99000: average return = -44830.2 cullsteps = 0.0\n",
      "step = 99500: loss = 3794.4082\tstep 99500: average return = -45220.6 cullsteps = 0.0\n",
      "step = 100000: loss = 3610.7466\tstep 100000: average return = -44694.5 cullsteps = 0.0\n",
      "step = 100500: loss = 2790.9102\tstep 100500: average return = -45496.7 cullsteps = 0.0\n",
      "step = 101000: loss = 2575.6860\tstep 101000: average return = -45316.2 cullsteps = 0.0\n",
      "step = 101500: loss = 2335.2925\tstep 101500: average return = -44902.2 cullsteps = 0.0\n",
      "step = 102000: loss = 2721.7397\tstep 102000: average return = -44837.4 cullsteps = 0.0\n",
      "step = 102500: loss = 3746.5166\tstep 102500: average return = -44808.8 cullsteps = 0.0\n",
      "step = 103000: loss = 3823.2300\tstep 103000: average return = -44852.2 cullsteps = 0.0\n",
      "step = 103500: loss = 3595.4067\tstep 103500: average return = -44754.4 cullsteps = 0.0\n",
      "step = 104000: loss = 3904.1836\tstep 104000: average return = -44920.0 cullsteps = 0.0\n",
      "step = 104500: loss = 4458.4409\tstep 104500: average return = -44966.6 cullsteps = 0.0\n",
      "step = 105000: loss = 4938.4624\tstep 105000: average return = -44776.9 cullsteps = 0.0\n",
      "step = 105500: loss = 4982.4468\tstep 105500: average return = -45100.7 cullsteps = 0.0\n",
      "step = 106000: loss = 4498.1094\tstep 106000: average return = -44839.3 cullsteps = 0.0\n",
      "step = 106500: loss = 6224.9624\tstep 106500: average return = -45114.4 cullsteps = 0.0\n",
      "step = 107000: loss = 6737.6455\tstep 107000: average return = -45093.2 cullsteps = 0.0\n",
      "step = 107500: loss = 7083.1899\tstep 107500: average return = -44520.2 cullsteps = 0.0\n",
      "step = 108000: loss = 7559.1475\tstep 108000: average return = -44988.6 cullsteps = 0.0\n",
      "step = 108500: loss = 7094.7578\tstep 108500: average return = -44805.4 cullsteps = 0.0\n",
      "step = 109000: loss = 9072.9121\tstep 109000: average return = -45252.0 cullsteps = 0.0\n",
      "step = 109500: loss = 6805.1787\tstep 109500: average return = -44506.9 cullsteps = 0.0\n",
      "step = 110000: loss = 8780.6533\tstep 110000: average return = -44517.4 cullsteps = 0.0\n",
      "step = 110500: loss = 8772.7646\tstep 110500: average return = -45045.4 cullsteps = 0.0\n",
      "step = 111000: loss = 9744.6133\tstep 111000: average return = -44574.0 cullsteps = 0.0\n",
      "step = 111500: loss = 10410.7920\tstep 111500: average return = -44652.1 cullsteps = 0.0\n",
      "step = 112000: loss = 11044.1846\tstep 112000: average return = -44394.4 cullsteps = 0.0\n",
      "step = 112500: loss = 10845.2646\tstep 112500: average return = -45066.9 cullsteps = 0.0\n",
      "step = 113000: loss = 11838.4355\tstep 113000: average return = -45178.6 cullsteps = 0.0\n",
      "step = 113500: loss = 9734.9961\tstep 113500: average return = -45120.6 cullsteps = 0.0\n",
      "step = 114000: loss = 13564.4316\tstep 114000: average return = -45545.0 cullsteps = 0.0\n",
      "step = 114500: loss = 14122.0566\tstep 114500: average return = -44419.9 cullsteps = 0.0\n",
      "step = 115000: loss = 15011.6816\tstep 115000: average return = -44709.4 cullsteps = 0.0\n",
      "step = 115500: loss = 14842.1934\tstep 115500: average return = -44947.2 cullsteps = 0.0\n",
      "step = 116000: loss = 13577.9482\tstep 116000: average return = -44839.4 cullsteps = 0.0\n",
      "step = 116500: loss = 11686.3271\tstep 116500: average return = -44856.0 cullsteps = 0.0\n",
      "step = 117000: loss = 15149.3975\tstep 117000: average return = -45669.6 cullsteps = 0.0\n",
      "step = 117500: loss = 15486.5146\tstep 117500: average return = -44840.2 cullsteps = 0.0\n",
      "step = 118000: loss = 15701.3359\tstep 118000: average return = -45126.1 cullsteps = 0.0\n",
      "step = 118500: loss = 10555.0254\tstep 118500: average return = -44930.1 cullsteps = 0.0\n",
      "step = 119000: loss = 12372.9326\tstep 119000: average return = -44995.4 cullsteps = 0.0\n",
      "step = 119500: loss = 11578.6572\tstep 119500: average return = -44606.8 cullsteps = 0.0\n",
      "step = 120000: loss = 11715.5459\tstep 120000: average return = -45143.4 cullsteps = 0.0\n",
      "step = 120500: loss = 14137.1611\tstep 120500: average return = -44691.4 cullsteps = 0.0\n",
      "step = 121000: loss = 15547.3457\tstep 121000: average return = -45066.6 cullsteps = 0.0\n",
      "step = 121500: loss = 11616.7783\tstep 121500: average return = -44635.1 cullsteps = 0.0\n",
      "step = 122000: loss = 13266.8936\tstep 122000: average return = -45009.7 cullsteps = 0.0\n",
      "step = 122500: loss = 8408.6318\tstep 122500: average return = -44978.5 cullsteps = 0.0\n",
      "step = 123000: loss = 10196.4111\tstep 123000: average return = -44989.3 cullsteps = 0.0\n",
      "step = 123500: loss = 9496.9365\tstep 123500: average return = -44717.3 cullsteps = 0.0\n",
      "step = 124000: loss = 6960.4570\tstep 124000: average return = -44614.2 cullsteps = 0.0\n",
      "step = 124500: loss = 10776.0771\tstep 124500: average return = -44907.1 cullsteps = 0.0\n",
      "step = 125000: loss = 2705.5730\tstep 125000: average return = -45252.7 cullsteps = 0.0\n",
      "step = 125500: loss = 5652.1802\tstep 125500: average return = -44867.2 cullsteps = 0.0\n",
      "step = 126000: loss = 3992.7432\tstep 126000: average return = -44879.2 cullsteps = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "# Initialize policy saver\n",
    "saver = policy_saver.PolicySaver(agent.policy)\n",
    "best_return = -10000\n",
    "root_dir = os.path.expanduser(root_dir)\n",
    "policy_dir = os.path.join(root_dir, 'policy')\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step <= 1 or step % log_interval == 0:\n",
    "    print('step = {0:4>}: loss = {1:.4f}'.format(step, train_loss), end=\"\\t\")\n",
    "\n",
    "  if step <= 1 or (step <= 100 and step % 50 == 0) or step % eval_interval == 0:\n",
    "    avg_return, cullsteps = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step {0}: average return = {1:.1f} cullsteps = {2:.1f}'.format(step, avg_return.numpy().item(), cullsteps))\n",
    "    returns.append(avg_return)\n",
    "    if avg_return > best_return:\n",
    "        if avg_return > -300:\n",
    "            best_return = avg_return\n",
    "            print('Final best return: ', best_return)\n",
    "            saver.save(os.path.join(policy_dir, str(global_step.numpy())))\n",
    "            break\n",
    "        else:\n",
    "            best_return = avg_return\n",
    "            print('New best return: ', best_return)\n",
    "            saver.save(os.path.join(policy_dir, str(global_step.numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_reward, culleps = compute_avg_return(eval_env, agent.policy, num_episodes=500)\n",
    "print (\"reward of learned policy: \", learned_reward.numpy(), \"cullsteps=\", culleps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ts = eval_env.reset()\n",
    "\n",
    "def get_action(obs):\n",
    "    \"\"\" execute the learned policy network \n",
    "       obs:  one float for global time, one float for each herd - the time since last culling\n",
    "    \"\"\"\n",
    "    _ts = TimeStep(tf.constant([0.]),\n",
    "                   tf.constant([0.]),\n",
    "                   tf.constant([1]),\n",
    "                   tf.constant([obs]))\n",
    "    # a = agent.collect_policy.action(_ts) # just to see how much is explored versus exploited\n",
    "    a = agent.policy.action(_ts)\n",
    "    return a.action.numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what the learned policy does on a grid of observations (5 steps per row&col)\n",
    "'''A = [[get_action([.0, x,y])\n",
    " for y in np.arange(0.,1.,.05,np.float32)]\n",
    " for x in np.arange(0.,1.,.05,np.float32)]\n",
    "A'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with parameters of manually designed q_network policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W, b = agent.policy.trainable_variables\n",
    "# W = W.numpy()\n",
    "# b = b.numpy()\n",
    "# print (\"weights\\n\", W, \"\\nbias\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nn(obs):\n",
    "#    y = np.dot(obs, W)+b\n",
    "#    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn([0.5,.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc-autonumbering": true,
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
