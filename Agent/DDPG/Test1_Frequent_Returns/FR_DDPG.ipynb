{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the environment `HerdEnv` of `herd_env.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "from tf_agents.trajectories import TimeStep\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FR_Env import FREnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "max_episode_length=10000\n",
    "henv_val = FREnv(herd_sizes = [32,32], expected_episode_length=100, max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.02, rand_infection_prob = 0.06)\n",
    "utils.validate_py_environment(henv_val, episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Herd Environment instance to be trained for\n",
    "max_episode_length=10000\n",
    "num_herds=2\n",
    "henv = FREnv(herd_sizes = [32,32], expected_episode_length=100, max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.02, rand_infection_prob = 0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': 1.0,\n",
      " 'observation': array([0., 0., 0.], dtype=float32),\n",
      " 'reward': 0.0,\n",
      " 'step_type': array(0, dtype=int32)})\n",
      "state:  [2 1 2 1 1 1] observation:  [1.e-04 1.e-04 1.e-04] \treward:  -0.075\n",
      "Final Reward =  -0.075\n"
     ]
    }
   ],
   "source": [
    "# show interor values of environment\n",
    "time_step = henv.reset()\n",
    "print(time_step)\n",
    "cumulative_reward = time_step.reward\n",
    "finished = False\n",
    "\n",
    "while not finished:\n",
    "  time_step = henv.step([0.,0.]) # do nothing\n",
    "  s = henv.get_state()\n",
    "  print(\"state: \", s, \"observation: \", time_step.observation, \"\\treward: \", time_step.reward)\n",
    "  cumulative_reward += time_step.reward\n",
    "  if time_step.step_type == StepType.LAST:\n",
    "    finished = True\n",
    "\n",
    "print('Final Reward = ', cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action spec:\n",
      " BoundedArraySpec(shape=(2,), dtype=dtype('float32'), name='action', minimum=0.0, maximum=1.0) \n",
      "\n",
      "time step spec:\n",
      " TimeStep(\n",
      "{'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
      " 'observation': BoundedArraySpec(shape=(3,), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=1.0),\n",
      " 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n",
      " 'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "action_spec = henv.action_spec()\n",
    "ts_spec = henv.time_step_spec()\n",
    "print(\"action spec:\\n\", action_spec, \"\\n\\ntime step spec:\\n\", ts_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scripted policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do nothing policy: cull never\n",
    "action_script0 = [(max_episode_length, [0,0])]\n",
    "\n",
    "# cull first herd every 20th step and second herd every 20th step\n",
    "action_script1 = [(19, [0,0]), \n",
    "                 (1, [1,1]),\n",
    "                 (19, [0,0]), \n",
    "                 (1, [1,1])] * int(1+max_episode_length/40)\n",
    "\n",
    "manual_scripted_policy0 = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=ts_spec,\n",
    "    action_spec=action_spec,\n",
    "    action_script=action_script0)\n",
    "\n",
    "manual_scripted_policy1 = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=ts_spec,\n",
    "    action_spec=action_spec,\n",
    "    action_script=action_script1)\n",
    "\n",
    "init_policy_state = manual_scripted_policy0.get_initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action= [0. 0.] \tpolicy_state [0, 1]\n",
      "action= [0. 0.] \tpolicy_state [0, 2]\n",
      "action= [0. 0.] \tpolicy_state [0, 3]\n",
      "action= [0. 0.] \tpolicy_state [0, 4]\n",
      "action= [0. 0.] \tpolicy_state [0, 5]\n",
      "action= [0. 0.] \tpolicy_state [0, 6]\n",
      "action= [0. 0.] \tpolicy_state [0, 7]\n",
      "action= [0. 0.] \tpolicy_state [0, 8]\n",
      "action= [0. 0.] \tpolicy_state [0, 9]\n",
      "action= [0. 0.] \tpolicy_state [0, 10]\n",
      "action= [0. 0.] \tpolicy_state [0, 11]\n",
      "action= [0. 0.] \tpolicy_state [0, 12]\n",
      "action= [0. 0.] \tpolicy_state [0, 13]\n",
      "action= [0. 0.] \tpolicy_state [0, 14]\n",
      "action= [0. 0.] \tpolicy_state [0, 15]\n",
      "action= [0. 0.] \tpolicy_state [0, 16]\n",
      "action= [0. 0.] \tpolicy_state [0, 17]\n",
      "action= [0. 0.] \tpolicy_state [0, 18]\n",
      "action= [0. 0.] \tpolicy_state [0, 19]\n",
      "action= [1. 1.] \tpolicy_state [1, 1]\n",
      "action= [0. 0.] \tpolicy_state [2, 1]\n"
     ]
    }
   ],
   "source": [
    "policy_state =  init_policy_state\n",
    "ts0 = henv.reset()\n",
    "for _ in range(21):\n",
    "    action_step = manual_scripted_policy1.action(ts0, policy_state)\n",
    "    policy_state = action_step.state\n",
    "    print(\"action=\", action_step.action, \"\\tpolicy_state\", policy_state)\n",
    "policy_state = manual_scripted_policy1.get_initial_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_policy = random_py_policy.RandomPyPolicy(time_step_spec=ts_spec, action_spec=action_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drive a rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=50, verbose=False):\n",
    "  total_return = 0.0\n",
    "  cullsteps = 0\n",
    "  for e in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "        policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "    else:\n",
    "        policy_state = None # other policies without memory\n",
    "    episode_return = 0.0\n",
    "    i=0\n",
    "    while not time_step.is_last():\n",
    "        i+=1\n",
    "        action_step = policy.action(time_step, policy_state)\n",
    "        if action_step.action[0] > 0 or action_step.action[1] > 0:\n",
    "            cullsteps += 1\n",
    "        policy_state = action_step.state\n",
    "        time_step = environment.step(action_step.action)\n",
    "        if isinstance(environment, FREnv):\n",
    "            state = environment.get_state()\n",
    "        else:\n",
    "            state = None # TF environment from wrapper does not have get_state()\n",
    "        episode_return += time_step.reward\n",
    "        if verbose:\n",
    "            print (f\"episode {e:>2} step{i:>4} action: \", action_step.action, \"state=\", state, \"obs=\", time_step.observation, \"reward=\", time_step.reward)\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  cullsteps /= num_episodes\n",
    "  return avg_return, cullsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average return of random policy: -5022.593 avg steps with culls per episode: 117.62\n"
     ]
    }
   ],
   "source": [
    "random_reward, cullsteps = compute_avg_return(henv, random_policy)\n",
    "print (f\"average return of random policy: {random_reward:.3f} avg steps with culls per episode: {cullsteps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 step   1 action:  [0. 0.] state= [2 2 2 2 1 1] obs= [1.e-04 1.e-04 1.e-04] reward= -0.1\n",
      "episode  0 step   2 action:  [0. 0.] state= [5 4 7 6 2 2] obs= [0.0002 0.0002 0.0002] reward= -0.325\n",
      "episode  0 step   3 action:  [0. 0.] state= [ 8  4 15 10  3  3] obs= [0.0003 0.0003 0.0003] reward= -0.625\n",
      "episode  0 step   4 action:  [0. 0.] state= [ 9  5 24 15  4  4] obs= [0.0004 0.0004 0.0004] reward= -0.975\n",
      "episode  0 step   5 action:  [0. 0.] state= [12  8 36 23  5  5] obs= [0.0005 0.0005 0.0005] reward= -1.475\n",
      "episode  0 step   6 action:  [0. 0.] state= [12 11 48 34  6  6] obs= [0.0006 0.0006 0.0006] reward= -2.05\n",
      "episode  0 step   7 action:  [0. 0.] state= [13 12 61 46  7  7] obs= [0.0007 0.0007 0.0007] reward= -2.675\n",
      "episode  0 step   8 action:  [0. 0.] state= [12 14 73 60  8  8] obs= [0.0008 0.0008 0.0008] reward= -3.325\n",
      "episode  0 step   9 action:  [0. 0.] state= [12 16 85 76  9  9] obs= [0.0009 0.0009 0.0009] reward= -4.025\n",
      "episode  0 step  10 action:  [0. 0.] state= [12 17 97 93 10 10] obs= [0.001 0.001 0.001] reward= -4.75\n",
      "episode  0 step  11 action:  [0. 0.] state= [ 11  17 108 110  11  11] obs= [0.0011 0.0011 0.0011] reward= -5.45\n",
      "episode  0 step  12 action:  [0. 0.] state= [ 11  17 119 127  12  12] obs= [0.0012 0.0012 0.0012] reward= -6.15\n",
      "episode  0 step  13 action:  [0. 0.] state= [ 13  18 132 145  13  13] obs= [0.0013 0.0013 0.0013] reward= -6.925\n",
      "episode  0 step  14 action:  [0. 0.] state= [ 13  20 145 165  14  14] obs= [0.0014 0.0014 0.0014] reward= -7.75\n",
      "episode  0 step  15 action:  [0. 0.] state= [ 13  22 158 187  15  15] obs= [0.0015 0.0015 0.0015] reward= -8.625\n",
      "episode  0 step  16 action:  [0. 0.] state= [ 15  23 173 210  16  16] obs= [0.0016 0.0016 0.0016] reward= -9.575\n",
      "episode  0 step  17 action:  [0. 0.] state= [ 14  20 187 230  17  17] obs= [0.0017 0.0017 0.0017] reward= -10.425\n",
      "episode  0 step  18 action:  [0. 0.] state= [ 17  21 204 251  18  18] obs= [0.0018 0.0018 0.0018] reward= -11.375\n",
      "episode  0 step  19 action:  [0. 0.] state= [ 17  23 221 274  19  19] obs= [0.0019 0.0019 0.0019] reward= -12.375\n",
      "episode  0 step  20 action:  [1. 1.] state= [  0   0 221 274   0   0] obs= [0.002 0.    0.   ] reward= -76.375\n",
      "episode  0 step  21 action:  [0. 0.] state= [  2   1 223 275   1   1] obs= [2.1e-03 1.0e-04 1.0e-04] reward= -12.45\n",
      "episode  0 step  22 action:  [0. 0.] state= [  4   6 227 281   2   2] obs= [0.0022 0.0002 0.0002] reward= -12.7\n",
      "episode  0 step  23 action:  [0. 0.] state= [  7   5 234 286   3   3] obs= [0.0023 0.0003 0.0003] reward= -13.0\n",
      "episode  0 step  24 action:  [0. 0.] state= [ 10   6 244 292   4   4] obs= [0.0024 0.0004 0.0004] reward= -13.4\n",
      "episode  0 step  25 action:  [0. 0.] state= [ 11   8 255 300   5   5] obs= [0.0025 0.0005 0.0005] reward= -13.875\n",
      "episode  0 step  26 action:  [0. 0.] state= [ 11  10 266 310   6   6] obs= [0.0026 0.0006 0.0006] reward= -14.4\n",
      "episode  0 step  27 action:  [0. 0.] state= [ 12  13 278 323   7   7] obs= [0.0027 0.0007 0.0007] reward= -15.025\n",
      "episode  0 step  28 action:  [0. 0.] state= [ 13  14 291 337   8   8] obs= [0.0028 0.0008 0.0008] reward= -15.7\n",
      "episode  0 step  29 action:  [0. 0.] state= [ 14  15 305 352   9   9] obs= [0.0029 0.0009 0.0009] reward= -16.425\n",
      "episode  0 step  30 action:  [0. 0.] state= [ 15  13 320 365  10  10] obs= [0.003 0.001 0.001] reward= -17.125\n",
      "episode  0 step  31 action:  [0. 0.] state= [ 15  13 335 378  11  11] obs= [0.0031 0.0011 0.0011] reward= -17.825\n",
      "episode  0 step  32 action:  [0. 0.] state= [ 16  13 351 391  12  12] obs= [0.0032 0.0012 0.0012] reward= -18.55\n",
      "episode  0 step  33 action:  [0. 0.] state= [ 18  13 369 404  13  13] obs= [0.0033 0.0013 0.0013] reward= -19.325\n",
      "episode  0 step  34 action:  [0. 0.] state= [ 21  15 390 419  14  14] obs= [0.0034 0.0014 0.0014] reward= -20.225\n",
      "episode  0 step  35 action:  [0. 0.] state= [ 22  13 412 432  15  15] obs= [0.0035 0.0015 0.0015] reward= -21.1\n",
      "episode  0 step  36 action:  [0. 0.] state= [ 22  13 434 445  16  16] obs= [0.0036 0.0016 0.0016] reward= -21.975\n",
      "episode  0 step  37 action:  [0. 0.] state= [ 22  12 456 457  17  17] obs= [0.0037 0.0017 0.0017] reward= -22.825\n",
      "episode  0 step  38 action:  [0. 0.] state= [ 25  16 481 473  18  18] obs= [0.0038 0.0018 0.0018] reward= -23.85\n",
      "episode  0 step  39 action:  [0. 0.] state= [ 25  19 506 492  19  19] obs= [0.0039 0.0019 0.0019] reward= -24.95\n",
      "episode  0 step  40 action:  [1. 1.] state= [  0   0 506 492   0   0] obs= [0.004 0.    0.   ] reward= -88.95\n",
      "episode  0 step  41 action:  [0. 0.] state= [  0   2 506 494   1   1] obs= [4.1e-03 1.0e-04 1.0e-04] reward= -25.0\n",
      "episode  0 step  42 action:  [0. 0.] state= [  5   3 511 497   2   2] obs= [0.0042 0.0002 0.0002] reward= -25.2\n",
      "episode  0 step  43 action:  [0. 0.] state= [  8   5 519 502   3   3] obs= [0.0043 0.0003 0.0003] reward= -25.525\n",
      "episode  0 step  44 action:  [0. 0.] state= [ 10   5 529 507   4   4] obs= [0.0044 0.0004 0.0004] reward= -25.9\n",
      "episode  0 step  45 action:  [0. 0.] state= [ 10   6 539 513   5   5] obs= [0.0045 0.0005 0.0005] reward= -26.3\n",
      "episode  0 step  46 action:  [0. 0.] state= [ 11   8 550 521   6   6] obs= [0.0046 0.0006 0.0006] reward= -26.775\n",
      "episode  0 step  47 action:  [0. 0.] state= [ 12   9 562 530   7   7] obs= [0.0047 0.0007 0.0007] reward= -27.3\n",
      "episode  0 step  48 action:  [0. 0.] state= [ 15  11 577 541   8   8] obs= [0.0048 0.0008 0.0008] reward= -27.95\n",
      "episode  0 step  49 action:  [0. 0.] state= [ 15  16 592 557   9   9] obs= [0.0049 0.0009 0.0009] reward= -28.725\n",
      "episode  0 step  50 action:  [0. 0.] state= [ 15  16 607 573  10  10] obs= [0.005 0.001 0.001] reward= -29.5\n",
      "episode  0 step  51 action:  [0. 0.] state= [ 16  17 623 590  11  11] obs= [0.0051 0.0011 0.0011] reward= -30.325\n",
      "episode  0 step  52 action:  [0. 0.] state= [ 18  19 641 609  12  12] obs= [0.0052 0.0012 0.0012] reward= -31.25\n",
      "episode  0 step  53 action:  [0. 0.] state= [ 22  20 663 629  13  13] obs= [0.0053 0.0013 0.0013] reward= -32.3\n",
      "episode  0 step  54 action:  [0. 0.] state= [ 23  23 686 652  14  14] obs= [0.0054 0.0014 0.0014] reward= -33.45\n",
      "episode  0 step  55 action:  [0. 0.] state= [ 24  24 710 676  15  15] obs= [0.0055 0.0015 0.0015] reward= -34.65\n",
      "episode  0 step  56 action:  [0. 0.] state= [ 24  26 734 702  16  16] obs= [0.0056 0.0016 0.0016] reward= -35.9\n",
      "episode  0 step  57 action:  [0. 0.] state= [ 25  26 759 728  17  17] obs= [0.0057 0.0017 0.0017] reward= -37.175\n",
      "episode  0 step  58 action:  [0. 0.] state= [ 25  25 784 753  18  18] obs= [0.0058 0.0018 0.0018] reward= -38.425\n",
      "episode  0 step  59 action:  [0. 0.] state= [ 25  25 809 778  19  19] obs= [0.0059 0.0019 0.0019] reward= -39.675\n",
      "episode  0 step  60 action:  [1. 1.] state= [  0   0 809 778   0   0] obs= [0.006 0.    0.   ] reward= -103.675\n",
      "episode  0 step  61 action:  [0. 0.] state= [  3   2 812 780   1   1] obs= [6.1e-03 1.0e-04 1.0e-04] reward= -39.8\n",
      "episode  0 step  62 action:  [0. 0.] state= [  9   4 821 784   2   2] obs= [0.0062 0.0002 0.0002] reward= -40.125\n",
      "episode  0 step  63 action:  [0. 0.] state= [  9   7 830 791   3   3] obs= [0.0063 0.0003 0.0003] reward= -40.525\n",
      "episode  0 step  64 action:  [0. 0.] state= [  9   9 839 800   4   4] obs= [0.0064 0.0004 0.0004] reward= -40.975\n",
      "episode  0 step  65 action:  [0. 0.] state= [ 10   9 849 809   5   5] obs= [0.0065 0.0005 0.0005] reward= -41.45\n",
      "episode  0 step  66 action:  [0. 0.] state= [ 11  11 860 820   6   6] obs= [0.0066 0.0006 0.0006] reward= -42.0\n",
      "episode  0 step  67 action:  [0. 0.] state= [ 13  13 873 833   7   7] obs= [0.0067 0.0007 0.0007] reward= -42.65\n",
      "episode  0 step  68 action:  [0. 0.] state= [ 13  14 886 847   8   8] obs= [0.0068 0.0008 0.0008] reward= -43.325\n",
      "episode  0 step  69 action:  [0. 0.] state= [ 14  14 900 861   9   9] obs= [0.0069 0.0009 0.0009] reward= -44.025\n",
      "episode  0 step  70 action:  [0. 0.] state= [ 16  16 916 877  10  10] obs= [0.007 0.001 0.001] reward= -44.825\n",
      "episode  0 step  71 action:  [0. 0.] state= [ 17  16 933 893  11  11] obs= [0.0071 0.0011 0.0011] reward= -45.65\n",
      "episode  0 step  72 action:  [0. 0.] state= [ 17  16 950 909  12  12] obs= [0.0072 0.0012 0.0012] reward= -46.475\n",
      "episode  0 step  73 action:  [0. 0.] state= [ 17  17 967 926  13  13] obs= [0.0073 0.0013 0.0013] reward= -47.325\n",
      "episode  0 step  74 action:  [0. 0.] state= [ 17  17 984 943  14  14] obs= [0.0074 0.0014 0.0014] reward= -48.175\n",
      "episode  0 step  75 action:  [0. 0.] state= [  16   18 1000  961   15   15] obs= [0.0075 0.0015 0.0015] reward= -49.025\n",
      "episode  0 step  76 action:  [0. 0.] state= [  18   18 1018  979   16   16] obs= [0.0076 0.0016 0.0016] reward= -49.925\n",
      "episode  0 step  77 action:  [0. 0.] state= [  19   18 1037  997   17   17] obs= [0.0077 0.0017 0.0017] reward= -50.85\n",
      "episode  0 step  78 action:  [0. 0.] state= [  20   20 1057 1017   18   18] obs= [0.0078 0.0018 0.0018] reward= -51.85\n",
      "episode  0 step  79 action:  [0. 0.] state= [  20   21 1077 1038   19   19] obs= [0.0079 0.0019 0.0019] reward= -52.875\n",
      "episode  0 step  80 action:  [1. 1.] state= [   0    0 1077 1038    0    0] obs= [0.008 0.    0.   ] reward= -116.875\n",
      "episode  0 step  81 action:  [0. 0.] state= [   2    2 1079 1040    1    1] obs= [8.1e-03 1.0e-04 1.0e-04] reward= -52.975\n",
      "episode  0 step  82 action:  [0. 0.] state= [   3    2 1082 1042    2    2] obs= [0.0082 0.0002 0.0002] reward= -53.1\n",
      "episode  0 step  83 action:  [0. 0.] state= [   5    3 1087 1045    3    3] obs= [0.0083 0.0003 0.0003] reward= -53.3\n",
      "episode  0 step  84 action:  [0. 0.] state= [   7    3 1094 1048    4    4] obs= [0.0084 0.0004 0.0004] reward= -53.55\n",
      "episode  0 step  85 action:  [0. 0.] state= [   8    4 1102 1052    5    5] obs= [0.0085 0.0005 0.0005] reward= -53.85\n",
      "episode  0 step  86 action:  [0. 0.] state= [   8    5 1110 1057    6    6] obs= [0.0086 0.0006 0.0006] reward= -54.175\n",
      "episode  0 step  87 action:  [0. 0.] state= [  10    5 1120 1062    7    7] obs= [0.0087 0.0007 0.0007] reward= -54.55\n",
      "episode  0 step  88 action:  [0. 0.] state= [  11    5 1131 1067    8    8] obs= [0.0088 0.0008 0.0008] reward= -54.95\n",
      "episode  0 step  89 action:  [0. 0.] state= [  13    6 1144 1073    9    9] obs= [0.0089 0.0009 0.0009] reward= -55.425\n",
      "episode  0 step  90 action:  [0. 0.] state= [  14    6 1158 1079   10   10] obs= [0.009 0.001 0.001] reward= -55.925\n",
      "episode  0 step  91 action:  [0. 0.] state= [  16    6 1174 1085   11   11] obs= [0.0091 0.0011 0.0011] reward= -56.475\n",
      "episode  0 step  92 action:  [0. 0.] state= [  17    9 1191 1094   12   12] obs= [0.0092 0.0012 0.0012] reward= -57.125\n",
      "episode  0 step  93 action:  [0. 0.] state= [  21   10 1212 1104   13   13] obs= [0.0093 0.0013 0.0013] reward= -57.9\n",
      "episode  0 step  94 action:  [0. 0.] state= [  21   13 1233 1117   14   14] obs= [0.0094 0.0014 0.0014] reward= -58.75\n",
      "episode  0 step  95 action:  [0. 0.] state= [  23   16 1256 1133   15   15] obs= [0.0095 0.0015 0.0015] reward= -59.725\n",
      "episode  0 step  96 action:  [0. 0.] state= [  22   15 1278 1148   16   16] obs= [0.0096 0.0016 0.0016] reward= -60.65\n",
      "episode  0 step  97 action:  [0. 0.] state= [  22   14 1300 1162   17   17] obs= [0.0097 0.0017 0.0017] reward= -61.55\n",
      "episode  0 step  98 action:  [0. 0.] state= [  22   15 1322 1177   18   18] obs= [0.0098 0.0018 0.0018] reward= -62.475\n",
      "episode  0 step  99 action:  [0. 0.] state= [  23   15 1345 1192   19   19] obs= [0.0099 0.0019 0.0019] reward= -63.425\n",
      "episode  0 step 100 action:  [1. 1.] state= [   0    0 1345 1192    0    0] obs= [0.01 0.   0.  ] reward= -127.425\n",
      "episode  0 step 101 action:  [0. 0.] state= [   2    1 1347 1193    1    1] obs= [1.01e-02 1.00e-04 1.00e-04] reward= -63.5\n",
      "episode  0 step 102 action:  [0. 0.] state= [   5    4 1352 1197    2    2] obs= [0.0102 0.0002 0.0002] reward= -63.725\n",
      "episode  0 step 103 action:  [0. 0.] state= [   7    5 1359 1202    3    3] obs= [0.0103 0.0003 0.0003] reward= -64.025\n",
      "episode  0 step 104 action:  [0. 0.] state= [  11    6 1370 1208    4    4] obs= [0.0104 0.0004 0.0004] reward= -64.45\n",
      "episode  0 step 105 action:  [0. 0.] state= [  14    7 1384 1215    5    5] obs= [0.0105 0.0005 0.0005] reward= -64.975\n",
      "episode  0 step 106 action:  [0. 0.] state= [  15   14 1399 1229    6    6] obs= [0.0106 0.0006 0.0006] reward= -65.7\n",
      "episode  0 step 107 action:  [0. 0.] state= [  16   13 1415 1242    7    7] obs= [0.0107 0.0007 0.0007] reward= -66.425\n",
      "episode  0 step 108 action:  [0. 0.] state= [  17   14 1432 1256    8    8] obs= [0.0108 0.0008 0.0008] reward= -67.2\n",
      "episode  0 step 109 action:  [0. 0.] state= [  17   13 1449 1269    9    9] obs= [0.0109 0.0009 0.0009] reward= -67.95\n",
      "episode  0 step 110 action:  [0. 0.] state= [  17   15 1466 1284   10   10] obs= [0.011 0.001 0.001] reward= -68.75\n",
      "episode  0 step 111 action:  [0. 0.] state= [  18   17 1484 1301   11   11] obs= [0.0111 0.0011 0.0011] reward= -69.625\n",
      "episode  0 step 112 action:  [0. 0.] state= [  18   19 1502 1320   12   12] obs= [0.0112 0.0012 0.0012] reward= -70.55\n",
      "episode  0 step 113 action:  [0. 0.] state= [  18   20 1520 1340   13   13] obs= [0.0113 0.0013 0.0013] reward= -71.5\n",
      "episode  0 step 114 action:  [0. 0.] state= [  17   21 1537 1361   14   14] obs= [0.0114 0.0014 0.0014] reward= -72.45\n",
      "episode  0 step 115 action:  [0. 0.] state= [  18   22 1555 1383   15   15] obs= [0.0115 0.0015 0.0015] reward= -73.45\n",
      "episode  0 step 116 action:  [0. 0.] state= [  18   23 1573 1406   16   16] obs= [0.0116 0.0016 0.0016] reward= -74.475\n",
      "episode  0 step 117 action:  [0. 0.] state= [  20   23 1593 1429   17   17] obs= [0.0117 0.0017 0.0017] reward= -75.55\n",
      "episode  0 step 118 action:  [0. 0.] state= [  19   23 1612 1452   18   18] obs= [0.0118 0.0018 0.0018] reward= -76.6\n",
      "episode  0 step 119 action:  [0. 0.] state= [  23   23 1635 1475   19   19] obs= [0.0119 0.0019 0.0019] reward= -77.75\n",
      "episode  0 step 120 action:  [1. 1.] state= [   0    0 1635 1475    0    0] obs= [0.012 0.    0.   ] reward= -141.75\n",
      "episode  0 step 121 action:  [0. 0.] state= [   1    0 1636 1475    1    1] obs= [1.21e-02 1.00e-04 1.00e-04] reward= -77.775\n",
      "episode  0 step 122 action:  [0. 0.] state= [   4    1 1640 1476    2    2] obs= [0.0122 0.0002 0.0002] reward= -77.9\n",
      "episode  0 step 123 action:  [0. 0.] state= [   7    4 1647 1480    3    3] obs= [0.0123 0.0003 0.0003] reward= -78.175\n",
      "episode  0 step 124 action:  [0. 0.] state= [   7    6 1654 1486    4    4] obs= [0.0124 0.0004 0.0004] reward= -78.5\n",
      "episode  0 step 125 action:  [0. 0.] state= [   9    8 1663 1494    5    5] obs= [0.0125 0.0005 0.0005] reward= -78.925\n",
      "episode  0 step 126 action:  [0. 0.] state= [   7   11 1670 1505    6    6] obs= [0.0126 0.0006 0.0006] reward= -79.375\n",
      "episode  0 step 127 action:  [0. 0.] state= [   7   12 1677 1517    7    7] obs= [0.0127 0.0007 0.0007] reward= -79.85\n",
      "episode  0 step 128 action:  [0. 0.] state= [   8   13 1685 1530    8    8] obs= [0.0128 0.0008 0.0008] reward= -80.375\n",
      "episode  0 step 129 action:  [0. 0.] state= [   8   17 1693 1547    9    9] obs= [0.0129 0.0009 0.0009] reward= -81.0\n",
      "episode  0 step 130 action:  [0. 0.] state= [  10   17 1703 1564   10   10] obs= [0.013 0.001 0.001] reward= -81.675\n",
      "episode  0 step 131 action:  [0. 0.] state= [  10   19 1713 1583   11   11] obs= [0.0131 0.0011 0.0011] reward= -82.4\n",
      "episode  0 step 132 action:  [0. 0.] state= [  10   24 1723 1607   12   12] obs= [0.0132 0.0012 0.0012] reward= -83.25\n",
      "episode  0 step 133 action:  [0. 0.] state= [  11   25 1734 1632   13   13] obs= [0.0133 0.0013 0.0013] reward= -84.15\n",
      "episode  0 step 134 action:  [0. 0.] state= [  12   24 1746 1656   14   14] obs= [0.0134 0.0014 0.0014] reward= -85.05\n",
      "episode  0 step 135 action:  [0. 0.] state= [  14   26 1760 1682   15   15] obs= [0.0135 0.0015 0.0015] reward= -86.05\n",
      "episode  0 step 136 action:  [0. 0.] state= [  14   27 1774 1709   16   16] obs= [0.0136 0.0016 0.0016] reward= -87.075\n",
      "episode  0 step 137 action:  [0. 0.] state= [  15   27 1789 1736   17   17] obs= [0.0137 0.0017 0.0017] reward= -88.125\n",
      "episode  0 step 138 action:  [0. 0.] state= [  17   27 1806 1763   18   18] obs= [0.0138 0.0018 0.0018] reward= -89.225\n",
      "episode  0 step 139 action:  [0. 0.] state= [  20   27 1826 1790   19   19] obs= [0.0139 0.0019 0.0019] reward= -90.4\n",
      "episode  0 step 140 action:  [1. 1.] state= [   0    0 1826 1790    0    0] obs= [0.014 0.    0.   ] reward= -154.4\n",
      "episode  0 step 141 action:  [0. 0.] state= [   1    0 1827 1790    1    1] obs= [1.41e-02 1.00e-04 1.00e-04] reward= -90.425\n",
      "episode  0 step 142 action:  [0. 0.] state= [   2    3 1829 1793    2    2] obs= [0.0142 0.0002 0.0002] reward= -90.55\n",
      "episode  0 step 143 action:  [0. 0.] state= [   3    4 1832 1797    3    3] obs= [0.0143 0.0003 0.0003] reward= -90.725\n",
      "episode  0 step 144 action:  [0. 0.] state= [   5    4 1837 1801    4    4] obs= [0.0144 0.0004 0.0004] reward= -90.95\n",
      "episode  0 step 145 action:  [0. 0.] state= [   9    6 1846 1807    5    5] obs= [0.0145 0.0005 0.0005] reward= -91.325\n",
      "episode  0 step 146 action:  [0. 0.] state= [  10    8 1856 1815    6    6] obs= [0.0146 0.0006 0.0006] reward= -91.775\n",
      "episode  0 step 147 action:  [0. 0.] state= [  11   12 1867 1827    7    7] obs= [0.0147 0.0007 0.0007] reward= -92.35\n",
      "episode  0 step 148 action:  [0. 0.] state= [  12   11 1879 1838    8    8] obs= [0.0148 0.0008 0.0008] reward= -92.925\n",
      "episode  0 step 149 action:  [0. 0.] state= [  12   11 1891 1849    9    9] obs= [0.0149 0.0009 0.0009] reward= -93.5\n",
      "episode  0 step 150 action:  [0. 0.] state= [  14   12 1905 1861   10   10] obs= [0.015 0.001 0.001] reward= -94.15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-7572.799996174872, 7.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show states for one rollout of second scripted policy\n",
    "compute_avg_return(henv, manual_scripted_policy1, num_episodes=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average return of do-nothing-policy: -10393.246 avg culls 0.0\n",
      "average return of manual policy: -5160.449 avg culls 4.065\n"
     ]
    }
   ],
   "source": [
    "manual_reward0, cullsteps = compute_avg_return(henv, manual_scripted_policy0, num_episodes=200)\n",
    "print (f\"average return of do-nothing-policy: {manual_reward0:.3f} avg culls {cullsteps}\")\n",
    "manual_reward1, cullsteps = compute_avg_return(henv, manual_scripted_policy1, num_episodes=200)\n",
    "print (f\"average return of manual policy: {manual_reward1:.3f} avg culls {cullsteps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=50, verbose=False):\n",
    "  total_return = 0.0\n",
    "  cullsteps = 0\n",
    "  for e in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "        policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "    else:\n",
    "        policy_state = None # other policies without memory\n",
    "    episode_return = 0.0\n",
    "    i=0\n",
    "    while not time_step.is_last():\n",
    "        i+=1\n",
    "        action_step = policy.action(time_step, policy_state)\n",
    "        if action_step.action[0][0] > 0 or action_step.action[0][1] > 0:\n",
    "            cullsteps += 1\n",
    "        policy_state = action_step.state\n",
    "        time_step = environment.step(action_step.action)\n",
    "        if isinstance(environment, FREnv):\n",
    "            state = environment.get_state()\n",
    "        else:\n",
    "            state = None # TF environment from wrapper does not have get_state()\n",
    "        episode_return += time_step.reward\n",
    "        if verbose:\n",
    "            print (f\"episode {e:>2} step{i:>4} action: \", action_step.action, \"state=\", state, \"obs=\", time_step.observation, \"reward=\", time_step.reward)\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  cullsteps /= num_episodes\n",
    "  return avg_return, cullsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Made it work to here, implement ddpg agent next.'''\n",
    "\n",
    "import functools\n",
    "from tf_agents.networks import nest_map\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.keras_layers import inner_reshape\n",
    "from tf_agents.networks.sequential import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 30000\n",
    "replay_buffer_max_length = 10000\n",
    "batch_size = 64\n",
    "num_eval_episodes = 100\n",
    "initial_collect_steps = 1000\n",
    "collect_steps_per_iteration = 5\n",
    "log_interval = 500\n",
    "eval_interval = 500\n",
    "target_update_period = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(FREnv(herd_sizes = [32,32], expected_episode_length=100, max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.02, rand_infection_prob = 0.06))\n",
    "eval_env = tf_py_environment.TFPyEnvironment(FREnv(herd_sizes = [32,32], expected_episode_length=100, max_episode_length=max_episode_length,\n",
    "               rand_recovery_prob = 0.02, rand_infection_prob = 0.06))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03)\n",
    "\n",
    "#Make critic and actor net for ddpg\n",
    "\n",
    "dense = functools.partial(\n",
    "    tf.keras.layers.Dense,\n",
    "    activation=tf.keras.activations.relu,\n",
    "    kernel_initializer=tf.compat.v1.variance_scaling_initializer(\n",
    "        scale=1./ 3.0, mode='fan_in', distribution='uniform'))\n",
    "\n",
    "\n",
    "def create_identity_layer():\n",
    "    return tf.keras.layers.Lambda(lambda x: x)\n",
    "\n",
    "\n",
    "def create_fc_network(layer_units):\n",
    "    return sequential.Sequential([dense(num_units) for num_units in layer_units])\n",
    "\n",
    "\n",
    "def create_actor_network(fc_layer_units, action_spec):\n",
    "    \"\"\"Create an actor network for DDPG.\"\"\"\n",
    "    flat_action_spec = tf.nest.flatten(action_spec)\n",
    "    if len(flat_action_spec) > 1:\n",
    "        raise ValueError('Only a single action tensor is supported by this network')\n",
    "    flat_action_spec = flat_action_spec[0]\n",
    "\n",
    "    fc_layers = [dense(num_units) for num_units in fc_layer_units]\n",
    "\n",
    "    num_actions = flat_action_spec.shape.num_elements()\n",
    "    action_fc_layer = tf.keras.layers.Dense(\n",
    "        num_actions,\n",
    "        activation=tf.keras.activations.tanh,\n",
    "        kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "            minval=-0.003, maxval=0.003))\n",
    "\n",
    "    scaling_layer = tf.keras.layers.Lambda(\n",
    "        lambda x: common.scale_to_spec(x, flat_action_spec))\n",
    "    return sequential.Sequential(fc_layers + [action_fc_layer, scaling_layer])\n",
    "\n",
    "\n",
    "def create_critic_network(obs_fc_layer_units,\n",
    "                          action_fc_layer_units,\n",
    "                          joint_fc_layer_units):\n",
    "    \"\"\"Create a critic network for DDPG.\"\"\"\n",
    "\n",
    "    def split_inputs(inputs):\n",
    "        return {'observation': inputs[0], 'action': inputs[1]}\n",
    "\n",
    "    obs_network = create_fc_network(\n",
    "        obs_fc_layer_units) if obs_fc_layer_units else create_identity_layer()\n",
    "    action_network = create_fc_network(\n",
    "        action_fc_layer_units\n",
    "        ) if action_fc_layer_units else create_identity_layer()\n",
    "    joint_network = create_fc_network(\n",
    "        joint_fc_layer_units) if joint_fc_layer_units else create_identity_layer()\n",
    "    \n",
    "    value_fc_layer = tf.keras.layers.Dense(\n",
    "        1,\n",
    "        activation=None,\n",
    "        kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "            minval=-0.003, maxval=0.003))\n",
    "\n",
    "    return sequential.Sequential([\n",
    "        tf.keras.layers.Lambda(split_inputs),\n",
    "        nest_map.NestMap({\n",
    "            'observation': obs_network,\n",
    "            'action': action_network\n",
    "        }),\n",
    "        nest_map.NestFlatten(),\n",
    "        tf.keras.layers.Concatenate(),\n",
    "        joint_network,\n",
    "        value_fc_layer,\n",
    "        inner_reshape.InnerReshape([1], [])\n",
    "    ])\n",
    "\n",
    "actor_fc_layers=(200, 150)\n",
    "critic_obs_fc_layers=(200,)\n",
    "critic_action_fc_layers=None\n",
    "critic_joint_fc_layers=(150,)\n",
    "\n",
    "actor_net = create_actor_network(actor_fc_layers, train_env.action_spec())\n",
    "critic_net = create_critic_network(critic_obs_fc_layers,\n",
    "                                       critic_action_fc_layers,\n",
    "                                       critic_joint_fc_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_counter = tf.Variable(0)\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "agent = ddpg_agent.DdpgAgent(\n",
    "        train_env.time_step_spec(),\n",
    "        train_env.action_spec(),\n",
    "        actor_network=actor_net,\n",
    "        critic_network=critic_net,\n",
    "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=1e-4),\n",
    "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=1e-3),\n",
    "        ou_stddev=0.2,\n",
    "        ou_damping=0.15,\n",
    "        target_update_tau=0.05,\n",
    "        target_update_period=5,\n",
    "        #dqda_clipping=dqda_clipping,\n",
    "        td_errors_loss_fn=tf.compat.v1.losses.huber_loss,\n",
    "        gamma=0.99,\n",
    "        #reward_scale_factor=reward_scale_factor,\n",
    "        #gradient_clipping=gradient_clipping,\n",
    "        #debug_summaries=debug_summaries,\n",
    "        #summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "        train_step_counter=global_step)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually initialize a reasonably good policy: kill both herds if the sum of observations is large\n",
    "#W = np.array([[0, 3 ,0, 2],[0, 0, 3, 2,]])\n",
    "#b = np.array([1, 0, 0, 0])\n",
    "#q_net.layers[0].set_weights([W,b])\n",
    "#agent.policy.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(\n",
       "{action: (64, 2, 2),\n",
       " discount: (64, 2),\n",
       " next_step_type: (64, 2),\n",
       " observation: (64, 2, 3),\n",
       " policy_info: (),\n",
       " reward: (64, 2),\n",
       " step_type: (64, 2)}), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(\n",
       "{action: tf.float32,\n",
       " discount: tf.float32,\n",
       " next_step_type: tf.int32,\n",
       " observation: tf.float32,\n",
       " policy_info: (),\n",
       " reward: tf.float32,\n",
       " step_type: tf.int32}), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1: loss = 39.7793\tstep 1: average return = -4567.9 cullsteps = 107.2\n",
      "step 50: average return = -4142.9 cullsteps = 100.0\n",
      "step 100: average return = -3638.5 cullsteps = 92.4\n",
      "step = 500: loss = 100.8475\tstep 500: average return = -12852.7 cullsteps = 112.5\n",
      "step = 1000: loss = 162.6963\tstep 1000: average return = -13519.6 cullsteps = 104.6\n",
      "step = 1500: loss = 136.3464\tstep 1500: average return = -8135.7 cullsteps = 90.2\n",
      "step = 2000: loss = 363.9132\tstep 2000: average return = -9320.3 cullsteps = 93.7\n",
      "step = 2500: loss = 756.0385\tstep 2500: average return = -8444.8 cullsteps = 86.6\n",
      "step = 3000: loss = 583.7759\tstep 3000: average return = -13385.8 cullsteps = 103.5\n",
      "step = 3500: loss = 428.0354\tstep 3500: average return = -9536.3 cullsteps = 74.8\n",
      "step = 4000: loss = 324.2596\tstep 4000: average return = -22768.3 cullsteps = 83.2\n",
      "step = 4500: loss = 359.7174\tstep 4500: average return = -10433.1 cullsteps = 61.9\n",
      "step = 5000: loss = 328.0521\tstep 5000: average return = -12757.8 cullsteps = 42.4\n",
      "step = 5500: loss = 216.4619\tstep 5500: average return = -16295.2 cullsteps = 31.9\n",
      "step = 6000: loss = 151.8226\tstep 6000: average return = -11144.7 cullsteps = 20.1\n",
      "step = 6500: loss = 192.4889\tstep 6500: average return = -11920.4 cullsteps = 12.4\n",
      "step = 7000: loss = 128.2690\tstep 7000: average return = -16448.9 cullsteps = 7.6\n",
      "step = 7500: loss = 124.1394\tstep 7500: average return = -8414.4 cullsteps = 4.9\n",
      "step = 8000: loss = 135.7821\tstep 8000: average return = -12721.5 cullsteps = 3.0\n",
      "step = 8500: loss = 178.4617\tstep 8500: average return = -9855.9 cullsteps = 2.0\n",
      "step = 9000: loss = 243.5737\tstep 9000: average return = -13482.2 cullsteps = 1.0\n",
      "step = 9500: loss = 105.5618\tstep 9500: average return = -8369.6 cullsteps = 0.0\n",
      "step = 10000: loss = 159.6585\tstep 10000: average return = -6868.6 cullsteps = 0.0\n",
      "step = 10500: loss = 44.6400\tstep 10500: average return = -13224.3 cullsteps = 0.0\n",
      "step = 11000: loss = 56.2331\tstep 11000: average return = -12268.0 cullsteps = 0.0\n",
      "step = 11500: loss = 181.1871\tstep 11500: average return = -12095.1 cullsteps = 0.0\n",
      "step = 12000: loss = 35.2720\tstep 12000: average return = -8671.5 cullsteps = 0.0\n",
      "step = 12500: loss = 65.5798\tstep 12500: average return = -8866.1 cullsteps = 0.0\n",
      "step = 13000: loss = 28.6714\tstep 13000: average return = -14041.7 cullsteps = 0.0\n",
      "step = 13500: loss = 24.9918\tstep 13500: average return = -13283.7 cullsteps = 0.0\n",
      "step = 14000: loss = 35.3049\tstep 14000: average return = -10137.5 cullsteps = 0.0\n",
      "step = 14500: loss = 290.3347\tstep 14500: average return = -11025.4 cullsteps = 0.0\n",
      "step = 15000: loss = 285.3611\tstep 15000: average return = -15572.4 cullsteps = 0.0\n",
      "step = 15500: loss = 191.7281\tstep 15500: average return = -10765.4 cullsteps = 0.0\n",
      "step = 16000: loss = 414.8409\tstep 16000: average return = -9087.5 cullsteps = 0.0\n",
      "step = 16500: loss = 230.2990\tstep 16500: average return = -10887.3 cullsteps = 0.0\n",
      "step = 17000: loss = 101.8884\tstep 17000: average return = -10579.8 cullsteps = 0.0\n",
      "step = 17500: loss = 137.6156\tstep 17500: average return = -12840.4 cullsteps = 0.0\n",
      "step = 18000: loss = 42.4142\tstep 18000: average return = -20093.5 cullsteps = 0.0\n",
      "step = 18500: loss = 36.2334\tstep 18500: average return = -12560.2 cullsteps = 0.0\n",
      "step = 19000: loss = 29.8942\tstep 19000: average return = -11395.5 cullsteps = 0.0\n",
      "step = 19500: loss = 292.5846\tstep 19500: average return = -8425.5 cullsteps = 0.0\n",
      "step = 20000: loss = 321.3519\tstep 20000: average return = -12616.3 cullsteps = 0.0\n",
      "step = 20500: loss = 169.7837\tstep 20500: average return = -17199.4 cullsteps = 0.0\n",
      "step = 21000: loss = 33.5798\tstep 21000: average return = -13817.1 cullsteps = 0.0\n",
      "step = 21500: loss = 350.5631\tstep 21500: average return = -10425.1 cullsteps = 0.0\n",
      "step = 22000: loss = 58.4924\tstep 22000: average return = -13998.9 cullsteps = 0.0\n",
      "step = 22500: loss = 151.4876\tstep 22500: average return = -20479.4 cullsteps = 0.0\n",
      "step = 23000: loss = 796.3660\tstep 23000: average return = -11663.6 cullsteps = 0.0\n",
      "step = 23500: loss = 62.9094\tstep 23500: average return = -16854.2 cullsteps = 0.0\n",
      "step = 24000: loss = 88.4923\tstep 24000: average return = -13117.1 cullsteps = 0.0\n",
      "step = 24500: loss = 449.8096\tstep 24500: average return = -13542.0 cullsteps = 0.0\n",
      "step = 25000: loss = 167.2029\tstep 25000: average return = -13172.2 cullsteps = 0.0\n",
      "step = 25500: loss = 35.6183\tstep 25500: average return = -7633.2 cullsteps = 0.0\n",
      "step = 26000: loss = 370.6053\tstep 26000: average return = -11981.4 cullsteps = 0.0\n",
      "step = 26500: loss = 534.4687\tstep 26500: average return = -13438.4 cullsteps = 0.0\n",
      "step = 27000: loss = 194.1235\tstep 27000: average return = -8440.9 cullsteps = 0.0\n",
      "step = 27500: loss = 30.4536\tstep 27500: average return = -11979.5 cullsteps = 0.0\n",
      "step = 28000: loss = 25.4786\t"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-922e0ab99a44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mavg_return\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcullsteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_avg_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_eval_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step {0}: average return = {1:.1f} cullsteps = {2:.1f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_return\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcullsteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreturns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-ae36664d23bc>\u001b[0m in \u001b[0;36mcompute_avg_return\u001b[0;34m(environment, policy, num_episodes, verbose)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mcullsteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0mseed_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeedStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msalt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf_agents_tf_policy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0mdistribution_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=wrong-arg-types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m     actions = tf.nest.map_structure(\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         distribution_step.action)\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0mdistribution_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=wrong-arg-types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m     actions = tf.nest.map_structure(\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         distribution_step.action)\n\u001b[1;32m    564\u001b[0m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tf_agents/distributions/reparameterized_sampling.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(distribution, reparam, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_shape, seed, name, **kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m     \"\"\"\n\u001b[1;32m   1232\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_and_control_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_sample_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_sample_and_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36m_call_sample_n\u001b[0;34m(self, sample_shape, seed, **kwargs)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     sample_shape = ps.convert_to_shape_tensor(\n\u001b[1;32m   1206\u001b[0m         ps.cast(sample_shape, tf.int32), name='sample_shape')\n\u001b[0;32m-> 1207\u001b[0;31m     sample_shape, n = self._expand_sample_shape_to_vector(\n\u001b[0m\u001b[1;32m   1208\u001b[0m         sample_shape, 'sample_shape')\n\u001b[1;32m   1209\u001b[0m     samples = self._sample_n(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36m_expand_sample_shape_to_vector\u001b[0;34m(self, x, name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;34m\"\"\"Helper to `sample` which ensures input is 1D.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m     \u001b[0mprod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1968\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1969\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_probability/python/internal/distribution_util.py\u001b[0m in \u001b[0;36mexpand_to_vector\u001b[0;34m(x, tensor_name, op_name, validate_args)\u001b[0m\n\u001b[1;32m   1400\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'expand_to_vector'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m     \u001b[0mx_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_shape_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m     \u001b[0mndims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorshape_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m   \"\"\"\n\u001b[0;32m-> 1430\u001b[0;31m   return convert_to_tensor_v2(\n\u001b[0m\u001b[1;32m   1431\u001b[0m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1434\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   \u001b[0;34m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m   return convert_to_tensor(\n\u001b[0m\u001b[1;32m   1437\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m       \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"convert_to_tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1508\u001b[0m def convert_to_tensor(value,\n\u001b[1;32m   1509\u001b[0m                       \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step <= 1 or step % log_interval == 0:\n",
    "    print('step = {0:4>}: loss = {1:.4f}'.format(step, train_loss), end=\"\\t\")\n",
    "\n",
    "  if step <= 1 or (step <= 100 and step % 50 == 0) or step % eval_interval == 0:\n",
    "    avg_return, cullsteps = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step {0}: average return = {1:.1f} cullsteps = {2:.1f}'.format(step, avg_return.numpy().item(), cullsteps))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_reward, culleps = compute_avg_return(eval_env, agent.policy, num_episodes=500)\n",
    "print (\"reward of learned policy: \", learned_reward.numpy(), \"cullsteps=\", culleps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ts = eval_env.reset()\n",
    "\n",
    "def get_action(obs):\n",
    "    \"\"\" execute the learned policy network \n",
    "       obs:  one float for global time, one float for each herd - the time since last culling\n",
    "    \"\"\"\n",
    "    _ts = TimeStep(tf.constant([0.]),\n",
    "                   tf.constant([0.]),\n",
    "                   tf.constant([1]),\n",
    "                   tf.constant([obs]))\n",
    "    # a = agent.collect_policy.action(_ts) # just to see how much is explored versus exploited\n",
    "    a = agent.policy.action(_ts)\n",
    "    return a.action.numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what the learned policy does on a grid of observations (5 steps per row&col)\n",
    "'''A = [[get_action([.0, x,y])\n",
    " for y in np.arange(0.,1.,.05,np.float32)]\n",
    " for x in np.arange(0.,1.,.05,np.float32)]\n",
    "A'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with parameters of manually designed q_network policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W, b = agent.policy.trainable_variables\n",
    "# W = W.numpy()\n",
    "# b = b.numpy()\n",
    "# print (\"weights\\n\", W, \"\\nbias\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nn(obs):\n",
    "#    y = np.dot(obs, W)+b\n",
    "#    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn([0.5,.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
