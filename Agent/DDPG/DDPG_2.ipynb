{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "valuable-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DDPG Implementation using tf agents library\n",
    "\n",
    "\n",
    "Before use, please set PATH to any existing directory for video, train, and eval directory output \n",
    "(Default: '~/')\n",
    "Additionally, please set an integer for subdirectory identification of each run of DDPG_Bipedal \n",
    "(Default: 28420).\n",
    "'''\n",
    "PATH = '/home/jovyan/Masterarbeit/DDPG'  \n",
    "RUN_ID = 28420\n",
    "'''\n",
    "Almost done! After this last variable is set, please run all cells.\n",
    "For testing CS5Gamma, please set following variable to 'True' (Default: 'False').\n",
    "'''\n",
    "CS5Gamma = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "practical-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "sys.path.insert(1, '/home/jovyan/Masterarbeit/reinforce-one/Environment')\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "import gin\n",
    "from six.moves import range\n",
    "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import parallel_py_environment\n",
    "from tf_agents.environments import suite_mujoco\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.keras_layers import inner_reshape\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import nest_map\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from Env import Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf27346f-d85b-4d21-9068-4098ba34f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/jovyan/Masterarbeit/DDPG'\n",
    "num_herds=2\n",
    "total_population=300\n",
    "py_env = Env(num_herds = num_herds, total_population = total_population, \n",
    "                                                  fix_episode_length = True, average_episode_length = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "informal-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defaults\n",
    "env_name = 'Epidemic_Env'\n",
    "num_iterations = 100000\n",
    "use_tf_functions = True\n",
    "\n",
    "# Replay Buffer Parameters & Noise Function Parameters\n",
    "initial_collect_steps = 5000 \n",
    "collect_steps_per_iteration = 5\n",
    "replay_buffer_capacity = 100000\n",
    "ou_stddev = 0.2 \n",
    "ou_damping = 0.15 \n",
    "\n",
    "# Target Update Parameters\n",
    "target_update_tau = 0.05\n",
    "target_update_period = 5\n",
    "\n",
    "# Train Step Parameters\n",
    "train_steps_per_iteration = 100 \n",
    "batch_size = 64\n",
    "actor_learning_rate = 1e-4\n",
    "critic_learning_rate = 1e-3\n",
    "td_errors_loss_fn = tf.compat.v1.losses.huber_loss\n",
    "gamma = 0.99\n",
    "\n",
    "# Evaluation and Summary Parameters\n",
    "num_eval_episodes = 100\n",
    "eval_interval = 1000\n",
    "log_interval = 1000\n",
    "summary_interval = 1000\n",
    "summaries_flush_secs = 10\n",
    "\n",
    "# Results Directory and Run ID\n",
    "run_id = RUN_ID    # ID to differentiate between runs\n",
    "root_dir = PATH    # Has to be an existing directory\n",
    "\n",
    "# For CS5Gamma configuration\n",
    "if CS5Gamma:\n",
    "    collect_steps_per_iteration = 5\n",
    "    gamma = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "friendly-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=50, verbose=False):\n",
    "  total_return = 0.0\n",
    "  cullsteps = 0 \n",
    "  for e in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "        policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "    else:\n",
    "        #print(policy.get_initial_state(batch_size=train_env.batch_size()))\n",
    "        policy_state = policy.get_initial_state(batch_size=1) # other policies without memory\n",
    "    episode_return = 0.0\n",
    "    i=0\n",
    "    while not time_step.is_last():\n",
    "        i+=1\n",
    "        action_step = policy.action(time_step, policy_state)\n",
    "        for i in range (num_herds, num_herds*2):\n",
    "            if action_step.action[0][i] > 0.1:\n",
    "                cullsteps += 1\n",
    "                break\n",
    "        policy_state = action_step.state\n",
    "        time_step = environment.step(action_step.action)\n",
    "\n",
    "        state = None # TF environment from wrapper does not have get_state()\n",
    "        episode_return += time_step.reward\n",
    "        if verbose:\n",
    "            print (f\"episode {e:>2} step{i:>4} action: \", action_step.action, \n",
    "                   \"state=\", state, \"obs=\", time_step.observation, \"reward=\", time_step.reward)\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  cullsteps /= num_episodes\n",
    "  return avg_return, cullsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "likely-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(\n",
    "    root_dir,\n",
    "    env_name='Env',\n",
    "    eval_env_name=None,\n",
    "    env_load_fn=suite_mujoco.load,\n",
    "    num_iterations=2000000,\n",
    "    actor_fc_layers=(400, 300),\n",
    "    critic_obs_fc_layers=(400,),\n",
    "    critic_action_fc_layers=None,\n",
    "    critic_joint_fc_layers=(300,),\n",
    "    # Params for collect\n",
    "    initial_collect_steps=1000,\n",
    "    collect_steps_per_iteration=5,\n",
    "    num_parallel_environments=1,\n",
    "    replay_buffer_capacity=100000,\n",
    "    ou_stddev=0.2,\n",
    "    ou_damping=0.15,\n",
    "    # Params for target update\n",
    "    target_update_tau=0.05,\n",
    "    target_update_period=5,\n",
    "    # Params for train\n",
    "    train_steps_per_iteration=1,\n",
    "    batch_size=64,\n",
    "    actor_learning_rate=1e-4,\n",
    "    critic_learning_rate=1e-3,\n",
    "    dqda_clipping=None,\n",
    "    td_errors_loss_fn=tf.compat.v1.losses.huber_loss,\n",
    "    gamma=0.995,\n",
    "    reward_scale_factor=1.0,\n",
    "    gradient_clipping=None,\n",
    "    use_tf_functions=True,\n",
    "    # Params for eval\n",
    "    num_eval_episodes=10,\n",
    "    eval_interval=10000,\n",
    "    # Params for checkpoints, summaries, and logging\n",
    "    log_interval=1000,\n",
    "    summary_interval=1000,\n",
    "    summaries_flush_secs=10,\n",
    "    debug_summaries=False,\n",
    "    summarize_grads_and_vars=False,\n",
    "    eval_metrics_callback=None):\n",
    "\n",
    "  \"\"\"A simple train and eval for DDPG.\"\"\"\n",
    "  root_dir = os.path.expanduser(root_dir)\n",
    "  train_dir = os.path.join(root_dir, 'train')\n",
    "  eval_dir = os.path.join(root_dir, 'eval')\n",
    "    \n",
    "  # Initialize policy saver\n",
    "  best_return = -10000\n",
    "  policy_dir = os.path.join(root_dir, 'policy')\n",
    "\n",
    "  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "      train_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "  train_summary_writer.set_as_default()\n",
    "\n",
    "  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "      eval_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "  eval_metrics = [\n",
    "      tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n",
    "      tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n",
    "  ]\n",
    "\n",
    "  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "  with tf.compat.v2.summary.record_if(\n",
    "      lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
    "    if num_parallel_environments > 1:\n",
    "      tf_env = tf_py_environment.TFPyEnvironment(\n",
    "          parallel_py_environment.ParallelPyEnvironment(\n",
    "              [lambda: env_load_fn(env_name)] * num_parallel_environments))\n",
    "    else:\n",
    "      tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "    eval_env_name = eval_env_name or env_name\n",
    "    eval_tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "\n",
    "    actor_net = create_actor_network(actor_fc_layers, tf_env.action_spec())\n",
    "    critic_net = create_critic_network(critic_obs_fc_layers,\n",
    "                                       critic_action_fc_layers,\n",
    "                                       critic_joint_fc_layers)\n",
    "\n",
    "    tf_agent = ddpg_agent.DdpgAgent(\n",
    "        tf_env.time_step_spec(),\n",
    "        tf_env.action_spec(),\n",
    "        actor_network=actor_net,\n",
    "        critic_network=critic_net,\n",
    "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=actor_learning_rate),\n",
    "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=critic_learning_rate),\n",
    "        ou_stddev=ou_stddev,\n",
    "        ou_damping=ou_damping,\n",
    "        target_update_tau=target_update_tau,\n",
    "        target_update_period=target_update_period,\n",
    "        dqda_clipping=dqda_clipping,\n",
    "        td_errors_loss_fn=td_errors_loss_fn,\n",
    "        gamma=gamma,\n",
    "        reward_scale_factor=reward_scale_factor,\n",
    "        gradient_clipping=gradient_clipping,\n",
    "        debug_summaries=debug_summaries,\n",
    "        summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "        train_step_counter=global_step)\n",
    "    tf_agent.initialize()\n",
    "\n",
    "    saver = policy_saver.PolicySaver(tf_agent.policy)\n",
    "    \n",
    "    train_metrics = [\n",
    "        tf_metrics.NumberOfEpisodes(),\n",
    "        tf_metrics.EnvironmentSteps(),\n",
    "        tf_metrics.AverageReturnMetric(),\n",
    "        tf_metrics.AverageEpisodeLengthMetric(),\n",
    "    ]\n",
    "\n",
    "    eval_policy = tf_agent.policy\n",
    "    collect_policy = tf_agent.collect_policy\n",
    "\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        tf_agent.collect_data_spec,\n",
    "        batch_size=tf_env.batch_size,\n",
    "        max_length=replay_buffer_capacity)\n",
    "\n",
    "    initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        tf_env,\n",
    "        collect_policy,\n",
    "        observers=[replay_buffer.add_batch],\n",
    "        num_steps=initial_collect_steps)\n",
    "\n",
    "    collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        tf_env,\n",
    "        collect_policy,\n",
    "        observers=[replay_buffer.add_batch] + train_metrics,\n",
    "        num_steps=collect_steps_per_iteration)\n",
    "\n",
    "    if use_tf_functions:\n",
    "      initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "      collect_driver.run = common.function(collect_driver.run)\n",
    "      tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "    # Collect initial replay data.\n",
    "    logging.info(\n",
    "        'Initializing replay buffer by collecting experience for %d steps with '\n",
    "        'a random policy.', initial_collect_steps)\n",
    "    initial_collect_driver.run()\n",
    "\n",
    "    results = metric_utils.eager_compute(\n",
    "        eval_metrics,\n",
    "        eval_tf_env,\n",
    "        eval_policy,\n",
    "        num_episodes=num_eval_episodes,\n",
    "        train_step=global_step,\n",
    "        summary_writer=eval_summary_writer,\n",
    "        summary_prefix='Metrics',\n",
    "    )\n",
    "    if eval_metrics_callback is not None:\n",
    "      eval_metrics_callback(results, global_step.numpy())\n",
    "    metric_utils.log_metrics(eval_metrics)\n",
    "\n",
    "    time_step = None\n",
    "    policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n",
    "\n",
    "    timed_at_step = global_step.numpy()\n",
    "    time_acc = 0\n",
    "\n",
    "    # Dataset generates trajectories with shape [Bx2x...]\n",
    "    dataset = replay_buffer.as_dataset(\n",
    "        num_parallel_calls=3,\n",
    "        sample_batch_size=batch_size,\n",
    "        num_steps=2).prefetch(3)\n",
    "    iterator = iter(dataset)\n",
    "\n",
    "    def train_step():\n",
    "      experience, _ = next(iterator)\n",
    "      return tf_agent.train(experience)\n",
    "\n",
    "    if use_tf_functions:\n",
    "      train_step = common.function(train_step)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "      start_time = time.time()\n",
    "      time_step, policy_state = collect_driver.run(\n",
    "          time_step=time_step,\n",
    "          policy_state=policy_state,\n",
    "      )\n",
    "      for _ in range(train_steps_per_iteration):\n",
    "        train_loss = train_step()\n",
    "      time_acc += time.time() - start_time\n",
    "\n",
    "      if global_step.numpy() % log_interval == 0:\n",
    "        logging.info('step = %d, loss = %f', global_step.numpy(),\n",
    "                     train_loss.loss)\n",
    "        steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n",
    "        logging.info('%.3f steps/sec', steps_per_sec)\n",
    "        tf.compat.v2.summary.scalar(\n",
    "            name='global_steps_per_sec', data=steps_per_sec, step=global_step)\n",
    "        timed_at_step = global_step.numpy()\n",
    "        time_acc = 0\n",
    "\n",
    "      for train_metric in train_metrics:\n",
    "        train_metric.tf_summaries(\n",
    "            train_step=global_step, step_metrics=train_metrics[:2])\n",
    "\n",
    "      if global_step.numpy() % eval_interval == 0:\n",
    "        results = metric_utils.eager_compute(\n",
    "            eval_metrics,\n",
    "            eval_tf_env,\n",
    "            eval_policy,\n",
    "            num_episodes=num_eval_episodes,\n",
    "            train_step=global_step,\n",
    "            summary_writer=eval_summary_writer,\n",
    "            summary_prefix='Metrics',\n",
    "        )\n",
    "        if eval_metrics_callback is not None:\n",
    "          eval_metrics_callback(results, global_step.numpy())\n",
    "        metric_utils.log_metrics(eval_metrics)\n",
    "        avg_return, cullsteps = compute_avg_return(eval_tf_env, eval_policy, num_episodes=100, verbose=False)\n",
    "        print('step {0}: average return = {1:.1f} cullsteps = {2:.1f}'.format(global_step.numpy(), \n",
    "                                                                            avg_return.numpy().item(), cullsteps))\n",
    "        if avg_return > best_return:\n",
    "            if avg_return > -300:\n",
    "                best_return = avg_return\n",
    "                print('Final best return: ', best_return)\n",
    "                saver.save(os.path.join(policy_dir, str(global_step.numpy())))\n",
    "                break\n",
    "            else:\n",
    "                best_return = avg_return\n",
    "                print('New best return: ', best_return)\n",
    "                saver.save(os.path.join(policy_dir, str(global_step.numpy())))\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "dense = functools.partial(\n",
    "    tf.keras.layers.Dense,\n",
    "    activation=tf.keras.activations.relu,\n",
    "    kernel_initializer=tf.compat.v1.variance_scaling_initializer(\n",
    "        scale=1./ 3.0, mode='fan_in', distribution='uniform'))\n",
    "\n",
    "\n",
    "def create_identity_layer():\n",
    "  return tf.keras.layers.Lambda(lambda x: x)\n",
    "\n",
    "\n",
    "def create_fc_network(layer_units):\n",
    "  return sequential.Sequential([dense(num_units) for num_units in layer_units])\n",
    "\n",
    "\n",
    "def create_actor_network(fc_layer_units, action_spec):\n",
    "  \"\"\"Create an actor network for DDPG.\"\"\"\n",
    "  flat_action_spec = tf.nest.flatten(action_spec)\n",
    "  if len(flat_action_spec) > 1:\n",
    "    raise ValueError('Only a single action tensor is supported by this network')\n",
    "  flat_action_spec = flat_action_spec[0]\n",
    "\n",
    "  fc_layers = [dense(num_units) for num_units in fc_layer_units]\n",
    "\n",
    "  num_actions = flat_action_spec.shape.num_elements()\n",
    "  action_fc_layer = tf.keras.layers.Dense(\n",
    "      num_actions,\n",
    "      activation=tf.keras.activations.tanh,\n",
    "      kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "          minval=-0.003, maxval=0.003))\n",
    "\n",
    "  scaling_layer = tf.keras.layers.Lambda(\n",
    "      lambda x: common.scale_to_spec(x, flat_action_spec))\n",
    "  return sequential.Sequential(fc_layers + [action_fc_layer, scaling_layer])\n",
    "\n",
    "\n",
    "def create_critic_network(obs_fc_layer_units,\n",
    "                          action_fc_layer_units,\n",
    "                          joint_fc_layer_units):\n",
    "  \"\"\"Create a critic network for DDPG.\"\"\"\n",
    "\n",
    "  def split_inputs(inputs):\n",
    "    return {'observation': inputs[0], 'action': inputs[1]}\n",
    "\n",
    "  obs_network = create_fc_network(\n",
    "      obs_fc_layer_units) if obs_fc_layer_units else create_identity_layer()\n",
    "  action_network = create_fc_network(\n",
    "      action_fc_layer_units\n",
    "  ) if action_fc_layer_units else create_identity_layer()\n",
    "  joint_network = create_fc_network(\n",
    "      joint_fc_layer_units) if joint_fc_layer_units else create_identity_layer(\n",
    "      )\n",
    "  value_fc_layer = tf.keras.layers.Dense(\n",
    "      1,\n",
    "      activation=None,\n",
    "      kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "          minval=-0.003, maxval=0.003))\n",
    "\n",
    "  return sequential.Sequential([\n",
    "      tf.keras.layers.Lambda(split_inputs),\n",
    "      nest_map.NestMap({\n",
    "          'observation': obs_network,\n",
    "          'action': action_network\n",
    "      }),\n",
    "      nest_map.NestFlatten(),\n",
    "      tf.keras.layers.Concatenate(),\n",
    "      joint_network,\n",
    "      value_fc_layer,\n",
    "      inner_reshape.InnerReshape([1], [])\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-builder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10000: average return = -45388.4 cullsteps = 0.0\n",
      "step 20000: average return = -44384.5 cullsteps = 0.0\n",
      "step 30000: average return = -23051.6 cullsteps = 99.3\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x7fd19c6c2850>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x7fd19c6c2850>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 40000: average return = -45085.5 cullsteps = 0.0\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x7fd19c682fd0>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <bound method DynamicEpisodeDriver.run of <tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver object at 0x7fd19c682fd0>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 50000: average return = -45830.0 cullsteps = 0.0\n",
      "step 60000: average return = -45150.6 cullsteps = 0.0\n",
      "step 70000: average return = -45144.4 cullsteps = 0.0\n",
      "step 80000: average return = -45021.3 cullsteps = 0.0\n",
      "step 90000: average return = -45236.0 cullsteps = 0.0\n",
      "step 100000: average return = -44854.8 cullsteps = 0.0\n",
      "step 110000: average return = -44679.0 cullsteps = 0.0\n",
      "step 120000: average return = -45004.2 cullsteps = 0.0\n",
      "step 130000: average return = -45000.4 cullsteps = 0.0\n",
      "step 140000: average return = -45457.5 cullsteps = 0.0\n",
      "step 150000: average return = -44677.4 cullsteps = 0.0\n",
      "step 160000: average return = -44888.6 cullsteps = 0.0\n",
      "step 170000: average return = -45608.5 cullsteps = 0.0\n",
      "step 180000: average return = -44952.2 cullsteps = 0.0\n",
      "step 190000: average return = -44578.8 cullsteps = 0.0\n",
      "step 200000: average return = -44974.4 cullsteps = 0.0\n",
      "step 210000: average return = -44751.4 cullsteps = 0.0\n",
      "step 220000: average return = -44734.5 cullsteps = 0.0\n",
      "step 230000: average return = -44297.1 cullsteps = 0.0\n",
      "step 240000: average return = -44689.0 cullsteps = 0.0\n",
      "step 250000: average return = -45581.0 cullsteps = 0.0\n",
      "step 260000: average return = -44412.1 cullsteps = 0.0\n",
      "step 270000: average return = -44400.9 cullsteps = 0.0\n",
      "step 280000: average return = -45567.3 cullsteps = 0.0\n",
      "step 290000: average return = -45045.6 cullsteps = 0.0\n",
      "step 300000: average return = -45055.8 cullsteps = 0.0\n",
      "step 310000: average return = -45109.3 cullsteps = 0.0\n",
      "step 320000: average return = -45538.2 cullsteps = 0.0\n",
      "step 330000: average return = -45053.9 cullsteps = 0.0\n",
      "step 340000: average return = -44629.9 cullsteps = 0.0\n",
      "step 350000: average return = -43864.9 cullsteps = 0.0\n",
      "step 360000: average return = -45005.2 cullsteps = 0.0\n",
      "step 370000: average return = -44928.1 cullsteps = 0.0\n",
      "step 380000: average return = -45196.8 cullsteps = 0.0\n",
      "step 390000: average return = -44850.8 cullsteps = 0.0\n",
      "step 400000: average return = -45199.3 cullsteps = 0.0\n",
      "step 410000: average return = -45360.4 cullsteps = 0.0\n",
      "step 420000: average return = -44602.9 cullsteps = 0.0\n",
      "step 430000: average return = -44441.4 cullsteps = 0.0\n",
      "step 440000: average return = -43932.6 cullsteps = 0.0\n",
      "step 450000: average return = -45062.9 cullsteps = 0.0\n",
      "step 460000: average return = -44876.4 cullsteps = 0.0\n",
      "step 470000: average return = -45190.4 cullsteps = 0.0\n",
      "step 480000: average return = -45321.7 cullsteps = 0.0\n",
      "step 490000: average return = -44721.0 cullsteps = 0.0\n",
      "step 500000: average return = -45117.8 cullsteps = 0.0\n",
      "step 510000: average return = -44713.5 cullsteps = 0.0\n",
      "step 520000: average return = -44772.2 cullsteps = 0.0\n",
      "step 530000: average return = -45106.6 cullsteps = 0.0\n",
      "step 540000: average return = -44484.4 cullsteps = 0.0\n",
      "step 550000: average return = -44892.8 cullsteps = 0.0\n",
      "step 560000: average return = -44955.4 cullsteps = 0.0\n",
      "step 570000: average return = -44598.0 cullsteps = 0.0\n",
      "step 580000: average return = -44980.8 cullsteps = 0.0\n",
      "step 590000: average return = -44545.5 cullsteps = 0.0\n",
      "step 600000: average return = -44806.4 cullsteps = 0.0\n",
      "step 610000: average return = -45398.9 cullsteps = 0.0\n",
      "step 620000: average return = -44575.4 cullsteps = 0.0\n",
      "step 630000: average return = -44650.3 cullsteps = 0.0\n",
      "step 640000: average return = -44731.6 cullsteps = 0.0\n",
      "step 650000: average return = -45942.9 cullsteps = 0.0\n",
      "step 660000: average return = -45587.2 cullsteps = 0.0\n",
      "step 670000: average return = -45112.8 cullsteps = 0.0\n",
      "step 680000: average return = -45512.4 cullsteps = 0.0\n",
      "step 690000: average return = -45465.0 cullsteps = 0.0\n",
      "step 700000: average return = -44844.1 cullsteps = 0.0\n",
      "step 710000: average return = -45074.1 cullsteps = 0.0\n",
      "step 720000: average return = -45387.3 cullsteps = 0.0\n",
      "step 730000: average return = -45371.8 cullsteps = 0.0\n",
      "step 740000: average return = -44516.6 cullsteps = 0.0\n",
      "step 750000: average return = -44867.2 cullsteps = 0.0\n",
      "step 760000: average return = -45370.1 cullsteps = 0.0\n",
      "step 770000: average return = -44802.4 cullsteps = 0.0\n",
      "step 780000: average return = -44849.9 cullsteps = 0.0\n",
      "step 790000: average return = -44979.5 cullsteps = 0.0\n",
      "step 800000: average return = -44947.9 cullsteps = 0.0\n",
      "step 810000: average return = -44491.2 cullsteps = 0.0\n",
      "step 820000: average return = -45681.1 cullsteps = 0.0\n",
      "step 830000: average return = -45482.2 cullsteps = 0.0\n",
      "step 840000: average return = -45223.3 cullsteps = 0.0\n"
     ]
    }
   ],
   "source": [
    "train_eval(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
