{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c844da0c-c325-4c1d-89aa-dc6f30a8cf54",
   "metadata": {},
   "source": [
    "Agent for epidemic control model  \n",
    "===================================\n",
    "This notebook will train an agent in an epidemic control environment using DDPG with RNNs.  \n",
    "  \n",
    "For use, please edit PATH variable below to any folder where training outputs can be stored.  \n",
    "Also, please create a folder titled 'policy' in PATH directory.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae57fd21-7a8e-4f2b-a9fd-998fb8cc1d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/jovyan/Masterarbeit/RNN_DDPG'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa4638f-e5a4-4a9c-aef3-0da5db96c608",
   "metadata": {},
   "source": [
    "Imports\n",
    "------------------------\n",
    "Firstly, all relevant dependencies will be imported.  \n",
    "Comments indicate what imports are generally used for or related to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beef1151-a79a-48a1-934b-dd2432ec325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "\n",
    "# Environment \n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "# Neural Networks\n",
    "from tf_agents.agents.ddpg import actor_rnn_network\n",
    "from tf_agents.agents.ddpg import critic_rnn_network\n",
    "# Agent \n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "# Experience Replay\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f283017f-7ad2-4b4e-80dd-63d77f64430b",
   "metadata": {},
   "source": [
    "Environment\n",
    "------------------  \n",
    "Next, an environment will be imported and initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df64898d-0f22-4ee2-ab83-62cc7fe0c678",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/jovyan/Masterarbeit/reinforce-one/Environment')\n",
    "from Env import Env\n",
    "\n",
    "num_herds = 2\n",
    "total_population = 300\n",
    "average_episode_length=200\n",
    "fix_episode_length = True\n",
    "\n",
    "py_env = Env(num_herds = num_herds, total_population = total_population, fix_episode_length = fix_episode_length, \n",
    "             average_episode_length = average_episode_length)\n",
    "\n",
    "# For use in training later\n",
    "train_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25e3bf-3a59-4d99-a3d2-200424f99140",
   "metadata": {},
   "source": [
    "Then, the environment will be tested with a simple scripted policy.  \n",
    "Average Returns will be saved as a threshhold for evaluation to save good policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc55d55f-d065-4f68-8702-7d1de4660c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env(environment, policy, num_episodes = 50):\n",
    "    if isinstance(environment, py_environment.PyEnvironment):\n",
    "        total_return = 0.0\n",
    "        cullsteps = 0 \n",
    "        if environment.action_spec().shape[0] == num_herds:\n",
    "            only_culls = True\n",
    "        else:\n",
    "            only_culls = False\n",
    "            \n",
    "        for e in range(num_episodes):\n",
    "            time_step = environment.reset()\n",
    "            if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "                policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "            else:\n",
    "                policy_state = policy.get_initial_state(batch_size=1) # other policies without memory\n",
    "            episode_return = 0.0\n",
    "            i=0\n",
    "            while not time_step.is_last():\n",
    "                i+=1\n",
    "                action_step = policy.action(time_step, policy_state)\n",
    "                if only_culls:\n",
    "                    for i in range (0, num_herds):\n",
    "                        if action_step.action[i] > 0:\n",
    "                            cullsteps += 1\n",
    "                else:\n",
    "                    for i in range (num_herds, num_herds*2):\n",
    "                        if action_step.action[i] > 0:\n",
    "                            cullsteps += 1\n",
    "                policy_state = action_step.state\n",
    "                time_step = environment.step(action_step.action)\n",
    "                episode_return += time_step.reward\n",
    "            total_return += episode_return\n",
    "\n",
    "        avg_return = total_return / num_episodes\n",
    "        cullsteps /= num_episodes\n",
    "        return avg_return, cullsteps\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e26f0c-67d5-48d1-91de-1d9190d6ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scripted Policy\n",
    "\n",
    "if py_env.action_spec().shape[0] == num_herds:\n",
    "    action_script = [(8, [0,0]), \n",
    "                     (1, [1,1]),\n",
    "                     (8, [0,0]), \n",
    "                     (1, [1,1])] * int(1 + average_episode_length)\n",
    "else:\n",
    "    action_script = [(8, [0,0,0,0]), \n",
    "                     (1, [0,0,1,1]),\n",
    "                     (8, [0,0,0,0]), \n",
    "                     (1, [0,0,1,1])] * int(1 + average_episode_length)\n",
    "    \n",
    "scr_pol = scripted_py_policy.ScriptedPyPolicy(time_step_spec=py_env.time_step_spec(),\n",
    "                                              action_spec=py_env.action_spec(), \n",
    "                                              action_script=action_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5876fcd-e58a-4d30-a58b-ade98488b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test environment with scripted policy\n",
    "#avg_return, culls = test_env(py_env, scr_pol , num_episodes = 200)\n",
    "\n",
    "# Multiply by 1.5 to save policy progress as well\n",
    "#threshhold = avg_return * 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc6bfc-ed0d-41e0-8abb-de69707da490",
   "metadata": {},
   "source": [
    "RNN DDPG\n",
    "----------\n",
    "Training of an Agent using DDPG with RNNs for actor and critic can begin.   \n",
    "First, set all hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd74ff0-5937-4982-84a0-142da498bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN hyperparams\n",
    "actor_fc_layers = (400, 300)\n",
    "actor_output_fc_layers = (100,)\n",
    "actor_lstm_size = (40,)\n",
    "critic_obs_fc_layers = (400,)\n",
    "critic_action_fc_layers = None\n",
    "critic_joint_fc_layers = (300,)\n",
    "critic_output_fc_layers = (100,)\n",
    "critic_lstm_size = (40,)\n",
    "\n",
    "# Agent hyperparams\n",
    "actor_learning_rate = 1e-4\n",
    "critic_learning_rate= 1e-3\n",
    "ou_stddev = 0.2\n",
    "ou_damping = 0.15\n",
    "tu_tau = 0.05\n",
    "tu_period = 5\n",
    "gamma = 0.995\n",
    "\n",
    "# Experience replay hyperparams\n",
    "initial_collect_episodes = 10\n",
    "collect_episodes_per_iteration = 1\n",
    "rb_capacity = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf4aa4-850d-4079-abdf-c644ad4095e9",
   "metadata": {},
   "source": [
    "Initiialize neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4061c4ea-54b7-4ca0-b733-0096ea178573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN actor critic\n",
    "actor_net = actor_rnn_network.ActorRnnNetwork(train_env.time_step_spec().observation, \n",
    "                                              train_env.action_spec(), \n",
    "                                              input_fc_layer_params=actor_fc_layers, \n",
    "                                              lstm_size = actor_lstm_size, \n",
    "                                              output_fc_layer_params=actor_output_fc_layers)\n",
    "\n",
    "critic_net_input_specs = (train_env.time_step_spec().observation, \n",
    "                          train_env.action_spec())\n",
    "\n",
    "critic_net = critic_rnn_network.CriticRnnNetwork(critic_net_input_specs, \n",
    "                                                 observation_fc_layer_params=critic_obs_fc_layers, \n",
    "                                                 action_fc_layer_params=critic_action_fc_layers, \n",
    "                                                 joint_fc_layer_params=critic_joint_fc_layers, \n",
    "                                                 lstm_size=critic_lstm_size, \n",
    "                                                 output_fc_layer_params=critic_output_fc_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e58348d-8798-43c0-b7b7-aa95733e16e9",
   "metadata": {},
   "source": [
    "Initialize agent and global step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5406509-faf9-423b-8813-7686a9df4298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global step tracks number of train steps\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "# DDPG Agent\n",
    "agent = ddpg_agent.DdpgAgent(train_env.time_step_spec(), \n",
    "                             train_env.action_spec(), \n",
    "                             actor_network = actor_net, \n",
    "                             critic_network = critic_net, \n",
    "                             actor_optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=actor_learning_rate), \n",
    "                             critic_optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=critic_learning_rate), \n",
    "                             ou_stddev = ou_stddev, \n",
    "                             ou_damping = ou_damping, \n",
    "                             target_update_tau = tu_tau, \n",
    "                             target_update_period = tu_period,  \n",
    "                             gamma = gamma, \n",
    "                             train_step_counter = global_step)\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a380d7e-a2ae-425f-9fa7-021b5ffa3a2c",
   "metadata": {},
   "source": [
    "Set up experience replay (replay buffer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "136d5918-bfb7-4cb4-b994-214268886913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Trajectory(\n",
      "{'action': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name=None, minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
      " 'observation': BoundedTensorSpec(shape=(7,), dtype=tf.float32, name=None, minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'policy_info': (),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57915636-fe43-402b-9061-06493a321534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
