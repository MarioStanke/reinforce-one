{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c844da0c-c325-4c1d-89aa-dc6f30a8cf54",
   "metadata": {},
   "source": [
    "# Agent for epidemic control model  \n",
    "This notebook will train an agent in an epidemic control environment using DDPG with RNNs.  \n",
    "  \n",
    "For use, please edit PATH variable below to any folder where training outputs can be stored.  \n",
    "Also, please create a folder titled 'policy' in PATH directory.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae57fd21-7a8e-4f2b-a9fd-998fb8cc1d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/jovyan/Masterarbeit/F'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa4638f-e5a4-4a9c-aef3-0da5db96c608",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Firstly, all relevant dependencies will be imported.  \n",
    "Comments indicate what imports are generally used for or related to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beef1151-a79a-48a1-934b-dd2432ec325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "\n",
    "# Environment \n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "# Neural Networks\n",
    "from tf_agents.agents.ddpg import actor_rnn_network\n",
    "from tf_agents.agents.ddpg import critic_rnn_network\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.networks import nest_map\n",
    "from tf_agents.keras_layers import inner_reshape\n",
    "import functools\n",
    "# Agent \n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "# Experience Replay\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "#Training\n",
    "from tf_agents.utils import common\n",
    "#Evaluation\n",
    "from tf_agents.policies import policy_saver\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f283017f-7ad2-4b4e-80dd-63d77f64430b",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "Next, an environment will be imported and initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df64898d-0f22-4ee2-ab83-62cc7fe0c678",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/jovyan/Masterarbeit/reinforce-one/Environment')\n",
    "from Env import Env\n",
    "\n",
    "num_herds = 2\n",
    "total_population = 300\n",
    "average_episode_length=200\n",
    "fix_episode_length = True\n",
    "\n",
    "py_env = Env(num_herds = num_herds, total_population = total_population, fix_episode_length = fix_episode_length, \n",
    "             average_episode_length = average_episode_length)\n",
    "\n",
    "# For use in training later\n",
    "train_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25e3bf-3a59-4d99-a3d2-200424f99140",
   "metadata": {},
   "source": [
    "Then, the environment will be tested with a simple scripted policy.  \n",
    "Average Returns will be saved as a threshhold for evaluation to save good policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc55d55f-d065-4f68-8702-7d1de4660c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env(environment, policy, num_episodes = 50):\n",
    "    if isinstance(environment, py_environment.PyEnvironment):\n",
    "        total_return = 0.0\n",
    "        cullsteps = 0 \n",
    "        if environment.action_spec().shape[0] == num_herds:\n",
    "            only_culls = True\n",
    "        else:\n",
    "            only_culls = False\n",
    "            \n",
    "        for e in range(num_episodes):\n",
    "            time_step = environment.reset()\n",
    "            if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "                policy_state = policy.get_initial_state() # remember where in the script we were\n",
    "            else:\n",
    "                policy_state = policy.get_initial_state(batch_size=1) # other policies without memory\n",
    "            episode_return = 0.0\n",
    "            i=0\n",
    "            while not time_step.is_last():\n",
    "                i+=1\n",
    "                action_step = policy.action(time_step, policy_state)\n",
    "                if only_culls:\n",
    "                    for i in range (0, num_herds):\n",
    "                        if action_step.action[i] > 0:\n",
    "                            cullsteps += 1\n",
    "                else:\n",
    "                    for i in range (num_herds, num_herds*2):\n",
    "                        if action_step.action[i] > 0:\n",
    "                            cullsteps += 1\n",
    "                policy_state = action_step.state\n",
    "                time_step = environment.step(action_step.action)\n",
    "                episode_return += time_step.reward\n",
    "            total_return += episode_return\n",
    "\n",
    "        avg_return = total_return / num_episodes\n",
    "        cullsteps /= num_episodes\n",
    "        return avg_return, cullsteps\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e26f0c-67d5-48d1-91de-1d9190d6ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scripted Policy\n",
    "\n",
    "if py_env.action_spec().shape[0] == num_herds:\n",
    "    action_script = [(8, [0,0]), \n",
    "                     (1, [1,1]),\n",
    "                     (8, [0,0]), \n",
    "                     (1, [1,1])] * int(1 + average_episode_length)\n",
    "else:\n",
    "    action_script = [(8, [0,0,0,0]), \n",
    "                     (1, [0,0,1,1]),\n",
    "                     (8, [0,0,0,0]), \n",
    "                     (1, [0,0,1,1])] * int(1 + average_episode_length)\n",
    "    \n",
    "scr_pol = scripted_py_policy.ScriptedPyPolicy(time_step_spec=py_env.time_step_spec(),\n",
    "                                              action_spec=py_env.action_spec(), \n",
    "                                              action_script=action_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d123696-794c-4d15-8bac-73cbf73c5043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test environment with scripted policy\n",
    "#avg_return, culls = test_env(py_env, scr_pol , num_episodes = 200)\n",
    "\n",
    "# Multiply by 1.5 to save policy progress as well\n",
    "#threshhold = avg_return * 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef484f6-499b-4461-a763-00444ea156b2",
   "metadata": {},
   "source": [
    "## Policy Evaluation  \n",
    " \n",
    "Define functions to evaluate an agent at any point during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45c95847-7206-41ac-aa1d-ac9849972597",
   "metadata": {},
   "outputs": [],
   "source": [
    " def plot_actions_and_states(action_list, inf_list):\n",
    "        t = np.linspace(0, len(action_list), num=len(action_list))\n",
    "        fig, (p1,p2,p3) = plt.subplots(1, 3, figsize=(7,12))\n",
    "        fig.suptitle('Actions over Time')\n",
    "        p1.set_title('Tests over Time')\n",
    "        p1.set_xlabel('Time')\n",
    "        p1.set_ylabel('Number of Tests')\n",
    "        p1.set_ylim(-0.1, 1.1)\n",
    "        p2.set_title('Herd Replacements over Time')\n",
    "        p2.set_xlabel('Time')\n",
    "        p2.set_ylabel('Replacement Probability')\n",
    "        p2.set_ylim(-0.1, 1.1) \n",
    "        p3.set_title('Infectious over Time')\n",
    "        p3.set_xlabel('Time')\n",
    "        p3.set_ylabel('Percentage of Infectious')\n",
    "        p3.set_ylim(-0.1, 1.1)\n",
    "        n_tests_h1, n_tests_h2, replace_h1, replace_h2, inf_h1, inf_h2 = [], [], [], [], [], []\n",
    "        for i in range(len(action_list)):\n",
    "            n_tests_h1.append(action_list[i][0])\n",
    "            n_tests_h2.append(action_list[i][1])\n",
    "            replace_h1.append(action_list[i][2])\n",
    "            replace_h2.append(action_list[i][3])\n",
    "            inf_h1.append(inf_list[i][0])\n",
    "            inf_h2.append(inf_list[i][1])\n",
    "        p1.plot(t, n_tests_h1, color='yellow', label = 'Herd 1', marker = '', linestyle = '-', alpha=0.7)\n",
    "        p1.plot(t, n_tests_h2, color='blue', label = 'Herd 2', marker = '', linestyle = '-', alpha=0.7)\n",
    "        p2.plot(t, replace_h1, color='yellow', label = 'Herd 1', marker = '', linestyle = '-', alpha=0.7)\n",
    "        p2.plot(t, replace_h2, color='blue', label = 'Herd 2', marker = '', linestyle = '-', alpha=0.7)\n",
    "        p3.plot(t, inf_h1, color='yellow', label = 'Herd 1', marker = '', linestyle = '-', alpha=0.7)\n",
    "        p3.plot(t, inf_h2, color='blue', label = 'Herd 2', marker = '', linestyle = '-', alpha=0.7)\n",
    "\n",
    "        p1.legend()\n",
    "        p2.legend()\n",
    "        p3.legend()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "071311f1-b096-496d-b8ad-5eee6d8e8399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_agent(env, policy, num_episodes=50, num_herds = num_herds, create_plot = False):\n",
    "    if isinstance(environment, tf_py_environment.TFPyEnvironment):\n",
    "        total_return = 0.0\n",
    "        culls = 0 \n",
    "        tests = 0\n",
    "        actions = []\n",
    "        infectious = []\n",
    "        if env.action_spec().shape[0] == num_herds:\n",
    "            raise ValueError('Only for environments with tests and culls.')\n",
    "\n",
    "        for e in range(num_episodes):\n",
    "            time_step = environment.reset()\n",
    "            if isinstance(policy, scripted_py_policy.ScriptedPyPolicy):\n",
    "                raise ValueError('Only for agent policies.')\n",
    "            else:\n",
    "                policy_state = policy.get_initial_state(env.batch_size)\n",
    "            episode_return = 0.0\n",
    "            i=0\n",
    "            \n",
    "            while not time_step.is_last():\n",
    "                i+=1\n",
    "                action_step = policy.action(time_step, policy_state)\n",
    "                \n",
    "                # Count total number of culls\n",
    "                for j in range (num_herds, num_herds*2):\n",
    "                    if action_step.action[0][j] >= 0.5:\n",
    "                        culls += 1\n",
    "                # Count number of steps where tests were done for each herd\n",
    "                for k in range (0, num_herds):\n",
    "                    if action_step.action[0][j] >= 0.:\n",
    "                        tests += 1\n",
    "                        \n",
    "                # Save actions and states for one episode\n",
    "                if e == np.int32(num_episodes/2):\n",
    "                    act = np.zeros(np.size(action_step.action[0]), np.float32)\n",
    "                    act[0] = action_step.action[0][0]\n",
    "                    act[1] = action_step.action[0][1]\n",
    "                    for c in range(num_herds, num_herds*2):\n",
    "                        if action_step.action[0][j] >= 0.5:\n",
    "                            act[j] = 1.\n",
    "                        else:\n",
    "                            act[j] = 0.\n",
    "                    actions.append(act)\n",
    "                    inf_percentages = np.zeros(num_herds, np.float32)\n",
    "                    state = env.get_state()\n",
    "                    for d in range (0, num_herds):\n",
    "                        inf_percentages[d] = (state[d+num_herds]/state[d])\n",
    "                    infectious.append(inf_percentages)\n",
    "                    \n",
    "                policy_state = action_step.state\n",
    "                time_step = environment.step(action_step.action)\n",
    "                episode_return += time_step.reward\n",
    "\n",
    "            total_return += episode_return\n",
    "        if create_plot:\n",
    "            fig = plot_actions_and_states(actions, infectious)\n",
    "        avg_return = total_return / num_episodes\n",
    "        culls /= num_episodes\n",
    "        tests /= num_episodes\n",
    "        tests /= num_herds\n",
    "        return avg_return, culls, tests\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acda5e7-21a5-4c7a-a525-7737fc0204f4",
   "metadata": {},
   "source": [
    "## Training\n",
    "In this section, define a function for agent training and evaluation.  \n",
    "First, create neural networks for use for variations in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f462ba1-b95b-40c0-b8e3-cb80620a660a",
   "metadata": {},
   "source": [
    "### RNN DDPG\n",
    "\n",
    "Set up actor and critic recurrent neural networks for training with DDPG using RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c24b1614-aa95-44fb-8ee1-2bf7a1453876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN hyperparams\n",
    "actor_fc_layers = (400, 300)\n",
    "actor_output_fc_layers = (100,)\n",
    "actor_lstm_size = (40,)\n",
    "critic_obs_fc_layers = (400,)\n",
    "critic_action_fc_layers = None\n",
    "critic_joint_fc_layers = (300,)\n",
    "critic_output_fc_layers = (100,)\n",
    "critic_lstm_size = (40,)\n",
    "\n",
    "# RNN actor critic\n",
    "actor_rnn = actor_rnn_network.ActorRnnNetwork(train_env.time_step_spec().observation, \n",
    "                                              train_env.action_spec(), \n",
    "                                              input_fc_layer_params=actor_fc_layers, \n",
    "                                              lstm_size = actor_lstm_size, \n",
    "                                              output_fc_layer_params=actor_output_fc_layers)\n",
    "\n",
    "critic_net_input_specs = (train_env.time_step_spec().observation, \n",
    "                          train_env.action_spec())\n",
    "\n",
    "critic_rnn = critic_rnn_network.CriticRnnNetwork(critic_net_input_specs, \n",
    "                                                 observation_fc_layer_params=critic_obs_fc_layers, \n",
    "                                                 action_fc_layer_params=critic_action_fc_layers, \n",
    "                                                 joint_fc_layer_params=critic_joint_fc_layers, \n",
    "                                                 lstm_size=critic_lstm_size, \n",
    "                                                 output_fc_layer_params=critic_output_fc_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0521fad7-dfd8-4bf9-8ae5-c1aee104bba5",
   "metadata": {},
   "source": [
    "### DDPG  \n",
    "Create actor and critic artificial neural networks for DDPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98974f12-a15d-4624-a538-3fb74e751460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ann hyperparameters\n",
    "actor_fc_layers=(200, 150)\n",
    "critic_obs_fc_layers=(200,)\n",
    "critic_action_fc_layers=None\n",
    "critic_joint_fc_layers=(150,)\n",
    "\n",
    "\n",
    "# Define creation functions \n",
    "\n",
    "dense = functools.partial(tf.keras.layers.Dense,\n",
    "                          activation=tf.keras.activations.relu,\n",
    "                          kernel_initializer=tf.compat.v1.variance_scaling_initializer(\n",
    "                              scale=1./ 3.0, mode='fan_in', distribution='uniform')\n",
    "                         )\n",
    "\n",
    "\n",
    "def create_identity_layer():\n",
    "    return tf.keras.layers.Lambda(lambda x: x)\n",
    "\n",
    "\n",
    "def create_fc_network(layer_units):\n",
    "    return sequential.Sequential([dense(num_units) for num_units in layer_units])\n",
    "\n",
    "\n",
    "def create_actor_network(fc_layer_units, action_spec):\n",
    "    flat_action_spec = tf.nest.flatten(action_spec)\n",
    "    if len(flat_action_spec) > 1:\n",
    "        raise ValueError('Only a single action tensor is supported by this network')\n",
    "    flat_action_spec = flat_action_spec[0]\n",
    "\n",
    "    fc_layers = [dense(num_units) for num_units in fc_layer_units]\n",
    "\n",
    "    num_actions = flat_action_spec.shape.num_elements()\n",
    "    \n",
    "    action_fc_layer = tf.keras.layers.Dense(num_actions,\n",
    "                                            activation=tf.keras.activations.tanh,\n",
    "                                            kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "                                                minval=-0.003, maxval=0.003)\n",
    "                                           )\n",
    "\n",
    "    scaling_layer = tf.keras.layers.Lambda(\n",
    "        lambda x: common.scale_to_spec(x, flat_action_spec))\n",
    "    return sequential.Sequential(fc_layers + [action_fc_layer, scaling_layer])\n",
    "\n",
    "\n",
    "def create_critic_network(obs_fc_layer_units,\n",
    "                          action_fc_layer_units,\n",
    "                          joint_fc_layer_units):\n",
    "    def split_inputs(inputs):\n",
    "        return {'observation': inputs[0], 'action': inputs[1]}\n",
    "    \n",
    "    if obs_fc_layer_units:\n",
    "        obs_network = create_fc_network(obs_fc_layer_units)  \n",
    "    else:\n",
    "        obs_network = create_identity_layer()\n",
    "    if action_fc_layer_units:    \n",
    "        action_network = create_fc_network(action_fc_layer_units)\n",
    "    else:\n",
    "        action_network = create_identity_layer()\n",
    "    if joint_fc_layer_units:    \n",
    "        joint_network = create_fc_network(joint_fc_layer_units) \n",
    "    else: \n",
    "        joint_network = create_identity_layer()\n",
    "    value_fc_layer = tf.keras.layers.Dense(1,\n",
    "                                           activation=None,\n",
    "                                           kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.003, maxval=0.003)\n",
    "                                          )\n",
    "\n",
    "    return sequential.Sequential([tf.keras.layers.Lambda(split_inputs),\n",
    "                                  nest_map.NestMap({'observation': obs_network,\n",
    "                                                    'action': action_network}),\n",
    "                                  nest_map.NestFlatten(),\n",
    "                                  tf.keras.layers.Concatenate(),\n",
    "                                  joint_network,\n",
    "                                  value_fc_layer,\n",
    "                                  inner_reshape.InnerReshape([1], [])\n",
    "                                 ])\n",
    "\n",
    "\n",
    "# Create neural networks\n",
    "\n",
    "actor_ann = create_actor_network(actor_fc_layers, \n",
    "                                 train_env.action_spec())\n",
    "critic_ann = create_critic_network(critic_obs_fc_layers,\n",
    "                                   critic_action_fc_layers,\n",
    "                                   critic_joint_fc_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd8d1e-fc46-4eb6-8d77-ac4ea56a6def",
   "metadata": {},
   "source": [
    "### Hyperparameters  \n",
    "Set hyperparameters for DDPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8c325f5-dfc6-4ebe-b5d6-81e081675a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 100000\n",
    "\n",
    "# Agent hyperparameters\n",
    "actor_learning_rate = 1e-4\n",
    "critic_learning_rate = 1e-3\n",
    "ou_stddev = 0.2\n",
    "ou_damping = 0.15\n",
    "target_update_tau = 0.05\n",
    "target_update_period = 5\n",
    "gamma = 0.995\n",
    "# Training hyperparameters\n",
    "train_steps_per_iteration = 10\n",
    "\n",
    "# Experience replay hyperparameters\n",
    "initial_collect_episodes = 10\n",
    "collect_episodes_per_iteration = 1\n",
    "rb_capacity = 100000\n",
    "batch_size = 64\n",
    "train_sequence_length = 1    # 1 for ddpg?\n",
    "\n",
    "# Evaluation hyperparameters\n",
    "eval_interval = 1000\n",
    "plots = False    # Only works if num_herds = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf4aa4-850d-4079-abdf-c644ad4095e9",
   "metadata": {},
   "source": [
    "### DDPG  \n",
    "Finally, define training function using tf-agent's ddpg agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb1188ab-e1ff-458f-b452-684bd5e5e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DDPG(num_iterations = num_iterations,\n",
    "         actor_net = None,\n",
    "         critic_net = None,\n",
    "         directory = PATH,\n",
    "         plots = plots,\n",
    "         eval_interval = eval_interval,\n",
    "         # Agent hyperparameters\n",
    "         actor_learning_rate = actor_learning_rate,\n",
    "         critic_learning_rate = critic_learning_rate,\n",
    "         ou_stddev = ou_stddev,\n",
    "         ou_damping = ou_damping,\n",
    "         target_update_tau = target_update_tau,\n",
    "         target_update_period = target_update_period,\n",
    "         gamma = gamma,\n",
    "         # Training hyperparameters\n",
    "         train_steps_per_iteration = train_steps_per_iteration,\n",
    "         # Experience replay hyperparameters\n",
    "         initial_collect_episodes = initial_collect_episodes,\n",
    "         collect_episodes_per_iteration = collect_episodes_per_iteration,\n",
    "         rb_capacity = rb_capacity,\n",
    "         batch_size = batch_size,\n",
    "         train_sequence_length = train_sequence_length,\n",
    "         ):\n",
    "    \n",
    "    if actor_net is None or critic_net is None:\n",
    "        raise ValueError('Please input an actor network and critic network')\n",
    "    # Global step tracks number of train steps\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "    # DDPG Agent\n",
    "    agent = ddpg_agent.DdpgAgent(train_env.time_step_spec(), \n",
    "                                 train_env.action_spec(), \n",
    "                                 actor_network = actor_net, \n",
    "                                 critic_network = critic_net, \n",
    "                                 actor_optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=actor_learning_rate), \n",
    "                                 critic_optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=critic_learning_rate), \n",
    "                                 ou_stddev = ou_stddev, \n",
    "                                 ou_damping = ou_damping, \n",
    "                                 target_update_tau = target_update_tau, \n",
    "                                 target_update_period = target_update_tau,  \n",
    "                                 gamma = gamma, \n",
    "                                 train_step_counter = global_step)\n",
    "    agent.initialize()\n",
    "    \n",
    "    # Tools for evaluation\n",
    "    eval_policy = agent.policy\n",
    "    saver = policy_saver.PolicySaver(eval_policy)\n",
    "    policy_dir = os.path.join(directory, 'policy')\n",
    "    best_return = -1000000\n",
    "\n",
    "    # Experience replay and sample collection tools\n",
    "    collect_policy = agent.collect_policy\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(agent.collect_data_spec,\n",
    "                                                                   batch_size=train_env.batch_size,\n",
    "                                                                   max_length=rb_capacity)\n",
    "\n",
    "    initial_collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(train_env,\n",
    "                                                                         collect_policy,\n",
    "                                                                         observers=[replay_buffer.add_batch],\n",
    "                                                                         num_episodes=initial_collect_episodes)\n",
    "\n",
    "    collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(train_env,\n",
    "                                                                 collect_policy,\n",
    "                                                                 observers=[replay_buffer.add_batch],\n",
    "                                                                 num_episodes=collect_episodes_per_iteration)\n",
    "    \n",
    "    # TF functions speed up training process\n",
    "    initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "    collect_driver.run = common.function(collect_driver.run)\n",
    "    agent.train = common.function(agent.train)\n",
    "    \n",
    "    # Collect initial random samples for replay buffer\n",
    "    initial_collect_driver.run()\n",
    "    \n",
    "    # Training starts\n",
    "    time_step = None\n",
    "    policy_state = collect_policy.get_initial_state(train_env.batch_size)\n",
    "    \n",
    "    dataset = replay_buffer.as_dataset(num_parallel_calls=3,\n",
    "                                       sample_batch_size=batch_size,\n",
    "                                       num_steps=train_sequence_length + 1).prefetch(3)\n",
    "    iterator = iter(dataset)\n",
    "    def train_step():\n",
    "        batches_of_trajectories, _ = next(iterator)\n",
    "        return agent.train(batches_of_trajectories)\n",
    "    train_step = common.function(train_step)\n",
    "    \n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        time_step, policy_state = collect_driver.run(time_step=time_step,\n",
    "                                                     policy_state=policy_state)    \n",
    "        for _ in range(train_steps_per_iteration):\n",
    "            train_loss = train_step()\n",
    "        \n",
    "        # Evaluation\n",
    "        if global_step.numpy() % eval_interval == 0:\n",
    "            average_return, culls, tests = eval_agent(eval_env, eval_policy, num_episodes=200, create_plot = plots)\n",
    "            print('Global Step = {0}, Average Return = {1:.1f}.'.format(global_step.numpy(), avg_return.numpy().item()))\n",
    "            print('Average Culls = {0}, Average Tests = {1}.'.format(culls, tests))                    \n",
    "            if average_return > best_return:\n",
    "                best_return = average_return\n",
    "                print('New best return: ', best_return)\n",
    "                saver.save(os.path.join(directory, str(global_step.numpy())))\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b462205-04dc-45b7-8778-51384f7ea951",
   "metadata": {},
   "source": [
    "# Run Functions (rename)  \n",
    "Now you can execute ddpg using either artificial or recurrent NNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56903148-5341-4610-b7eb-c14e818cf72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n",
      "hi\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Integer division by zero\n\t [[{{node StatefulPartitionedCall/FloorMod}}]] [Op:__inference_train_step_2788]\n\nFunction call stack:\ntrain_step\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fcd723ccc5e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m loss = DDPG(num_iterations = num_iterations,\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mactor_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_ann\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mcritic_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic_ann\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mplots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplots\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-0431f2e732c5>\u001b[0m in \u001b[0;36mDDPG\u001b[0;34m(num_iterations, actor_net, critic_net, directory, plots, eval_interval, actor_learning_rate, critic_learning_rate, ou_stddev, ou_damping, target_update_tau, target_update_period, gamma, train_steps_per_iteration, initial_collect_episodes, collect_episodes_per_iteration, rb_capacity, batch_size, train_sequence_length)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_steps_per_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    954\u001b[0m               *args, **kwds)\n\u001b[1;32m    955\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m       return self._concrete_stateful_fn._call_flat(\n\u001b[0m\u001b[1;32m    957\u001b[0m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Integer division by zero\n\t [[{{node StatefulPartitionedCall/FloorMod}}]] [Op:__inference_train_step_2788]\n\nFunction call stack:\ntrain_step\n"
     ]
    }
   ],
   "source": [
    "loss = DDPG(num_iterations = num_iterations,\n",
    "            actor_net = actor_ann,\n",
    "            critic_net = critic_ann,\n",
    "            directory = PATH,\n",
    "            plots = plots,\n",
    "            eval_interval = eval_interval,\n",
    "            # Agent hyperparameters\n",
    "            actor_learning_rate = actor_learning_rate,\n",
    "            critic_learning_rate = critic_learning_rate,\n",
    "            ou_stddev = ou_stddev,\n",
    "            ou_damping = ou_damping,\n",
    "            target_update_tau = target_update_tau,\n",
    "            target_update_period = target_update_period,\n",
    "            gamma = gamma,\n",
    "            # Experience replay hyperparameters\n",
    "            initial_collect_episodes = initial_collect_episodes,\n",
    "            collect_episodes_per_iteration = collect_episodes_per_iteration,\n",
    "            rb_capacity = rb_capacity,\n",
    "            batch_size = batch_size,\n",
    "            train_sequence_length = train_sequence_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
