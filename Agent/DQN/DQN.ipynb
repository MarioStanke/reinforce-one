{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "variable-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DQN Implementation using tf agents library\n",
    "Make sure PATH is an existing repository. \n",
    "Please create a depository PATH/plots for plots showing what is happening\n",
    "in the environment during policy evaluation.\n",
    "'''\n",
    "PATH = '~/Masterarbeit/DQN'  \n",
    "root_dir = PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accepted-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "import gin\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import range\n",
    "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments.examples import masked_cartpole  # pylint: disable=unused-import\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.keras_layers import dynamic_unroll_layer\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from Env_P2_N import Env_P2_N\n",
    "#from P_Env_P2_N import P_Env_P2_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "730e4183-6b8a-4629-9e9a-dc77f9c6b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plots(\n",
    "    plot_dir,    # Directory for plot output\n",
    "    plot_policy,    # Policy to be used for video creation\n",
    "    plot_id,\n",
    "    num_episodes = 1):\n",
    "\n",
    "    plot_env = tf_py_environment.TFPyEnvironment(P_Env_P1(root_dir = plot_dir,\n",
    "                                                         global_step = plot_id))\n",
    "    for _ in range (num_episodes):\n",
    "        time_step = plot_env.reset()\n",
    "        policy_init = plot_policy.get_initial_state(plot_env.batch_size)\n",
    "        policy_step = plot_policy.action(time_step, policy_init)\n",
    "        while True:\n",
    "            if time_step.is_last():\n",
    "                break\n",
    "            time_step = plot_env.step(policy_step.action)\n",
    "            policy_step = plot_policy.action(time_step, policy_step.state)\n",
    "        #plot_env.close()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "human-scotland",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "KERAS_LSTM_FUSED = 2\n",
    "\n",
    "\n",
    "@gin.configurable\n",
    "def train_eval(\n",
    "    root_dir,\n",
    "    #Params for Env\n",
    "    env_name='Env_P2_N',\n",
    "    n_herds = 5,\n",
    "    total_pop = 5000,\n",
    "    \n",
    "    num_iterations=100000,\n",
    "    train_sequence_length=1,\n",
    "    # Params for QNetwork\n",
    "    fc_layer_params=(100,),\n",
    "    # Params for QRnnNetwork\n",
    "    input_fc_layer_params=(50,),\n",
    "    lstm_size=(20,),\n",
    "    output_fc_layer_params=(20,),\n",
    "\n",
    "    # Params for collect\n",
    "    initial_collect_steps=1000,\n",
    "    collect_steps_per_iteration=2,\n",
    "    epsilon_greedy=0.1,\n",
    "    replay_buffer_capacity=100000,\n",
    "    # Params for target update\n",
    "    target_update_tau=0.05,\n",
    "    target_update_period=5,\n",
    "    # Params for train\n",
    "    train_steps_per_iteration=1,\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-3,\n",
    "    n_step_update=1,\n",
    "    gamma=0.99,\n",
    "    reward_scale_factor=1.0,\n",
    "    gradient_clipping=None,\n",
    "    use_tf_functions=True,\n",
    "    # Params for eval\n",
    "    num_eval_episodes=10,\n",
    "    eval_interval=1000,\n",
    "    # Params for checkpoints\n",
    "    train_checkpoint_interval=10000,\n",
    "    policy_checkpoint_interval=5000,\n",
    "    rb_checkpoint_interval=20000,\n",
    "    # Params for summaries and logging\n",
    "    log_interval=1000,\n",
    "    summary_interval=1000,\n",
    "    summaries_flush_secs=10,\n",
    "    debug_summaries=False,\n",
    "    summarize_grads_and_vars=False,\n",
    "    eval_metrics_callback=None):\n",
    "    \n",
    "    \"\"\"A simple train and eval for DQN.\"\"\"\n",
    "    root_dir = os.path.expanduser(root_dir)\n",
    "    train_dir = os.path.join(root_dir, 'train')\n",
    "    eval_dir = os.path.join(root_dir, 'eval')\n",
    "\n",
    "    train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "        train_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "    train_summary_writer.set_as_default()\n",
    "\n",
    "    eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "        eval_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "    eval_metrics = [\n",
    "        tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n",
    "        tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n",
    "    ]\n",
    "\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "    with tf.compat.v2.summary.record_if(\n",
    "        lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
    "        tf_env = tf_py_environment.TFPyEnvironment(Env_P2_N(num_herds = n_herds, total_population = total_pop))\n",
    "        eval_tf_env = tf_py_environment.TFPyEnvironment(Env_P2_N(num_herds = n_herds, total_population = total_pop))\n",
    "\n",
    "        if train_sequence_length != 1 and n_step_update != 1:\n",
    "            raise NotImplementedError(\n",
    "            'train_eval does not currently support n-step updates with stateful '\n",
    "            'networks (i.e., RNNs)')\n",
    "\n",
    "        action_spec = tf_env.action_spec()\n",
    "        num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "\n",
    "        if train_sequence_length > 1:\n",
    "            q_net = create_recurrent_network(\n",
    "            input_fc_layer_params,\n",
    "            lstm_size,\n",
    "            output_fc_layer_params,\n",
    "            num_actions)\n",
    "        else:\n",
    "            q_net = create_feedforward_network(fc_layer_params, num_actions)\n",
    "            train_sequence_length = n_step_update\n",
    "\n",
    "    # TODO(b/127301657): Decay epsilon based on global step, cf. cl/188907839\n",
    "        tf_agent = dqn_agent.DqnAgent(\n",
    "            tf_env.time_step_spec(),\n",
    "            tf_env.action_spec(),\n",
    "            q_network=q_net,\n",
    "            epsilon_greedy=epsilon_greedy,\n",
    "            n_step_update=n_step_update,\n",
    "            target_update_tau=target_update_tau,\n",
    "            target_update_period=target_update_period,\n",
    "            optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate),\n",
    "            td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "            gamma=gamma,\n",
    "            reward_scale_factor=reward_scale_factor,\n",
    "            gradient_clipping=gradient_clipping,\n",
    "            debug_summaries=debug_summaries,\n",
    "            summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "            train_step_counter=global_step)\n",
    "        tf_agent.initialize()\n",
    "\n",
    "        train_metrics = [\n",
    "            tf_metrics.NumberOfEpisodes(),\n",
    "            tf_metrics.EnvironmentSteps(),\n",
    "            tf_metrics.AverageReturnMetric(),\n",
    "            tf_metrics.AverageEpisodeLengthMetric(),\n",
    "        ]\n",
    "\n",
    "        eval_policy = tf_agent.policy\n",
    "        collect_policy = tf_agent.collect_policy\n",
    "\n",
    "        replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "            data_spec=tf_agent.collect_data_spec,\n",
    "            batch_size=tf_env.batch_size,\n",
    "            max_length=replay_buffer_capacity)\n",
    "\n",
    "        collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "            tf_env,\n",
    "            collect_policy,\n",
    "            observers=[replay_buffer.add_batch] + train_metrics,\n",
    "            num_steps=collect_steps_per_iteration)\n",
    "\n",
    "        train_checkpointer = common.Checkpointer(\n",
    "            ckpt_dir=train_dir,\n",
    "            agent=tf_agent,\n",
    "            global_step=global_step,\n",
    "            metrics=metric_utils.MetricsGroup(train_metrics, 'train_metrics'))\n",
    "        policy_checkpointer = common.Checkpointer(\n",
    "            ckpt_dir=os.path.join(train_dir, 'policy'),\n",
    "            policy=eval_policy,\n",
    "            global_step=global_step)\n",
    "        rb_checkpointer = common.Checkpointer(\n",
    "            ckpt_dir=os.path.join(train_dir, 'replay_buffer'),\n",
    "            max_to_keep=1,\n",
    "            replay_buffer=replay_buffer)\n",
    "\n",
    "        train_checkpointer.initialize_or_restore()\n",
    "        rb_checkpointer.initialize_or_restore()\n",
    "\n",
    "        if use_tf_functions:\n",
    "            # To speed up collect use common.function.\n",
    "            collect_driver.run = common.function(collect_driver.run)\n",
    "            tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "        initial_collect_policy = random_tf_policy.RandomTFPolicy(\n",
    "            tf_env.time_step_spec(), tf_env.action_spec())\n",
    "\n",
    "        # Collect initial replay data.\n",
    "        logging.info(\n",
    "            'Initializing replay buffer by collecting experience for %d steps with '\n",
    "            'a random policy.', initial_collect_steps)\n",
    "        dynamic_step_driver.DynamicStepDriver(\n",
    "            tf_env,\n",
    "            initial_collect_policy,\n",
    "            observers=[replay_buffer.add_batch] + train_metrics,\n",
    "            num_steps=initial_collect_steps).run()\n",
    "\n",
    "        results = metric_utils.eager_compute(\n",
    "            eval_metrics,\n",
    "            eval_tf_env,\n",
    "            eval_policy,\n",
    "            num_episodes=num_eval_episodes,\n",
    "            train_step=global_step,\n",
    "            summary_writer=eval_summary_writer,\n",
    "            summary_prefix='Metrics',\n",
    "        )\n",
    "        if eval_metrics_callback is not None:\n",
    "            eval_metrics_callback(results, global_step.numpy())\n",
    "        metric_utils.log_metrics(eval_metrics)\n",
    "\n",
    "        time_step = None\n",
    "        policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n",
    "\n",
    "        timed_at_step = global_step.numpy()\n",
    "        time_acc = 0\n",
    "\n",
    "        # Dataset generates trajectories with shape [Bx2x...]\n",
    "        dataset = replay_buffer.as_dataset(\n",
    "            num_parallel_calls=3,\n",
    "            sample_batch_size=batch_size,\n",
    "            num_steps=train_sequence_length + 1).prefetch(3)\n",
    "        iterator = iter(dataset)\n",
    "\n",
    "        def train_step():\n",
    "            experience, _ = next(iterator)\n",
    "            return tf_agent.train(experience)\n",
    "\n",
    "        if use_tf_functions:\n",
    "            train_step = common.function(train_step)\n",
    "\n",
    "        for _ in range(num_iterations):\n",
    "            start_time = time.time()\n",
    "            time_step, policy_state = collect_driver.run(\n",
    "                time_step=time_step,\n",
    "                policy_state=policy_state,\n",
    "            )\n",
    "            for _ in range(train_steps_per_iteration):\n",
    "                train_loss = train_step()\n",
    "            time_acc += time.time() - start_time\n",
    "\n",
    "            if global_step.numpy() % log_interval == 0:\n",
    "                logging.info('step = %d, loss = %f', global_step.numpy(),\n",
    "                        train_loss.loss)\n",
    "                steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n",
    "                logging.info('%.3f steps/sec', steps_per_sec)\n",
    "                tf.compat.v2.summary.scalar(\n",
    "                    name='global_steps_per_sec', data=steps_per_sec, step=global_step)\n",
    "                timed_at_step = global_step.numpy()\n",
    "                time_acc = 0\n",
    "\n",
    "            for train_metric in train_metrics:\n",
    "                train_metric.tf_summaries(\n",
    "                    train_step=global_step, step_metrics=train_metrics[:2])\n",
    "\n",
    "            if global_step.numpy() % train_checkpoint_interval == 0:\n",
    "                train_checkpointer.save(global_step=global_step.numpy())\n",
    "\n",
    "            if global_step.numpy() % policy_checkpoint_interval == 0:\n",
    "                policy_checkpointer.save(global_step=global_step.numpy())\n",
    "\n",
    "            if global_step.numpy() % rb_checkpoint_interval == 0:\n",
    "                rb_checkpointer.save(global_step=global_step.numpy())\n",
    "\n",
    "            if global_step.numpy() % eval_interval == 0:\n",
    "                results = metric_utils.eager_compute(\n",
    "                    eval_metrics,\n",
    "                    eval_tf_env,\n",
    "                    eval_policy,\n",
    "                    num_episodes=num_eval_episodes,\n",
    "                    train_step=global_step,\n",
    "                    summary_writer=eval_summary_writer,\n",
    "                    summary_prefix='Metrics',\n",
    "                )\n",
    "                if eval_metrics_callback is not None:\n",
    "                    eval_metrics_callback(results, global_step.numpy())\n",
    "                metric_utils.log_metrics(eval_metrics)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "logits = functools.partial(\n",
    "    tf.keras.layers.Dense,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.constant_initializer(-0.2))\n",
    "\n",
    "\n",
    "dense = functools.partial(\n",
    "    tf.keras.layers.Dense,\n",
    "    activation=tf.keras.activations.relu,\n",
    "    kernel_initializer=tf.compat.v1.variance_scaling_initializer(\n",
    "        scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "\n",
    "fused_lstm_cell = functools.partial(\n",
    "    tf.keras.layers.LSTMCell, implementation=KERAS_LSTM_FUSED)\n",
    "\n",
    "\n",
    "def create_feedforward_network(fc_layer_units, num_actions):\n",
    "    print(num_actions)\n",
    "    return sequential.Sequential(\n",
    "      [dense(num_units) for num_units in fc_layer_units]\n",
    "      + [logits(num_actions)])\n",
    "\n",
    "\n",
    "def create_recurrent_network(\n",
    "    input_fc_layer_units,\n",
    "    lstm_size,\n",
    "    output_fc_layer_units,\n",
    "    num_actions):\n",
    "    rnn_cell = tf.keras.layers.StackedRNNCells(\n",
    "        [fused_lstm_cell(s) for s in lstm_size])\n",
    "    return sequential.Sequential(\n",
    "        [dense(num_units) for num_units in input_fc_layer_units]\n",
    "        + [dynamic_unroll_layer.DynamicUnroll(rnn_cell)]\n",
    "        + [dense(num_units) for num_units in output_fc_layer_units]\n",
    "        + [logits(num_actions)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decent-neutral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 2 2 2 2 2]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars\n  In call to configurable 'train_eval' (<function train_eval at 0x7f6ff66f71f0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ea82a0703565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m       \u001b[0mscope_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" in scope '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_or_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m       \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgin_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/gin/utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mproxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-efdd7090d01d>\u001b[0m in \u001b[0;36mtrain_eval\u001b[0;34m(root_dir, env_name, n_herds, total_pop, num_iterations, train_sequence_length, fc_layer_params, input_fc_layer_params, lstm_size, output_fc_layer_params, initial_collect_steps, collect_steps_per_iteration, epsilon_greedy, replay_buffer_capacity, target_update_tau, target_update_period, train_steps_per_iteration, batch_size, learning_rate, n_step_update, gamma, reward_scale_factor, gradient_clipping, use_tf_functions, num_eval_episodes, eval_interval, train_checkpoint_interval, policy_checkpoint_interval, rb_checkpoint_interval, log_interval, summary_interval, summaries_flush_secs, debug_summaries, summarize_grads_and_vars, eval_metrics_callback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             num_actions)\n\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mq_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_feedforward_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc_layer_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0mtrain_sequence_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_step_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-efdd7090d01d>\u001b[0m in \u001b[0;36mcreate_feedforward_network\u001b[0;34m(fc_layer_units, num_actions)\u001b[0m\n\u001b[1;32m    270\u001b[0m     return sequential.Sequential(\n\u001b[1;32m    271\u001b[0m       \u001b[0;34m[\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum_units\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfc_layer_units\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m       + [logits(num_actions)])\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         activity_regularizer=activity_regularizer, **kwargs)\n\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars\n  In call to configurable 'train_eval' (<function train_eval at 0x7f6ff66f71f0>)"
     ]
    }
   ],
   "source": [
    "train_eval(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
